%% ------------------------------------------------------------------------- %%
\chapter{Estratégias usuais de modelagem}
\label{cap:regressao}

%Há duas formas de atacar esse problema utilizando modelagem estatística. A primeira é assumir um modelo interpretável para o mecanismo gerador do fenômeno e estimar os parâmetros que caracterizam esse modelo. Se o modelo utilizado for uma boa representação da natureza, as suas estimativas trarão informação sobre a relação entre as variáveis. A segunda maneira é utilizar modelos não-interpretáveis em conjunto com alguma técnica para interpretação do modelo. Vamos reproduzir aqui as duas alternativas, começando com a primeira.

\begin{flushright}
	\textcolor{gray}{
		\begin{tabular}{r}
			Data will often point with almost \\
			equal emphasis on several possible\\
			models, and it is important that the \\
			statistician recognize and accept this.\\ 
			--- McCullah and Nelder (1989)
		\end{tabular} 
	}	
\end{flushright}
\vspace{5mm}

O grande objetivo de uma análise estatística é usar um conjunto de dados para gerar conhecimento sobre um fenômeno de interesse. Podemos pensar nesse fenômeno como um mecanismo da natureza, desconhecido e complexo, no qual um conjunto de variáveis explicativas $\mathbf{X}$ são transformadas em uma variável resposta $Y$\footnote{Também podemos ter o caso multivariado, em que são geradas um conjunto de variáveis respostas $\mathbf{Y}$.} (Figura \ref{fig:cap-modelos-black-box}). Os dados são o resultado desse processo \citep{Breiman2001}.  

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\linewidth]{figuras/cap-modelos-black-box.png}
	\caption{Esquematização do mecanismo gerador dos dados.}
	\label{fig:cap-modelos-black-box}
\end{figure}


No contexto da modelagem estatística supervisionada\footnote{No qual uma variável resposta \textit{supervisiona} a estimação dos parâmetros do modelo. Na prática, são os casos em que temos acesso a uma amostra da variável resposta.} \citep{Hastie2008}, dada a variável resposta $Y$ e o vetor de variáveis explicativas $\mathbf{X} = (X_{1}, \dots, X_{p})$, queremos encontrar funções $f$'s tais que

\begin{equation}
Y \approx f(\mathbf{X}),
\label{mod:y-appr-X}
\end{equation}
isto é, queremos uma função $f(\cdot)$ que descreva o mecanismo gerador dos dados da forma mais precisa possível. A partir dessa função, poderíamos tanto fazer predições --- descobrir qual é o novo valor de $Y$ para novas observações $\mathbf{X}$ --- quanto inferência --- investigar como as variáveis $\mathbf{X}$ e $Y$ estão relacionadas.

A expressão (\ref{mod:y-appr-X}) representa diversas classes de modelos, a depender da escolha de $f(\cdot)$. De uma forma geral, modelos estatísticos são simplificações da realidade e, por isso, estão sujeitos a erros. Quando modelamos a série de um poluente, por exemplo, estamos supondo que a sua concentração ao longo do tempo pode ser aproximada por uma função matemática. Neste caso, o erro do modelo quantifica o quanto a nossa função se afasta do verdadeiro mecanismo gerador do poluente. Parte desse erro é irredutível e se deve a impedimentos práticos, como erros de medida, variáveis que não podem ser observadas e desconhecimento de outros fatores que influenciam o fenômeno. No entanto, o erro total pode ser minimizado pela escolha adequada do modelo utilizado, o que torna essencial o desenvolvimento de estratégias de modelagem que contemplem as particularidades de cada estudo. Assim, podemos reescrever (\ref{mod:y-appr-X}), já no contexto de séries temporais, como

\begin{equation}
Y_t = f(\X) + \epsilon_t,
\label{mod:y-equal-X-e}
\end{equation}
sendo $\epsilon_t$ um erro aleatório, isto é, um componente que representa toda a informação de $Y_t$ que não pode ser explicada pelos preditores $\X$. Apesar de a expressão (\ref{mod:y-appr-X}) ser mais intuitiva, (\ref{mod:y-equal-X-e}) é mais conveniente para a formulação dos modelos estatísticos.

Na prática, há duas abordagens bastante utilizadas na especificação da função $f(\cdot)$. A primeira consiste em supor um modelo probabilístico para o fenômeno sob estudo, de tal forma que $f(\cdot)$ seja uma função dos parâmetros de alguma distribuição conhecida, que podem ser estimados a partir dos dados. Essa estratégia geralmente produz modelos interpretáveis, que trazem informação sobre a relação da variável resposta e os preditores, e por isso é preferível quando o interesse é fazer inferência. A segunda abordagem é mais flexível e permite que os próprios dados definam uma estrutura para $f(\cdot)$. Essa estratégia dificilmente gera modelos interpretáveis, pois a complexidade do mecanismo gerador dos dados é refletida na função resultante. Por outro lado, a maior flexibilidade leva a uma maior precisão desses modelos, sendo muito utilizados para predição. A abordagem escolhida deve atender aos objetivos do estudo. Nesta tese, exploraremos exemplos de ambos os casos.

%A relação (\ref{mod:y-appr-X}) implica que parte da variabilidade de $Y$ pode ser explicada pelo componente sistemático $f(\X)$. Em outras palavras, $f(\cdot)$ representa a informação que $\mathbf{X}$ nos fornece sobre $Y$. 

Neste capítulo, introduziremos os principais modelos interpretáveis utilizados na literatura para análise de dados de poluição do ar, como o modelo de regressão linear, modelos aditivos e modelos para séries temporais. No Capítulo \ref{cap:aprendizado_estatistico}, discutiremos técnicas focadas em predição, como validação cruzada e regularização, mas que muitas vezes também podem ser utilizadas para inferência.

\section{Regressão linear}
\label{sec:modelo-linear}

O modelo de regressão linear corresponde à aproximação (\ref{mod:y-appr-X}) mais simples e bem estabelecida dentro da modelagem estatística. Mesmo com a disponibilidade de modelos mais flexíveis, essa classe de modelos ainda é bastante utilizada hoje em dia, principalmente por se ajustar bem a diversos problemas reais, facilidade de interpretação dos resultados e estar disponível nos principais programas estatísticos.

Em estudos de poluição do ar, modelos de regressão linear podem ser ajustados para investigar a relação entre variáveis explicativas e uma variável resposta, seja a concentração de poluentes ou dados epidemiológicos. \cite{Saldiva1995}, por exemplo, utilizou esses modelos para estudar o efeito de alguns poluentes nas taxas de mortalidade de idosos, controlando por condições climáticas e sazonais. Já \cite{Salvo2017} utilizou para associar os níveis de material particulado com a proporção de carros a álcool e gasolina.

Apesar da sua popularidade, a complexidade presente nos estudos de poluição atmosférica, como relações não-lineares e autocorrelação, pode desqualificar o modelo de regressão linear como a opção mais adequada para o ajuste dos dados. Pela sua facilidade de implementação e interpretação, ele é uma boa ferramenta para uma análise preliminar.

Nas próximas seções, especificaremos o modelo de regressão linear, discutiremos as suas restrições e apresentaremos as maneiras mais utilizadas para tratar séries com tendência, sazonalidade e autocorrelação.


\subsection{Especificação do modelo}
\label{sec:linear-espec-modelo}

Seja $Y_t$ a variável resposta,  $\X = (X_{1t}, ..., X_{pt})$ um vetor de variáveis explicativas cuja associação com $Y_t$ estamos interessados em avaliar e $t = 1, \dots, n$ a ordem na qual essas variáveis foram medidas. Aqui, não faremos suposições sobre a natureza dos preditores $\X$, isto é, essas variáveis podem ser fixas ou aleatórias, qualitativas ou quantitativas. Dado os vetores de parâmetros desconhecidos $\boldsymbol{\beta} = (\beta_1, ..., \beta_p)$, o modelo de regressão linear pode ser definido por

\begin{equation}
Y_t = \alpha + \beta_1 X_{1t} + ... + \beta_p X_{pt} + \epsilon_t, \quad t = 1, ..., n.
\label{mod:linear}
\end{equation}
Em geral, supomos que os erros $(\epsilon_1, ..., \epsilon_n)$ tenham média zero, variância constante (homoscedasticidade) e sejam não-correlacionados\footnote{A suposição de distribuição Normal também é feita em alguns casos. Essa suposição é relevante na construção de intervalos de confiança e testes de hipóteses para os coeficientes do modelo. No entanto, para amostras grandes, característica comum em estudos de poluição do ar, existem resultados assintóticos \citep{Casella2001} que garantem a validade desses procedimentos.}. Além disso, a especificação (\ref{mod:linear}) impõe que a relação entre a resposta $Y_t$ e os preditores $\X$ seja linear e aditiva.

A suposição de linearidade estabelece que a variação esperada em $Y_t$ causada pelo acréscimo de uma unidade em $X_{it}$, mantidos fixados os outros preditores, é constante e não depende do valor de $X_{it}$. A interpretação dos coeficientes será discutida com mais detalhes na Seção \ref{sec:linearidade} e nas aplicações dos Capítulos \ref{cap:combustiveis} e \ref{cap:saude}. Conceitos mais gerais podem ser encontrados em \cite{Hastie2008} e \cite{James2013}.

%O coeficiente $\alpha$ representa a concentração média do poluente quando todas as variáveis do modelo valem zero e, por essa razão, não costuma ter interpretação prática em estudos desta natureza.

A suposição de aditividade estabelece que a variação esperada em $Y_t$ causada por uma mudança no preditor $X_{it}$ independe do valor (fixado) dos outros preditores. Essa suposição pode ser relaxada com a introdução de termos de interação (ver Seção 3.3 de \cite{James2013}), que abordaremos na Seção \ref{sec:aditividade}.

Na prática, os coeficientes $\beta_1, ..., \beta_p$ são desconhecidos e precisam ser estimados. O procedimento de estimação mais utilizado é o método de mínimos quadrados \citep{Hastie2008}. Outro método bastante utilizado é a estimação por máxima verossimilhança \citep{Casella2001}. Sob a suposição de normalidade, as duas abordagens são equivalentes.

Como o instante em que as observações omissas ocorrem não é relevante no processo de estimação, o modelo linear é uma alternativa para avaliar a associação de séries com ``buracos'' ou grandes períodos sem informação, apesar de a identificação da estrutura de tendência e sazonalidade ser mais difícil em dados com essa característica. 

A adequação do modelo é avaliada a partir de medidas de qualidade de ajuste, como o $R^2$ e o erro quadrático médio, e da \textit{análise de resíduos}. A partir da expressão (\ref{mod:linear}), para $t = 1, ..., n$, podemos definir os resíduos como

\begin{equation}
	r_t = Y_t - \widehat{Y}_t,
	\label{RLN-residuos}
\end{equation}
em que $\hat{Y}_t$ representa o valor predito de $Y_t$ com base nas estimativas dos coeficientes do modelo. Os resíduos medem o quanto os valores preditos se afastam dos valores observados, sendo muito úteis para avaliar a qualidade do ajuste e a violação das suposições do modelo. Esse tópico será discutido com mais detalhes na Seção \ref{sec:reg-quali-mod}.

No R, os modelos de regressão linear podem ser ajustados via mínimos quadrados com a função \texttt{lm()} do pacote \texttt{stats} ou utilizando a função \texttt{train} do pacote \texttt{caret} com \texttt{method = "lm"}. O pacote \texttt{caret} traz uma abordagem padronizada para o ajuste de modelos estatísticos.

A seguir, abordaremos como modelar tendência e sazonalidade utilizando o modelo de regressão linear.

%Em alguns casos, as variáveis respostas podem ser medidas em diferentes locais de uma região. \cite{Salvo2014}, por exemplo, mediram a concentração de ozônio (e outras medidas meteorológicas e de tráfego) em diversas estações ao longo da cidade de São Paulo. Como esse componente espacial pode ser um fator de confundimento, ele deve ser considerado pelo modelo. Na Seção \ref{RLN-comp-espacial}, será abordada a incorporação de componentes espaciais no modelo linear para controlar a associação entre observações coletadas em localidades próximas.

%Em geral, os estudos de poluição do ar consideram variáveis positivas, assimétricas, possivelmente correlacionadas e heteroscedásticas. Essas características podem infringir as suposições do modelo linear, causando problemas no ajuste. Técnicas de diagnóstico --- em especial, a análise dos resíduos --- são uma boa opção para verificar se as suposições estabelecidas estão sendo violadas. Na Seção \ref{RLN-suposicoes}, será discutido como utilizar essas técnicas para avaliar se o modelo está bem ajustado e, em caso negativo, maneiras de contornar esse problema.

\subsection{Incorporando tendência e sazonalidade}
\label{sec:tend-sazon}

% REFs: Lin1999, Saldiva1995 (dummies para mês)
%      Schwartz1992 (dummies para estação)

Séries de poluição do ar não costumam ser estacionárias. Como vimos nos exemplos do Capítulo \ref{cap:analise-exploratoria}, é comum encontrarmos tendência (positivas ou negativas) e diversos tipos de sazonalidade (diária, semanal, anual etc). Fatores como crescimento populacional, industrialização, aumento da frota de automóveis, leis de regulamentação de combustíveis, entre outros, podem gerar mudanças a longo prazo na concentração de poluentes, alterando o comportamento da série, e muitas vezes não temos informação disponível para incorporá-los no modelo. 

Como o modelo de regressão linear não faz suposições sobre a estacionariedade da variável resposta, podemos modelar a tendência e a sazonalidade da série incluindo preditores que controlam esses componentes\footnote{Em vez de transformar a série original, como discutido na Seção \ref{sec:componentes-temporais}. As vantagens de se incluir um termo de tendência ao modelo, em vez de se transformar $Y_t$, são: (1) poder interpretar os coeficientes do modelo em função da variável original e (2) estimar a tendência da série.}. A inclusão desses termos é interessante principalmente nos casos em que não estamos interessados em estudar a evolução da série, mas sim o efeito de preditores na variável resposta, independentemente desses componentes.

Para acrescentar um termo de tendência linear ao modelo (\ref{mod:linear}), podemos especificar $X_{1t} = t$, $t = 1, ..., n$. Assim, um coeficiente $\beta_1$ positivo em (\ref{mod:linear}) indica que $Y$ cresce linearmente com o tempo, enquanto um coeficiente negativo indica que $Y$ decresce linearmente com o tempo. Podemos definir outras formas para a tendência, como quadrática, $X_{1t} = t^2$, ou logarítmica, $X_{1t} = \log(t)$. A Figura \ref{fig:exemplo-serie-tendencia-linear-quadratica} mostra um exemplo de séries com tendências linear e quadrática.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{figuras/exemplo-serie-tendencia-linear-quadratica}
	\caption{Exemplos de séries com tendência linear e quadrática, ambas positivas.}
	\label{fig:exemplo-serie-tendencia-linear-quadratica}
\end{figure}

Note que, se modelarmos a tendência dessa maneira, estamos impondo a mesma função ao longo de todo período observado. Em alguns casos, a tendência pode ser diferente em certos intervalos de tempo (Figura \ref{fig:exemplo-serie-tendencias-diferentes}). Uma alternativa seria definir um termo de tendência para cada intervalo, por exemplo:

	\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{figuras/exemplo-serie-tendencias-diferentes}
	\caption{Exemplos de uma série com tendência não-constante.}
	\label{fig:exemplo-serie-tendencias-diferentes}
\end{figure}

\begin{displaymath}
X_{1t} = \left\{
\begin{array}{ll}
t, & \textnormal{se $t$ pertence ao conjunto \{1, 2, ..., $m$\}; e} \\
0, & \textnormal{em caso contrário}.
\end{array}
\right.
\end{displaymath}
e
\begin{displaymath}
X_{2t} = \left\{
\begin{array}{ll}
t-m, & \textnormal{se $t$ pertence ao conjunto \{$m+1$, $m+2$, ..., $m+n$\}; e} \\
0, & \textnormal{em caso contrário}.
\end{array}
\right.
\end{displaymath}

A sazonalidade também pode ser controlada por meio de variáveis explicativas. A sua presença indica que a média da variável resposta está associada a efeitos periódicos, ligados a intervalos de tempo, como dias, semanas, meses, estações do ano, temporadas de chuva etc. Os níveis de ozônio, por exemplo, crescem no verão e diminuem no inverno; o número de problemas respiratórios tende a aumentar nos meses mais secos; e a concentração de diversos poluentes varia nos fins de semana, devido à menor intensidade de tráfego. 

%O gráfico da série é uma boa forma de se detectar a sazonalidade. Com uma análise descritiva adequada, é quase sempre possível identificar quando este componente é gerado por uma das variáveis explicativas. O gráfico dos resíduos contra o tempo do modelo (\ref{modelo-linear}) também pode ser usado para identificar a sazonalidade da série.

De uma maneira geral, podemos classificar a sazonalidade como \textit{determinística} --- o padrão é constante ao longo do tempo --- ou \textit{estocástica} --- o padrão muda ao longo do tempo. É possível controlar a sazonalidade determinística no modelo (\ref{mod:linear}) a partir de variáveis indicadoras. Se, por exemplo, acreditamos que há um efeito sazonal de mês, podemos adicionar ao modelo 11 variáveis indicadoras $X_{it}$, $i = 1, ..., 11$ tais que

\begin{equation}
	X_{it} = \left\{
	\begin{array}{ll}
		1, & \textnormal{se a observação $t$ pertence ao $i$-ésimo mês do ano; e} \\
		0, & \textnormal{caso contrário}.
	\end{array}
	\right.
\end{equation}
Com essa formulação, o mês de dezembro será tomado como referência, isto é, a interpretação dos coeficientes correspondentes aos meses será feita sempre em relação ao mês de dezembro. 

A inclusão de variáveis indicadoras também pode ser feita para controlar o efeito de variáveis que não estão disponíveis na amostra. \cite{Belusic2015}, por exemplo, utilizaram variáveis indicadoras para a hora do dia com o objetivo de controlar o efeito do trânsito no monitoramento de diversos poluentes na cidade de Zagreb, na Croácia. Dessa forma, cada coeficiente explicará as condições específicas da hora do dia a que ele se refere. Uma desvantagem dessa estratégia é não podermos avaliar se o coeficiente de fato está capturando o efeito do trânsito ou qualquer outra variável associada com a concentração dos poluentes que também varia a cada hora.

Para mais informações sobre a utilização de variáveis indicadoras em modelos de regressão, consultar a Seção 3.3.1 de \cite{James2013}.

Se a sazonalidade for estocástica, procedimentos um pouco mais sofisticados serão necessários para controlá-la. Não trataremos desse tópico neste trabalho. Mais informações podem ser encontradas em \cite{Shumway2006}.

A seguir, discutiremos como contornar as suposições de erros não-correlacionados, homoscedasticidade, linearidade e aditividade utilizando o modelo de regressão linear.

\subsection{Tratando erros correlacionados}

O modelo de regressão linear supõe que os erros $(\epsilon_1, \dots, \epsilon_n)$ sejam não-correlacionados. Em estudos de poluição do ar, essa suposição é, em geral, inadequada. Como discutimos na Seção \ref{sec:autocorrelacao}, é natural que observações de uma série temporal sejam autocorrelacionadas. A formação de gases na atmosfera, por exemplo, é um processo contínuo ao longo do tempo, sendo que as concentrações medidas no instante $t$ podem estar fortemente associadas aos níveis observados nas últimas horas ou mesmo nos últimos dias. 

Uma forma de avaliar a violação dessa suposição é construir o gráfico dos resíduos do modelo em função do tempo. A presença de padrões na sequência de pontos, isto é, resíduos adjacentes com valores próximos, é um indício de correlação. Na Figura \ref{fig:exemplo-serie-correlacao}, apresentamos os resíduos de um modelo de regressão linear ajustado em dados auto-correlacionados e em dados não-correlacionados. Para o primeiro caso, observe que os pontos adjacentes tendem a permanecer em um mesmo lado da reta $y = 0$. Na ausência de correlação, temos uma sequência aleatória de valores positivos e negativos.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.7\linewidth]{figuras/exemplo-serie-correlacao}
	\caption{Comparação entre os gráficos dos resíduos de um modelo linear contra o tempo para dados auto-correlacionados e dados não correlacionados.}
	\label{fig:exemplo-serie-correlacao}
\end{figure}

Se as observações são muito correlacionadas, os erros-padrão estimados pelo modelo de regressão linear tenderão a subestimar os verdadeiros erros, o que comprometeria a inferência, já que os valores $p$ associados seriam menores do que deveriam ser. Nesses casos, outras estratégias de modelagem devem ser adotadas.

Outro tipo de correlação muito comum é a causada por observações que pertencem a um mesmo grupo. Indivíduos de uma mesma família, por exemplo, compartilham a mesma genética e tendem apresentar respostas correlacionadas em estudos epidemiológicos. A localidade também configura formação de grupos, já que pessoas que moram numa mesma região geralmente estão expostas às mesmas condições ambientais. Observações realizadas em diferentes localizações, mas no mesmo instante também podem apresentar correlação. \cite{Salvo2017} utilizaram concentrações horárias de alguns poluentes em diversas estações de monitoramento em São Paulo, e é natural supor que as medidas feitas na mesma hora ou no mesmo dia estão correlacionadas.

A depender dos objetivos do estudo, agregar os dados pode ser boa uma alternativa para reduzir o efeito da correlação. Se estamos trabalhando com uma série horária e não temos o objetivo de investir a relação entre as variáveis ao longo do dia, podemos simplificar o problema utilizando a série de médias diárias (veja o exemplo discutido na Seção \ref{sec:combustiveis-dimensionando}). Assim, eliminamos a correlação gerada pelas medidas realizadas dentro do mesmo dia.

Para reduzir o efeito da correlação na estimação da variabilidade dos coeficientes, \cite{Salvo2017} utilizaram métodos robustos para o cálculo do erro-padrão. Os chamados \textit{clustered standard errors} \citep{Cameron2015} são obtidos a partir de uma especificação da matriz de variâncias e covariâncias que contempla a correlação entre indivíduos de um mesmo grupo. Essa técnica tem como vantagem a necessidade de especificar um modelo para os dados agrupados, mas faz a suposição que o número de grupos tende ao infinito.

Outra alternativa consiste na utilização de modelos que permitem a especificação das observações correlacionadas, como os modelos mistos \citep{McCulloch2001, Demidenko2013}.

%Uma alternativa é utilizar variáveis indicadoras para a unidade de tempo em que as medidas foram realizadas, como hora do dia, dia da semana, semana do mês etc. \hl{Essas variáveis passam a explicar a variação da resposta entre as unidades de tempo, ajudando a reduzir o efeito da correlação.} \cite{Salvo2014}, por exemplo, utilizou variáveis indicadoras para a hora em que a concentração de ozônio foi medida (das 13 às 16 horas) e ajustou um modelo de regressão linear para explicar os níveis de ozônio em função da proporção de carros a gasolina na cidade de São Paulo. 

%A suposição de independência entre as observações também pode ser inadequada em muitos casos. Na análise feita em \cite{Salvo2014}, por exemplo, o modelo supõe que as concentrações horárias de ozônio medidas em uma mesma estação são independentes, o que não seria razoável, pois esperamos que observações feitas no mesmo dia (ou até na mesma semana) sejam próximas. Para \textit{absorver} o efeito da correlação entre as medidas, os autores utilizam variáveis dummies para hora e dia da semana. Discutiremos essa estratégia no próximo capítulo.

\subsection{Contornado a suposição de homoscedasticidade}
\label{sec:heteroscedasticidade}

Assim como a média, a variância de $Y$ também pode mudar segundo algum preditor ou o próprio tempo, violando a suposição de homoscedasticidade do modelo de regressão linear. Nesses casos, precisamos escolher entre utilizar modelos mais flexíveis, que contemplem variância não-constante, ou aplicar transformações que estabilizem a variância das informações.

O gráfico dos resíduos em função dos valores preditos é uma boa ferramenta para identificar heteroscedasticidade. Como podemos observar na Figura \ref{fig:exemplo-serie-heteroscedasticidade}, nuvens de pontos em forma de funil são indícios de observações heteroscedásticas: a variância é maior para valores preditos menores.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{figuras/exemplo-serie-heteroscedasticidade}
	\caption{Gráfico dos resíduos contra os valores preditos. Exemplo de nuvem de pontos em forma de funil, indicando heteroscedasticidade.}
	\label{fig:exemplo-serie-heteroscedasticidade}
\end{figure}

Uma maneira de estabilizar a variância das observações é transformar a variável $Y$ usando funções côncavas, como $\log Y$ e $\sqrt{Y}$. Uma outra alternativa consiste em ponderar as observações com pesos proporcionais ao inverso de sua variância, mas essa técnica se limita aos casos em que a variabilidade pode ser estimada com precisão.

Os modelos lineares generalizados duplos \citep{Gilberto2013} e os modelos mistos \citep{McCulloch2001, Demidenko2013} são alternativas aos modelos de regressão linear que modelam também a variância das observações.

\subsection{Contornado a suposição de linearidade}
\label{sec:linearidade}

Para entendermos melhor a suposição de linearidade, vamos considerar o modelo de regressão linear mais simples, com apenas um preditor:

\begin{equation}
	Y_t = \beta_0 + \beta_1 X_{t} + \epsilon_t, \quad t = 1, \dots, n.
	\label{mod:linear-simples}
\end{equation}

Ao estimarmos os parâmetros $\beta_0$ e $\beta_1$ (pelo método de mínimos quadrados, por exemplo), obtemos a seguinte reta de regressão

\begin{equation}
\widehat{Y_t} = \hat{\beta}_0 + \hat{\beta}_1 X_{t}, \quad t = 1, \dots, n,
\label{mod:reta-de-regressao-simples}
\end{equation}
sendo $\widehat{Y_t}$ o valor de $Y_t$ predito pelo modelo e $\hat{\beta}_0$ e $\hat{\beta}_1$ as estimativas de $\beta_0$ e $\beta_1$ respectivamente. Note que (\ref{mod:reta-de-regressao-simples}) representa a equação de uma reta com intercepto $\hat{\beta}_0$ e coeficiente angular $\hat{\beta}_1$. Isso significa que essa reta cruza o eixo $y$ no ponto $\hat{\beta}_0$ e, se variamos o valor de $X_{t}$ em uma unidade, $\widehat{Y_t}$ vai variar $\hat{\beta}_1$ unidades, não importa qual seja o valor de $X_{t}$ ($\hat{\beta}_1$ determina a inclinação da reta). Essa associação entre $\widehat{Y_t}$ e $X_{t}$ (ou $Y_t$ e $X_{t}$) é dita ser \textit{linear} com respeito aos parâmetros e está ilustrada na Figura \ref{fig:suposicao-linearidade}, para $\hat{\beta}_0$ igual a 0 e $\hat{\beta}_1$ igual a 10. Quando temos mais de um preditor, como no modelo (\ref{mod:linear}), a interpretação é análoga para cada par $(\widehat{Y_t}, X_{it})$ se mantivermos as as outras variáveis fixadas.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.7\linewidth]{figuras/suposicao-linearidade.pdf}
	\caption{A estimativa $\hat{\beta}$ representa a variação em $Y$ quando acrescemos $X$ em uma unidade, não importando o valor de $X$.}
	\label{fig:suposicao-linearidade}
\end{figure}

Repare que a suposição de linearidade é mais forte do que apenas monotonicidade. Enquanto a monotonicidade restringe que a associação entre as variáveis seja sempre crescente ou decrescente, a linearidade também restringe o quanto a variável resposta varia quando o preditor aumenta ou diminui em uma unidade. Essa diferença é importante pois muitas vezes utilizamos modelos lineares na tentativa de explicar relações que são apenas monotônicas, o que pode levar a estimativas pouco confiáveis e conclusões equivocadas. Uma discussão mais detalhada sobre esse problema pode ser encontrada em \cite{Achen2005}.

%A suposição de linearidade é restritiva pois a verdadeira relação entre $Y_t$ e $X_t$ pode ter outras formas. \Ex

Os resíduos, definidos pela expressão (\ref{RLN-residuos}), podem ser utilizados para avaliar se a suposição de linearidade é razoável. A ideia consiste em construir o gráfico dos resíduos contra os valores preditos e verificar se a nuvem de pontos apresenta algum padrão. Nuvens em forma de ``U'', por exemplo, mostram que o modelo não está bem ajustado para valores extremos de $Y$, indicando não-linearidade (veja Figura \ref{fig:exemplo-residuos-linearidade-forma-U}).

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{figuras/exemplo-residuos-linearidade-forma-U}
	\caption{Gráfico dos resíduos contra os valores preditos, um exemplo de nuvem de pontos em forma de ``U'', indicando não-linearidade.}
	\label{fig:exemplo-residuos-linearidade-forma-U}
\end{figure}

Uma maneira simples de contornar esse problema é ajustar modelos da forma

\begin{equation}
Y_t = \beta_0 + \beta_1 T(X_{t}) + \epsilon_t, \quad t = 1, \dots, n,
\label{mod:linear-transformacao}
\end{equation}
em que $T(\cdot)$ representa uma função ``linearizadora''. As escolhas mais comuns para $T(X)$ são $\log X$ e $\sqrt{X}$. Observe que, embora a relação entre $Y$ e $X$ em (\ref{mod:linear-transformacao}) não seja mais linear, o modelo continua sendo linear nos parâmetros. Um ponto negativo nessa abordagem é a perda de interpretabilidade do modelo, já que os parâmetros estarão associados agora à $T(X)$ e não mais a $X$.

Modelos polinomiais \citep{James2013} também podem ser utilizados para contornar a não-linearidade. Dado um único preditor $X$, um modelo polinomial pode ser especificado como

\begin{displaymath}
	Y_t = \beta_0 + \beta_1 X_t + \beta_2 X_t^2 + \cdots +  \beta_p X_t^p + \epsilon_t, \quad t = 1, \dots, n.
	%\label{mod:polinomial}
\end{displaymath}
Essa classe de modelos é bem flexível e permite ajustar associações complexas entre as variáveis $X$ e $Y$, sendo uma boa alternativa para predição, mas pouco utilizados para inferência devido à falta de interpretação.

Modelos de regressão segmentada \citep{Muggeo2003} são outra alternativa para ajustar relações não-lineares. Vamos supor que, para um determinado local, as médias diárias da concentração de ozônio e da temperatura sejam relacionadas como na gráfico da esquerda da Figura \ref{fig:cap-regressao-exemplo-reg-seg}. O ajuste de um modelo de regressão segmentada a exemplo consiste em estimar um ou mais pontos de corte para a temperatura e, em cada região formada, ajustar uma reta de regressão com inclinação possivelmente diferente. O gráfico da direita apresenta as retas de regressão segmentada para um ponto de corte na temperatura.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{figuras/cap-regressao-exemplo-reg-seg.pdf}
	\caption{Exemplo de regressão segmentada. À esquerda, o gráfico de concentrações médias diárias de ozônio pela temperatura média diária. À direita, um modelo de regressão segmentada ajustada aos pontos com um ponto de corte.}
	\label{fig:cap-regressao-exemplo-reg-seg}
\end{figure}

A especificação do modelo, além de permitir que a inclinação das retas ajustadas mude em cada ponto de corte, possui parâmetros interpretáveis, ao contrário do modelo polinomial e dos modelos aditivos (ver Seção \ref{sec:gam}). A ideia por trás do algoritmo de estimação consiste em achar os pontos de corte no preditor que melhor representem mudanças na relação preditor/resposta. Isso é feito utilizando uma técnica de \textit{linearização} do modelo utilizando a expansão de Taylor de primeira ordem (ver \citep{Muggeo2003} para mais detalhes). Dessa forma, o ponto de corte passa a ser um parâmetro do modelo e a dificuldade computacional é a mesma de um ajuste de um modelo de regressão linear.


%Uma terceira maneira utiliza funções escadas para representar o preditor cuja associação com a resposta é supostamente não-linear. Essa variável é dividida em M categorias, transformando-se em uma variável qualitativa. Isso é equivalente a ajustar uma função constante em cada subintervalo.

Mais detalhes sobre linearidade e outras alternativas para contornar essa suposição podem ser encontradas em \cite{Hastie2008} e \cite{James2013}.

\subsection{Contornado a suposição de aditividade}
\label{sec:aditividade}


Pela suposição de aditividade, os termos do modelo (\ref{mod:linear}) são sempre somados, permitindo que cada coeficiente possa ser interpretado independentemente dos demais se os mantivermos fixados.

Na prática, o efeito de uma variável explicativa $X_1$ em $Y$ pode depender do nível de um outro preditor $X_2$. O efeito da poluição do ar ($X_1$) em crises respiratórias ($Y$), por exemplo, é muito mais acentuado em certas condições climáticas, como dias de baixa umidade ($X_2$). Essa relação entre $X_1$ e $X_2$ na variabilidade de $Y$ é chamada de \textit{interação}. 

Gráficos de perfis \citep{Singer2012} podem ser utilizados para identificar interação entre variáveis. Esses gráficos exigem que pelo menos um dos preditores seja categórico. Se ambas variáveis forem quantitativas, uma delas pode ser categorizada para a construção dos gráficos de perfis.

A interação de duas variáveis pode ser contemplada pelo modelo de regressão linear acrescentando-se termos da forma $X_1 \times X_2$. Interações de três ou mais variáveis também podem ser incluídas, mas dificilmente tem interpretação prática.

Termos de interação bastante utilizados em estudos de poluição do ar são aqueles entre as variáveis meteorológicas. Em geral, além de controlarmos o efeito marginal da temperatura, umidade, precipitação, radiação, vento etc., precisamos também incluir o efeito conjunto dessas variáveis.

\subsection{Avaliando a qualidade do ajuste}
\label{sec:reg-quali-mod}

Como discutido na introdução deste capítulo, nossos modelos sempre estarão sujeitos a erros. Assim, além de  verificarmos se o modelo escolhido viola as suposições pré-estabelecidas, também precisamos avaliar a magnitude do erro que estamos cometendo ao utilizarmos suas estimativas para descrever o evento sob estudo. Para modelos de regressão linear, isso pode ser feito a partir da raiz do erro quadrático médio (RMSE\footnote{Sigla para o termo em inglês \textit{root mean square error}. Utilizaremos aqui a sigla em inglês porque ela é bastante comum na literatura e nos programas estatísticos.}) e do coeficiente de determinação ($R^2$).

A raiz do erro quadrático médio é uma estimativa do desvio-padrão de $\epsilon$, uma medida do quanto, em média, a resposta $Y$ se desvia da verdadeira reta de regressão\footnote{No caso do modelo de regressão linear simples, por exemplo, a verdadeira reta de regressão é dada por $Y = \tilde{\beta}_0 + \tilde{\beta}_1X$, em que $\tilde{\beta}_0$ e $\tilde{\beta}_1$ representam os verdadeiros valores de $\beta_0$ e $\beta_1$. Na prática, $\tilde{\beta}_0$ e $\tilde{\beta}_1$ são desconhecidos e substituídos por valores estimados, como apresentado em (\ref{mod:reta-de-regressao-simples}).}. Valores baixos de RMSE significam que $\hat{Y_t} \approx Y_t$, para $t = 1, \dots, n$, sugerindo que o modelo está bem ajustado aos dados. Como essa medida depende da magnitude da variável resposta, não existem pontos de corte para definir o que é um RMSE pequeno.

O coeficiente de determinação é uma medida da proporção da variância de $Y$ explicada pelos preditores incluídos no modelo. Esse coeficiente varia entre 0 e 1 e, ao contrário do RMSE, não depende da escala de $Y$. Valores próximos de 1 apontam que uma porção considerável da variabilidade está sendo explicada, indicando que o modelo se ajusta bem aos dados. Na prática, valores de $R^2$ maiores que 0.7 são considerados altos.

Valores altos de RMSE ou baixos de $R^2$ sugerem problemas com o modelo. Não-linearidade e omissão de preditores importantes são os mais comuns. No primeiro caso, a principal estratégia é transformar os preditores cuja associação com $Y$ suspeitamos ser não-linear, assim como discutido na Seção \ref{sec:linearidade}. A solução para o segundo caso é obter mais informação sobre o fenômeno sob analise e incluir novos preditores ao modelo. Essa é uma tarefa complicada, pois dificilmente temos acesso a novas variáveis explicativas, e geralmente demonstra uma falha no delineamento do estudo.

Ao se avaliar o RMSE e $R^2$, um cuidado muito importante deve ser tomado. Acrescentar mais preditores ao modelo sempre irá diminuir o RMSE e aumentar o $R^2$, o que torna a estratégia de escolher o modelo com menor RMSE ou menor $R^2$ problemática. O excesso de parâmetros pode gerar \textit{sobreajuste} (ou \textit{overfitting}, em inglês), que acontece quando o modelo passa a explicar padrões que não são generalizáveis para a população. Um modelo sobreajustado captura a variação gerada pelos erros aleatórios, que, por construção, não pode ser explicada pelos preditores. Sendo assim, o modelo será ótimo para representar a amostra, mas, em geral, péssimo para ser estendido para um contexto mais amplo. 

Uma maneira de evitar esse problema no modelo de regressão linear é utilizar versões do RMSE e do $R^2$ penalizadas pelo número de parâmetros, conhecidas como RMSE ajustado e $R^2$ ajustado. Os valores dessas medidas diminuem quando acrescentamos variáveis que não colaboram muito para explicar a variabilidade de $Y$, o que nos permite controlar o balanço (\textit{trade off}) existente entre um modelo mal ajustado e um modelo sobreajustado. Discutiremos o conceito de sobreajuste com mais detalhes no Capítulo \ref{cap:aprendizado_estatistico}.

Na prática, o coeficiente $R^2$ é muito mais utilizado que o RMSE para a avaliação do ajuste de modelos de regressão linear. Com objetivo de explicar a variabilidade da concentração de ozônio na cidade de São Paulo, \cite{Salvo2014}, por exemplo, ajustaram sete modelos lineares com diferentes preditores para controlar os efeitos meteorológicos e de tráfego e escolheram aquele com maior $R^2$ como o modelo final.

Em alguns casos, a complexidade do fenômeno sob estudo demandará modelos mais flexíveis que o modelo de regressão linear. A seguir, discutiremos os modelos lineares generalizados, uma ampla classe de modelos que permite a utilização de distribuições interessantes para o ajuste de diversos casos práticos, e os modelos aditivos generalizados, que relaxa a suposição de linearidade entre a variável resposta e os preditores.


\section{Modelos lineares generalizados}
\label{cap:glm}

É muito comum na modelagem estatística assumirmos um modelo probabilístico para os dados. O que de fato estamos fazendo é supor que as observações no mundo real estão distribuídas conforme uma distribuição de probabilidades, cujos parâmetros podem ser relacionados com os coeficientes do modelo e estimados, por exemplo, por máxima verossimilhança \citep{Casella2001}.

Para o modelo de regressão linear discutido na última seção, podemos utilizar o método de mínimos quadrados para estimação e, para grandes amostras, existem resultados assintóticos que garantem as propriedades necessárias para a construção de intervalos de confiança e testes de hipóteses para as estimativas. Quando trabalhamos com amostras pequenas, não podemos garantir a validade dos resultados assintóticos, e então precisamos supor que a variável resposta é normalmente distribuída para a construção dos intervalos e testes. Embora muito utilizada na prática, a distribuição Normal pode ser restritiva na prática, pois ela assume que as observações variam na reta real (valores positivos e negativos) e são simetricamente distribuídas em torno da média.

A concentração de poluentes é uma medida positiva, em geral assimétrica e heteroscedástica. Quando estamos trabalhando com dados epidemiológicos, o número de casos de doenças ou mortalidade é uma medida de contagem, isto é, assume apenas valores não-negativos inteiros. Se queremos aplicar modelos que fazem suposições sobre a distribuição de probabilidade das observações, é importante que possamos escolher distribuições compatíveis com a natureza dos dados. Nesses casos, as distribuições Gama e Poisson seriam, respectivamente, boas alternativas para a modelagem de concentração de poluentes e dados epidemiológicos de contagem.

Nesse sentido, os modelos lineares generalizados, introduzidos por \cite{Nelder1972}, são uma generalização do modelo de regressão linear que permitem a utilização de distribuições para dados assimétricos (Gama, Normal inversa, Log-normal), dados de contagem (Poisson, Binomial negativa), dados binários (Binomial), entre outros. Nas próximas seções, discutiremos como utilizar essa classe de modelos para o ajuste de dados de poluição do ar.

\subsection{Especificação do modelo}

Sejam $Y_t$ e $\X$ definidos como na Seção \ref{sec:linear-espec-modelo}. O modelo linear generalizado pode ser definido como

\begin{displaymath}
Y_t|\mathbf{X}_t \stackrel{ind}{\sim} \mathcal{D}(\mu_t, \phi)
\end{displaymath}
\begin{equation}
g(\mu_t) = \alpha + \beta_1 X_{1t} + ... + \beta_p X_{pt}, \quad t = 1, \dots, n,
\label{mod:glm}
\end{equation}
sendo\footnote{A notação $Y_t|\mathbf{X}_t \stackrel{ind}{\sim} \mathcal{D}(\mu_t, \phi)$ significa que, conhecido os valores dos preditores $\mathbf{X_t}$, as variáveis $Y_1, \dots, Y_n$ são independentes e seguem a distribuição $\mathcal{D}$, governada pelos parâmetros $\mu_t$ e $\phi$.}  $\mathcal{D}$ uma distribuição pertencente à família exponencial\footnote{A família exponencial corresponde a uma classe de distribuições de probabilidade que, sob certas condições de regularidade, apresentam algumas características em comum. Essas características permitem que o mesmo \textit{framework} de estimação possa ser utilizado para qualquer uma das distribuições dentro dessa família. Para mais informações, consulte \cite{Gilberto2013}.}, $g(\cdot)$ uma função de ligação, $\mu_t$ um parâmetro de posição e $\phi$ um parâmetro de precisão\footnote{Se $\phi$ e um parâmetro de precisão, $\phi^{-1}$ é um parâmetro de dispersão. Algumas distribuições não têm um parâmetro de precisão. Nas distribuições Binomial e Poisson, por exemplo, $\phi = 1$ e a precisão é uma função da média $\mu$.}. 

%Se $\mathcal{D}$ é a distribuição Normal e $g(\cdot)$ é a função identidade, (\ref{mod:glm}) se reduz ao modelo de regressão linear (\ref{mod:linear}).

Os parâmetros deste modelo podem ser estimados por máxima verossimilhança. Os cálculos envolvem o uso de procedimentos iterativos, como Newton-Raphson e escore de Fisher \citep{Dobson1990}. Distribuições que têm um parâmetro de precisão permitem a modelagem conjunta de $\mu$ e $\phi$. Essa abordagem é conhecida como \textit{modelo linear generalizado duplo} e flexibiliza a suposição de homoscedasticidade feita em (\ref{mod:glm}). Mais informações sobre esses modelos podem ser encontradas em \cite{Gilberto2013}.

A especificação dos termos de tendência e sazonalidade para modelos lineares generalizados pode ser feita da mesma forma que no modelo linear (ver Seção \ref{sec:tend-sazon}). A utilização de resíduos para avaliar a qualidade do ajuste também pode ser conduzida de forma análoga à apresentada nas seções anteriores. Os resíduos mais utilizados em modelos lineares generalizados são definidos a partir da \textit{função desvio}. Uma técnica muito utilizada é a construção de gráficos envelope para investigar a adequação da distribuição escolhida para os dados. Para mais informações sobre a análise de resíduos de modelos lineares generalizados, consulte \cite{Gilberto2013}. 

Os modelos com distribuição Gama, Normal inversa e Log normal são boas alternativas para ajustar dados positivos assimétricos, sendo, em geral, mais adequados para concentrações de poluentes do que a distribuição Normal. Discutiremos os dois primeiros na Seção \ref{sec:glm-assimetricos}.

Dados de contagem, como o número de casos de uma doença ou mortalidade, são usualmente ajustados pelo modelo Poisson. \cite{Gleice2001b}, por exemplo, utilizaram esse modelo para avaliar a associação entre poluição atmosférica e marcadores de mortalidade em idosos na cidade de São Paulo. No entanto, a distribuição Poisson impõe que a média e a variância das observações são iguais e pode não se ajustar bem quando os dados apresentam \textit{sobredispersão} (variância maior que a média). O modelo com resposta binomial negativa é uma alternativa nesses casos, já que permite a modelagem conjunta dos parâmetros de posição e dispersão. Discutiremos esses modelos com mais detalhes na Seção \ref{sec:glm-contagem}.

\subsection{Modelos para dados positivos assimétricos}
\label{sec:glm-assimetricos}

A distribuição Gama costuma ser a principal alternativa para o ajuste de dados positivos assimétricos. Se $Y \sim \textnormal{Gama}(\mu, \phi)$, sendo $\mu > 0$ a média de $Y$, $\phi > 0$ um parâmetro de precisão, a sua função densidade de probabilidade está representada na Figura \ref{fig:gamma-distribution} para $\mu = 1$ e diversos valores de $\phi$. Podemos observar que, à medida que $\phi$ aumenta, a distribuição Gama se torna mais simétrica em torno da média. Conforme $\phi$ tende para infinito, $Y$ se aproxima da distribuição Normal de média $\mu$ e variância $\mu^2\phi^{-1}$, o que torna a distribuição Gama atrativa para a modelagem tanto de observações assimétricas quanto de observações simétricas cuja dispersão varia em função da média ao quadrado.

%\begin{displaymath}
%f(y; \mu, \phi) = \frac{1}{\Gamma(\phi)}\left(\frac{\phi y}{\mu}\right)^\phi \exp \left(-\frac{\phi %y}{\mu}\right)\frac{1}{y}, \quad y > 0,
%\end{displaymath}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{figuras/gamma-distribution}
	\caption{Função densidade da distribuição Gama com $\mu = 1$ e diversos valores de $\phi$. Conforme $\phi$ aumenta, a distribuição se torna menos assimétrica, centralizando-se ao redor da média.}
	\label{fig:gamma-distribution}
\end{figure}

Uma alternativa para a distribuição Gama é a Normal inversa. Considere agora $Y \sim \textnormal{NI}(\mu, \phi)$, novamente sendo $\mu > 0$ a média de $Y$ e $\phi > 0$ um parâmetro de precisão. Podemos ver pela Figura \ref{fig:inverse-normal-distribution} que, para $\mu = 1$, a simetria da distribuição diminui conforme $\phi$ aumenta. Mais precisamente, $Y$ se aproxima de uma distribuição Normal com média $\mu$ e variância $\mu^3 \phi^{-1}$. A Normal inversa é apropriada para modelar tanto observações assimétricas quanto observações simétricas cuja dispersão varia em função da média ao cubo.

%\begin{displaymath}
%f(y; \mu, \phi) = \frac{\phi^{1/2}}{\sqrt{2\pi y^3}}\exp\left\{-\frac{\phi (y - \mu)^2}{2\mu^2y}\right\}, %\quad y > 0.
%\end{displaymath}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{figuras/inverse-normal-distribution}
	\caption{Função densidade da distribuição Normal inversa com $\mu = 1$ e diversos valores de $\phi$. Conforme $\phi$ aumenta, a distribuição se torna menos assimétrica, centralizando-se ao redor da média.}
	\label{fig:inverse-normal-distribution}
\end{figure}

As funções de ligação mais utilizadas em ambos os modelos são a identidade $(g(\mu) = \mu)$, a logarítmica $(g(\mu) = \log(\mu))$ e a recíproca $(g(\mu) = 1/\mu)$. Gráficos de resíduos podem ser feitos para avaliar a adequabilidade da distribuição e da função de ligação escolhidas. Para mais informações sobre análise de diagnóstico para modelos lineares generalizados, consultar \cite{Williams1987} e \cite{Gilberto2013} . 

No R, os modelos Gama e Normal inversa podem ser ajustados com a função \texttt{glm()} do pacote \texttt{stats}, utilizando os argumentos \texttt{family = Gamma} e \texttt{family = inverse.gaussian}, respectivamente. No pacote \texttt{caret}, modelos lineares generalizados podem ser ajustados utilizando a função \texttt{train()} com \texttt{method="glm"}.

Outras distribuições da família exponencial também podem ser utilizadas para a análise de dados positivos assimétricos, como a Weibull, a Pareto e a Log-Normal \citep{Simon2006}. Fora do contexto de modelos lineares generalizados,  a distribuição de Birnbaum-Saunders generalizada (GBS) é outra alternativa para o ajuste de dados positivos assimétricos. \cite{Leiva2008}, por exemplo, utilizaram o modelo GBS para ajustar concentrações horárias de dióxido de enxofre em Santiago, no Chile, mostrando que essa distribuição se ajustava melhor aos dados do que a Log-Normal. Para mais informações sobre a distribuição de Birnbaum-Saunders, consulte \cite{Barros2009} e \cite{Leiva2015}.

\subsection{Modelos para dados de contagem}
\label{sec:glm-contagem}

Em algumas situações, o objetivo do estudo de poluição do ar não está em descrever as séries de poluentes, mas sim utilizá-las para explicar eventos epidemiológicos, como, por exemplo, a morbidade ou mortalidade causada por doenças respiratórias. A variável resposta nesses estudos é, em geral, uma contagem, isto é, assume valores inteiros positivos que representam o número de casos da doença ou de mortes em cada instante observado.

\cite{SchwartzDockery1992}, por exemplo, utilizaram o modelo de Poisson para avaliar a relação entre a concentração de material particulado e o número de mortes no dia seguinte, sugerindo uma associação positiva entre as variáveis. \cite{Gleice2001b} também utilizando o modelo Poisson, estudaram a relação entre a concentração de alguns poluentes e marcadores de mortalidade em idosos na cidade de São Paulo, controlando por variáveis meteorológicas. Os autores observaram uma associação positiva entre mortalidade e níveis de CO, SO$_2$ e, em menor escala, PM10. Já \cite{Saldiva1995} discutiram a utilização de um modelo Poisson para modelar a associação entre concentração de diversos poluentes e a mortalidade em idosos, mas optaram pelo ajuste de um modelo gaussiano, justificando que a aproximação pela distribuição Normal era válida pois a média diária de mortes era suficiente alta (62 eventos por dia).

Se a variável resposta $Y$, segue uma Poisson com parâmetro $\lambda$, simbolicamente $Y \sim \textnormal{Poisson}(\lambda)$, o modelo assume que o evento sob estudo ocorre com taxa $\lambda$ dentro de um intervalo de tempo fixado\footnote{Esse intervalo de tempo se refere à frequência com que os dados são coletados, isto é, se as séries são diárias, semanais, mensais, anuais etc.}. Essa taxa representa o valor médio\footnote{A distribuição de Poisson atribui maiores probabilidades aos valores próximos à média $\lambda$.} de casos observados no intervalo e, na prática, queremos explicá-la a partir de séries de poluentes, controlando por variáveis climáticas. Dessa forma, para o modelo Poisson, temos $\mu_t = \lambda_t$ em (\ref{mod:glm}). A função de ligação mais utilizada nesse contexto é a logarítmica.  

Na distribuição Poisson, a média é igual à variância, isto é, $E(Y) = VAR(Y) = \lambda$. Isso gera uma restrição importante no modelo Poisson, deixando-o inadequado para o ajuste de dados com sobredispersão, observações com a variância maior do que a média\footnote{Para o modelo Poisson, $\phi = 1$.}. Uma alternativa nesse caso é a utilização de modelos com resposta Binomial Negativa.

Se $Y \sim \textnormal{BN}(\mu, \phi)$, temos que $E(Y) = \mu$ e $VAR(Y) = \mu + \mu^2/\phi$, com $\mu \geq 0$ e $\phi > 0$, o que faz a distribuição Binomial Negativa adequada para dados com variância maior do que a média.

Modelos de contagem geralmente são utilizados para obter uma estimativa do risco de mortalidade associado a cada poluente, isto é, qual a variação esperada na taxa de mortalidade se aumentássemos (ou diminuíssemos) a concentração de um poluente em $m$ unidades. Essa quantidade, conhecida como dose-resposta, concentração-resposta ou exposição-resposta, é muito importante para a implementação de medidas para a redução da poluição do ar pois quantifica de forma objetiva o efeito dos poluentes na saúde pública. Outra métrica bastante utilizada são as \textit{funções de impacto na saúde}, discutidas brevemente na Seção  \ref{sec:health-impact-functions}.

No R, o modelo Poisson pode ser ajustado com a função \texttt{glm()} do pacote \texttt{stats}, utilizando o argumento \texttt{family = poisson}, enquanto o modelo com resposta Binomial Negativa utilizando a função \texttt{glm.nb()} do pacote \texttt{MASS}. Utilizando o pacote \texttt{caret}, esses modelos podem ser ajustados utilizando a função \texttt{train()} com \texttt{method="glm"}.

%% ------------------------------------------------------------------------- %%
\section{Modelos aditivos generalizados}
\label{sec:gam}

%\cite{Carslaw2007}
%\cite{Belusic2015}

%Em geral, um modelo de regressão paramétrico assume que a forma de $f$ é conhecida a não ser por um número finito de parâmetros desconhecidos. 

Os modelos lineares têm um papel muito importante na análise de dados, provendo técnicas de inferência e predição computacionalmente simples e de fácil interpretação. Contudo, em problemas reais, a relação entre a variável resposta e os preditores pode não ser linear, tornando os modelos lineares muito restritivos. No estudo de poluentes atmosféricos, por exemplo, o aspecto temporal dos dados gera efeitos sazonais cuja relação com a variável reposta é muito melhor representada por curvas senoidais do que por retas.

Os modelos aditivos generalizados \citep{Hastie2008} são um método integrado, automático e flexível para identificar e caracterizar relações não-lineares entre as variáveis. Ao contrário das estratégias discutidas na Seção \ref{sec:linearidade}, como transformação da variável e regressão polinomial, os modelos aditivos generalizados não são lineares nos parâmetros, permitindo a estimação de funções não-lineares entre os preditores e a resposta. \cite{Belusic2015}, por exemplo, utilizaram essa classe de modelos para avaliar quais as variáveis mais importantes para descrever a série de diversos poluentes em Zagreb, Croácia. O modelo ajustado apontou que as variáveis meteorológicas explicavam a maior proporção da variabilidade dos poluentes.

Neste seção, vamos discutir o ajuste e interpretação dos modelos aditivos generalizados no contexto de estudos de poluição do ar.


\subsection{Especificação do modelo}

O modelo aditivo generalizado é uma extensão do modelo linear generalizado que permite associar cada um dos preditores à variável resposta a partir de funções não-lineares, mantendo a suposição de aditividade (Seção \ref{sec:aditividade}). Como nas seções anteriores, sejam $Y_t$ e $\mathbf{X}_t$ séries temporais que representam, respectivamente, a variável resposta e as variáveis preditoras, com $t = 1, ..., n$. O modelo aditivo generalizado pode ser escrito como

\begin{displaymath}
Y_t|\mathbf{X}_t \stackrel{ind}{\sim} \mathcal{D}(\mu_t, \phi)
\end{displaymath}
\begin{equation}
g(\mu_t) = \beta_0 + f_1(X_{1t}) + \cdots + f_p(X_{pt}),
\label{mod:gam}
\end{equation}
sendo $\mathcal{D}$ uma distribuição pertencente à família exponencial e $f_i$, $i = 1, ..., p$, funções possivelmente não-lineares. No caso mais simples, assim como nos modelos lineares generalizados, supõe-se que as variáveis $Y_t$ são homoscedásticas, independentes e normalmente distribuídas.

Existem diversas propostas sobre como as funções $f_1, ..., f_{p}$ devem ser representadas, incluindo o uso de \textit{splines} naturais, \textit{splines} suavizados e regressão local \citep{Hastie1990}. Outro ponto importante diz respeito à suavidade dessas funções, controlada por \textit{parâmetros de alisamento}, que devem ser determinados a priori\footnote{Uma maneira de determinar valores para esses parâmetros é utilizar validação cruzada, que será discutida no Capítulo \ref{cap:aprendizado_estatistico}.}. Curvas muito suaves podem ser muito restritivas, enquanto curvas muito \textit{rugosas} podem sobreajustar os dados (\textit{overfitting}). Discutiremos esse tema com mais detalhes na Seção \ref{sec:gam-splines}.

O procedimento de estimação no contexto de modelos aditivos generalizados depende da forma escolhida para as funções $f_1, ..., f_{p}$. A utilização de \textit{splines} naturais, por exemplo, permite a aplicação direta de mínimos quadrados, graças à sua construção a partir de \textit{funções base} (ver Seção \ref{sec:gam-splines}). Já para \textit{splines} penalizados, o processo de estimação envolve algoritmos um pouco mais complexos, como \textit{backfitting} \citep{Breiman1985}. Para mais informações sobre a estimação dos parâmetros dos modelos lineares generalizados, consulte \cite{Hastie1990} e \cite{Hastie2008}.

%A expressão (\ref{gam}) considera que a associação com todos os preditores será estimada a partir de funções não-lineares. No entanto, em muitos casos, pode ser interessante considerar o alisamento de apenas algumas variáveis explicativas. Em estudos de poluição, costuma-se alisar os preditores cuja associação com a variável resposta suspeita-se ser não-linear, como o tempo e algumas variáveis climáticas. 

A seguir, introduziremos os conceitos de \textit{splines} e regressão local, e apresentaremos os principais aspectos em torno do ajuste dessas técnicas.

%Nesta abordagem, a escolha dos parâmetros de suavização é crucial. Em geral, os algoritmos de estimação permitem que os próprios dados determinem os parâmetros que melhor se ajustam aos dados. No entanto, há casos em que a estrutura dos dados (sazonal, por exemplo) pede que esses valores sejam fixados. Uma visão geral deste tópico será apresentada na Seção \ref{gam-parametros}.

%Por fim, na Seção \ref{gam-omissos}, discutiremos os problemas gerados pela presença de dados faltantes no processo de estimação, assim como formas de contornar esse problema.

\subsection{Splines e regressão local}
\label{sec:gam-splines}

Para introduzir o conceito de \textit{splines} e regressão local, vamos considerar novamente o modelo mais simples, com apenas uma variável explicativa

\begin{equation}
	Y_t = \beta_0 + \beta_1 X_t + \epsilon_t, \quad t = 1, ..., t.
	\label{mod:gam-simples}
\end{equation}
Uma das principais ideias por trás dos modelos aditivos generalizados está na utilização de \textit{funções bases}. Essa abordagem considera uma família de transformações $b_1(X), b_2(X), ..., b_k(X)$, fixadas e conhecidas, no lugar de $X$ em (\ref{mod:gam-simples}). Assim, o modelo (\ref{mod:gam-simples}) passa a ser
	
\begin{equation}
Y_t = \beta_0 + \beta_1b_1(X_t) + \beta_2b_2(X_t) + \cdots + \beta_kb_k(X_t) + \epsilon_t, \quad t = 1, ..., t,
\label{mod:gam-base}
\end{equation}
que pode assumir diversas classes de associações não-lineares entre $X$ e $Y$. Note que o modelo polinomial apresentado na Seção \ref{sec:linearidade} é um caso particular de (\ref{mod:gam-base}), com $b_j(X_t) = X^j_t$, $j = 1, \dots, k$.  

Como uma tentativa para aumentar a flexibilidade da curva ajustada, podemos segmentar $X$ e ajustar diferentes polinômios de grau $d$ em cada um dos intervalos\footnote{Em contrapartida ao modelo polinomial, que ajusta um único polinômio sobre todo o intervalo de variação de $X$.}. Cada ponto de segmentação é chamado de \textit{nó}, e uma segmentação com $k$ nós gera $k+1$ polinômios. Na Figura \ref{fig:cap-regressao-exemplo-splines} apresentamos um exemplo com polinômios de terceiro grau e 4 nós. Nesse exemplo, a expressão (\ref{mod:gam-base}) tem a forma 

\begin{displaymath}
Y_t = \left\{
\begin{array}{ll}
\beta_{01} + \beta_{11}X_t + \beta_{21}X^2_t + \beta_{31}X_t^3 + \epsilon_t, & \textnormal{se } X_t \leq - 0.5, \\
\beta_{02} + \beta_{12}X_t + \beta_{22}X^2_t + \beta_{32}X_t^3 + \epsilon_t, & \textnormal{se } -0.5 < X_t \leq 0, \\
\beta_{02} + \beta_{13}X_t + \beta_{23}X^2_t + \beta_{33}X_t^3 + \epsilon_t, & \textnormal{se } 0 < X_t \leq 0.5, \\
\beta_{02} + \beta_{14}X_t + \beta_{24}X^2_t + \beta_{34}X_t^3 + \epsilon_t, & \textnormal{se } 0.5 < X_t \leq 1, \\
\beta_{05} + \beta_{15}X_t + \beta_{25}X^2_t + \beta_{35}X_t^3 + \epsilon_t, & \textnormal{se } X_t > 1, \\
\end{array}
\right.
\end{displaymath}
sendo que as funções base $b_1(X), b_2(X), ..., b_k(X)$ nesse caso são construídas com a ajuda de funções indicadoras. Esse modelo é conhecido como modelo polinomial cúbico segmentado.
 

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{figuras/cap-regressao-exemplo-splines.pdf}
	\caption{Polinômios de terceiro grau ajustados em cada segmentação da variável $X$. Os nós são os pontos $x = -0.5$, $x = 0$, $x = 0.5$ e $x = 1$.}
	\label{fig:cap-regressao-exemplo-splines}
\end{figure}

Repare que a curva formada pela junção de cada um dos polinômios na Figura \ref{fig:cap-regressao-exemplo-splines} não é contínua, isto é, apresenta saltos nos nós. Essa característica não é desejável para um modelo ajustado, já que essas descontinuidades não são interpretáveis. Para contornar esse problema, vamos definir um \textit{spline} de grau $d$ como um polinômio segmentado de grau $d$ com as $d-1$ primeiras derivadas contínuas em cada nó. Essa restrição garante a continuidade e suavidade (ausência de vértices) da curva obtida. 

Utilizando a representação por bases (\ref{mod:gam-base}), um \textit{spline} cúbico com $k$ nós pode ser modelado por

\begin{displaymath}
	Y_t = \beta_0 + \beta_1b_1(X_t) + \beta_2b_2(X_t) + ... + \beta_{k+3}b_{k+3}(X_t) + \epsilon_t, \quad t = 1, ..., t,
\end{displaymath}
para uma escolha apropriada de funções $b_1(X), b_2(X), ..., b_{k+3}(X)$. Usualmente, essas funções envolvem três termos polinomiais --- $X$, $X^2$ e $X^3$, mais precisamente --- e $k$ termos $h(X, c_1), \dots, h(X, c_k)$ da forma

\begin{displaymath}
h(X, c_j) = (x - c_j)^3_+ = \left\{
\begin{array}{cl}
(x - c_j)^3, & \textnormal{se } x < c_j, \\
0, & \textnormal{em caso contrário,}
\end{array}
\right.
\end{displaymath}
sendo $c_1, \dots, c_k$ os $k$ nós. Assim, incluindo o termo $\beta_0$, o ajuste de um \textit{spline} cúbico com $k$ nós envolve a estimação de $k+4$ parâmetros e, portanto, utiliza $k+4$ graus de liberdade. Mais detalhes sobre a construção dessas restrições podem ser encontrados em \cite{Hastie2008} e \cite{James2013}.

Além das restrições sobre as derivadas, podemos adicionar \textit{restrições de fronteira}, exigindo que a função seja linear na região de $X$ abaixo do menor nó e acima do maior nó. Essas restrições diminuem a variância nos extremos do preditor, produzindo estimativas mais estáveis. Um \textit{spline} cúbico com restrições de fronteira é chamado de \textit{spline natural}.

No ajuste de \textit{splines} cúbicos ou naturais, o número de nós determina o grau de suavidade da curva, e a sua escolha pode ser feita por \textit{validação cruzada} \citep{James2013}. De uma forma geral, a maior parte dos nós é posicionada nas regiões do preditor com mais informação, isto é, mais observações. Por pragmatismo, para modelos com mais de uma variável explicativa, costuma-se adotar o mesmo número de nós para todos os preditores.

Os \textit{splines suavizados} constituem uma classe de funções suavizadoras que não utilizam a abordagem por funções bases. De maneira resumida, um \textit{spline} suavizado é uma função $f$ que minimiza a seguinte expressão

\begin{equation}
	\sum_{i = 1}^n (Y_i - f(X_i))^2 + \lambda \int f''(u)^2 du.
\end{equation}
O primeiro termo dessa expressão garante que $f$ se ajustará bem aos dados, enquanto o segundo penaliza a sua variabilidade, isto é, controla o quanto $f$ será suave. A suavidade é regulada pelo parâmetro $\lambda$, sendo que $f$ se torna mais suave conforme $\lambda$ cresce. A escolha desse parâmetro é geralmente feita por validação cruzada.

Uma outra forma para ajustar funções não-lineares entre $X$ e $Y$ é a regressão local. Essencialmente, essa técnica consiste em ajustar modelos de regressão simples em regiões de pontos ao redor de cada observação $x_0$ do preditor $X$. Essas regiões são formadas pelos $k$ pontos mais próximos de $x_0$, sendo que o parâmetro $s = k/n$, determina o quão suave ou rugosa será a curva ajustada. O ajuste é feito por mínimos quadrados ponderados, e os pesos são inversamente proporcionais à distância do ponto em relação a $x_0$. Assim, os pontos na vizinhança de $x_0$ mais afastados recebem peso menor. 

No R, modelos lineares generalizados podem ser ajustados utilizando-se a função \texttt{gam()} do pacote \texttt{mgcv}. Essa função permite a utilização de \textit{splines} como função suavizadora. Para a utilização de regressão local, é necessário usar a função \texttt{gam()} do pacote \texttt{gam}. Também é possível utilizar o pacote \texttt{caret}, a partir da função \texttt{train()} e \texttt{method = "gam"}.

Para mais informações sobre \textit{splines}, regressão local e modelos lineares aditivos em geral, consultar \cite{Hastie2008} e \cite{James2013}.

\section{Modelos de séries temporais}

Às vezes, queremos explicar a série $Y$ apenas por seus valores defasados no tempo (autocorrelação) ou pelos valores defasados dos preditores $X_{1}, \dots, X_{p}$ (correlação cruzada). Como os modelos de regressão apresentados até aqui permitem que $Y$ seja influenciada apenas por valores contemporâneos das variáveis explicativas, eles podem ser insuficientes para explicar toda as relações temporais presente em uma série. 

Nesta seção, vamos introduzir a classe de modelos ARIMA \citep{Box1970}, que contemplam a correlação gerada por relações lineares entre observações defasadas no tempo da própria variável. A associação entre a variável resposta e valores defasados no tempo de covariáveis não será tratada aqui, mas são contemplados por modelos de regressão defasada (\textit{lagged regression}), discutidos nas seções 4.10 e 5.6 de \cite{Shumway2006}. 

\subsection{Modelos autorregressivos (AR)}

Modelos autorregressivos se baseiam na ideia de que $Y_t$ pode ser explicada como uma função de $p$ valores passados $Y_{t-1}, \dots, Y_{t-p}$, sendo $p$ o número de passos no passado necessários para prever o valor no instante $t$. Se $Y_t$ é uma série estacionária, o modelo autorregressivo de ordem $P$, abreviado como AR($p$), é definido como

\begin{equation}
	Y_t = \phi_1 Y_{t-1} + \cdots + \phi_p Y_{t-p} + w_t,
\label{mod:AR}
\end{equation}
sendo $\phi_1, \dots, \phi_p$ constantes com $\phi_p \neq 0$ e $w_t \sim N(0, \sigma^2_w)$, $t \geq 0$. Sem perda de generalidade, assume-se que a média de $Y_t$ é zero\footnote{Se a média de $Y_t$ é $\mu \neq 0$, então o modelo é definido para $Y_t - \mu$, o que equivale a acrescentar um intercepto $\alpha = \mu(1 - \phi_1 - \dots \phi_p)$ ao modelo (\ref{mod:AR}).}.

Os modelos AR($p$) são muito utilizados em Economia, onde é natural pensar o valor de alguma variável no instante $t$ como função de seus valores defasados, e em algumas áreas da Física e Geofísica, onde os estimadores auto-regressivos são utilizados para estimar o espectro de certos processos. 

\subsection{Modelos autorregressivos e de médias móveis (ARMA)}

Uma alternativa para o modelo AR($p$) é o modelo de médias móveis de ordem $q$. Esse modelo assume que $Y_t$ é gerado a partir de uma combinação linear dos erros $w_t, w_{t-1}, \dots, w_{t-q}$. Formalmente, o modelo de médias móveis de ordem $q$, MA($q$), é definido como

\begin{equation}
Y_t = w_t + \theta_1 w_{t-1} + \cdots + \theta_q w_{t-q},
\label{mod:MA}
\end{equation}
sendo $\theta_1, \dots, \theta_q$ constantes com $\theta_q \neq 0$ e $w_t \sim N(0, \sigma^2_w)$, $t \geq 0$. 

Ao contrário dos modelos auto-regressivos, representar um processo por um modelo de médias móveis puro parece não ser intuitivo.

A utilização de modelos com termos auto-regressivos e de médias móveis pode ser uma boa alternativa para muitas séries encontradas na prática, pois eles normalmente requerem um menor número de parâmetros para explicar a autocorrelação da série \citep{Morettin2004}. Nesse sentido, dizemos que uma série temporal $Y_t$ é ARMA($p$, $q$) se ela é estacionária e se

\begin{equation}
Y_t = \phi_1 Y_{t-1} + \cdots + \phi_p Y_{t-p} + w_t + \theta_1 w_{t-1} + \cdots + \theta_q Y_{t-q},
\label{mod:ARMA}
\end{equation}
com $\phi_p \neq 0$, $\theta_q \neq 0$ e $\sigma^2_w > 0$.

Repare que os modelos AR($p$) e MA($q$) são casos particulares do ARMA($p$, $q$), com $q = 0$ e $p = 0$ respectivamente.

A estimação dos parâmetros $(\phi_1, \dots, \phi_p)$ e $(\theta_1, \dots, \theta_q)$ pode ser feita por máxima verossimilhança ou pelo método de mínimos quadrados. Para mais informações, consulte a Seção 3.6 de \cite{Shumway2006}.

As três classes de modelos apresentadas até aqui consideram que a série $Y_t$ é estacionária, o que normalmente não acontece na prática. Para flexibilizar essa restrição, apresentaremos a seguir os modelos ARIMA($p$, $d$, $q$), uma extensão da classe ARMA que considera a diferenciação de grau $d$ da série para eliminar a não-estacionariedade.

\subsection{Modelos autorregressivos integrados e de médias móveis (ARIMA)}
\label{sec:arima}

Vimos na Seção \ref{sec:tend-sazon} que séries não-estacionárias podem ser diferenciadas para se alcançar a estacionariedade. De maneira geral, essa estratégia é válida para séries que não apresentam \textit{comportamento explosivo} ou, em outros termos, que apresentam alguma homogeneidade em seu comportamento não-estacionário. \cite{Morettin2004} enquadram séries dessa natureza, chamadas de \textit{séries não-estacionárias homogêneas}, em dois grupos:

\begin{itemize}
	\item séries que oscilam ao redor de um nível médio durante algum tempo e depois saltam para outro nível temporário; e
	\item séries que oscilam em uma direção por algum tempo e depois mudam para outra direção temporária.
\end{itemize}
O primeiro tipo requer apenas uma diferença para torná-las estacionária, enquanto o segundo requer duas. Dessa forma, a série não-estacionária homogênea $Y_t$ é dita ser ARIMA($p$, $d$, $q$) se $\Delta^d Y_t$, como definido em (\ref{def:diff}), é ARMA($p$, $q$).

Como discutido na Seção 3.8 de \cite{Shumway2006} e no Capítulo 6 de \cite{Morettin2004}, precisamos seguir alguns passos essenciais no ajuste de modelos ARIMA:

\begin{enumerate}
	\item Construir o gráfico da série.
	\item Transformar a série, se preciso.
	\item Identificar a ordem de dependência do modelo.
	\item Estimar os parâmetros.
	\item Diagnóstico.
	\item Selecionar o melhor modelo.
\end{enumerate}

No primeiro passo, podemos encontrar anomalias, como heteroscedasticidade, a partir da gráfico da série contra o tempo. No passo 2, corrigimos essas anomalias utilizando alguma transformação. 

No passo 3, precisamos identificar as ordens $p$, $d$ e $q$ do modelo. O próprio gráfico da série irá sugerir se alguma diferenciação será necessária. Se alguma diferenciação for realizada, calculamos $\Delta Y_t$, $t = 2, \dots, n$, e checamos no gráfico da série  $\Delta Y_t$
contra o tempo $t$ se outra diferenciação é necessária. Continuamos esse processo, sempre checando os gráficos da série diferenciada contra o tempo\footnote{Cuidado para não introduzir dependência onde não existe. Por exemplo, $Y_t = w_t$ é serialmente não-correlacionada, mas $\Delta Y_t = w_t - w_{t-1}$ é MA(1).}.

Com o valor de $d$ selecionado, observamos o gráfico da função de autocorrelação amostral e da função de autocorrelação parcial amostral de $\Delta^d Y_t$. Sugestões para os valores de $p$ e $q$ podem ser encontrados segundo os critérios apresentados na Tabela \ref{tab:behavior_ACF_PACF_ARMA}.

\begin{table}[h!]
	\centering
	\caption{Critérios para a escolha da ordem de modelos ARIMA.}
	\begin{tabular}{c|c|c|c}
		\hline 
		& AR($p$) & MA($q$) & ARMA($p$, $q$) \\ 
		\hline 
		ACF & Calda longa & Desaparece após o \textit{lag} $q$ & Calda longa \\ 
		PACF & Desaparece após o \textit{lag} $p$ & Calda longa  & Calda longa  \\ 
		\hline 	
	\end{tabular}
\label{tab:behavior_ACF_PACF_ARMA}
\end{table}

A ideia nesse passo é, a partir dos gráficos da função de autocorrelação e autocorrelação parcial, escolher alguns valores para $p$, $d$ e $q$ e, no passo 4, ajustar os respectivos modelos. Assim, a partir da análise de diagnóstico realizada no passo 5, selecionar o modelo que melhor se ajustou aos dados no passo 6.

A classe ARIMA pode ser generalizada para incluir o ajuste da sazonalidade. Essa nova classe, conhecida como SARIMA, inclui termos autoregressivos e de médias móveis para termos separados por \textit{lags} de tamanho $s$. Para mais informações, recomendamos a leitura do Capítulo 10 de \cite{Morettin2004} e da Seção 3.9 de \cite{Shumway2006}.  

Na linguagem R, uma maneira conveniente de se ajustar um modelo ARIMA é utilizar a função \texttt{auto.arima()} do pacote \texttt{forecast}. O algoritmo construído nessa função retorna o melhor modelo ARIMA com base em alguma métrica de qualidade de ajuste do modelo\footnote{As opções disponíveis são AIC, AICc ou BIC.}, ajustando todas as combinações de valores para $p$, $d$ e $q$ segundo alguma restrição (combinação de todas as ordens menores que 5, por exemplo).

%\subsection{Modelos de função de transferência}

%Em muitos estudos de séries temporais, não conseguimos explicar a variabilidade de uma variável apenas pelos seus próprios valores defasados no tempo. A classe ARIMA apresentada nas últimas seções não contempla a inclusão de covariáveis, o que pode ser um grande impeditivo para o uso desses modelos.

%Modelos de função de transferência descrevem a relação entre os preditores e a variável resposta de um fenômeno usando uma razão de polinômios, sendo a ordem do modelo igual à ordem do polinômio do denominador.

\section{Modelos não-supervisionados}

Os modelos apresentados nas seções anteriores são chamados de supervisionados pois uma variável resposta \textit{supervisiona} o aprendizado sobre fenômeno de interesse. Em alguns problemas, nós não temos acesso à variável resposta, nos restando buscar informação sobre o fenômeno de interesse apenas a partir da correlação entre os preditores. Essa estratégia é chamada de \textit{análise não-supervisionada} e é geralmente utilizada para realizar agrupamentos e redução de dimensionalidade.

Devido ao seu carácter subjetivo e a impossibilidade de testar a qualidade dos ajustes, as conclusões de uma análise não-supervisionada devem ser tratadas com o mesmo cuidado de uma análise exploratória.

Em estudos de poluição do ar, modelos não-supervisionados são utilizados principalmente para a detecção de fontes de poluentes, isto é, dadas as medidas de concentração de diversos poluentes ao longo de um período, formamos grupos com as emissões mais correlacionadas e, a partir de inventários de poluição e conhecimento teórico, identificamos quais fontes podem ser representadas por cada grupo \citep{Thurston1985, Buhr1992, Chavent2009}. Em estudos epidemiológicos, essas técnicas também podem ser utilizados para determinar quais doenças são as principais causas de internação em hospitais \citep{Tecer2009}. 

A seguir, apresentaremos a análise de componentes principais e a análise fatorial, dois modelos não-supervisionados bastante utilizados em estudos de poluição do ar.

\subsection{Análise de componentes principais}

Suponha que queiramos investigar a concentração de dois poluentes, digamos $X_1$ e $X_2$. Dada uma amostra de tamanho $n$ dessas variáveis, para explorar esses dados descritivamente, poderíamos construir um gráfico de dispersão de $X_1$ contra $X_2$ e, a partir dele, observar tanto a variabilidade desses poluentes quanto como eles estão correlacionados. Se eles apresentarem correlação positiva, teríamos indícios de que eles são gerados pela mesma fonte ou sob as mesmas condições atmosféricas.

Suponha agora que, em vez de 2 poluentes, tivéssemos 10. Para construir gráficos de dispersão para todas as combinações dois a dois, precisaríamos analisar 45 gráficos, sendo que cada um deles só traria uma pequena parte da informação contida nos dados, pois estaríamos ignorando possíveis interações entre as variáveis.

Em geral, para $p$ poluentes, gostaríamos de uma maneira de visualizar o máximo possível da informação contida no espaço $p$-dimensional gerado pelos preditores $X_1, \dots, X_p$ em uma representação (gráfica) com poucas (duas) dimensões. Esse é o objetivo da análise de componentes principais.

Dado um conjunto de preditores $X_1, X_2, \dots, X_p$, a análise de componentes principais visa encontrar uma projeção ortogonal $Z_1, Z_2, \dots, Z_p$, tal que

\[
VAR(Z_1) \geq VAR(Z_2) \geq \cdots, VAR(Z_p).
\]
Isso implica que, em geral, com apenas as primeiras variáveis $Z_1, Z_2, \dots, Z_p$, digamos $Z_1$ e $Z_2$, podemos explicar a maior parte da variabilidade dos preditores $X_1, X_2, \dots, X_p$. Assim, $Z_1$ e $Z_2$ representariam em apenas 2 dimensões a maior parte da informação contida nos dados originais.

Cada variável $Z_i$, chamada de $i$-ésima componente principal, é uma combinação linear dos preditores $X_1, X_2, \dots, X_p$, isto é,

\begin{equation}
Z_i = \phi_{1i} X_1 + \phi_{2i} X_2 + \cdots \phi_{pi} X_p,
\end{equation}
sendo que os elementos $\phi_{1i}, \dots, \phi_{pi}$ são os \textit{pesos} do $i$-ésimo componente principal. Como esses pesos são normalizados, $\sum_{j = 1}^p{\phi_{ji}^2} = 1$, temos que $\phi_{ji} < 1$, para todo $j = 1, \dots, p$. Assim, os  $\phi_{1i}, \dots, \phi_{pi}$ próximos de 1 indicam preditores positivamente associados e cuja variabilidade está sendo representada por $Z_i$.

Como $Z_1, Z_2, \dots, Z_p$ representa uma projeção ortogonal, cada par de componentes ($Z_i$, $Z_j$) é não-correlacionado. Dessa forma, o componente $Z_2$, por exemplo, é a combinação linear de $X_1, X_2, \dots, X_p$ de maior variância entre todas as combinações lineares que são não-correlacionadas com $Z_1$. Isso quer dizer que as fontes de variação representadas por $Z_2$ são não-correlacionadas com as encontradas em $Z_1$.

Voltando ao nosso exemplo com os poluentes, se a análise de componentes principais indicasse os poluentes $X_1$, $X_3$ e $X_5$ como aqueles com maiores pesos para o componente $Z_1$ e os poluentes $X_2$, $X_3$ e $X_4$ como aqueles com maior peso para o componente $Z_2$, então esses dois grupos de poluentes caracterizariam duas fontes não-correlacionadas de poluição\footnote{ A identificação da fonte deve então ser feita a partir dos poluentes que a compõe utilizando conhecimento teórico sobre emissões, inventários de emissão e resultados de estudo anteriores.}.

Os cálculos por trás da análise de componentes principais envolvem decomposição espectral \citep{Nicholson2006}, uma técnica de álgebra linear para decompor matrizes em função de seus autovetores e autovalores. 

Na linguagem R, podemos realizar uma análise de componentes principais utilizando a função \texttt{prcomp()} do pacote \texttt{stats}.

\subsection{Análise Fatorial}

Assim como a análise de componentes principais, a análise fatorial também pode ser utilizada para redução de dimensionalidade. A segunda técnica difere da primeira em dois pontos principais. Primeiro, a análise fatorial supõe que a variância e covariância contida em um conjunto de variáveis $X_1, X_2, \dots, X_p$ pode ser explicada por um conjunto menor de fatores latentes (não-observáveis). Se esses fatores são definidos a priori, o modelo pode ser utilizado para testar teorias sobre a relação entre os fatores e as variáveis observadas. O segundo ponto diz respeito à inclusão de erros aleatórios. Enquanto a análise de componentes principais calcula um novo conjunto de variáveis que explica 100\% da variabilidade das variáveis originais, a análise fatorial considera que parte da variabilidade das variáveis originais pode ser explicada pelos fatores latentes, mas uma outra parte é devida a ruído aleatório.

No exemplo da seção anterior, no qual queremos identificar fontes de emissão a partir da concentração de poluentes medidas ao longo de um determinado período, poderíamos pensar em cada fonte como um fator latente, que pode ser representado por uma combinação linear das concentrações observadas. A inclusão de erros aleatórios seria justificada, por exemplo, pela variação nas concentrações causadas por condições atmosféricas.

Dada uma amostra de tamanho $n$ dos preditores $X_1, X_2, \dots, X_p$, a análise fatorial procura estimar os pesos $l_{ij}$, $i = 1, \dots, p$ e $j = 1, \dots, m$, tais que

\[
Z_{ik} = l_{i1}F_{1k} + \cdots l_{im}F_{mk} + \epsilon_{ik},
\]
sendo que, para a $k$-ésima observação da amostra, $Z_{ik} = \frac{X_{ik} - \bar{X}_i}{\sigma_i}$ é o $i$-ésimo preditor normalizado, $F_{jk}$ é o $j$-ésimo fator e $\epsilon_{ik}$ é o erro associado ao $i$-ésimo preditor.

O número de fatores $m$ deve ser escolhido antes do ajuste, sendo que a interpretação de cada fator é externa ao modelo. Mesmo quando cada fator é construído teoricamente a priori, o modelo não indica qual dos termos $F_{jk}$ representa cada fator.

Mais informações sobre análise fatorial, como o processo de estimação e interpretação geométrica, podem ser encontradas em \cite{Everitt2011}. Na linguagem R, esse modelo pode ser ajustado utilizando a função \texttt{factanal} do pacote \texttt{stats}.

\section{Outros modelos}

\subsection{Modelos mistos}

Os modelos mistos (ou modelos de coeficientes aleatórios) foram introduzidos por \cite{Fisher1918} para estudar a correlação de traços hereditários em pais, mães e filhos. Eles são particularmente úteis em estudos clínicos, ensaios biológicos e estudos sociais, nos quais variáveis medidas em uma mesma unidade amostral (medidas repetidas) ou unidades amostrais com agrupamentos naturais\footnote{Como membros de uma mesma família, pacientes de um mesmo hospital, animais de uma mesma ninhada ou moradores de uma mesma região.} (dados hierárquicos) geram observações correlacionadas.

Essa classe de modelos utiliza \textit{coeficientes aleatórios} para controlar os efeitos individuais e de grupo que não podem ser observados\footnote{Como carga genética, criação, o efeito do atendimento por um mesmo médico, níveis de poluição do ar de uma mesma região etc.}. Ao contrário dos coeficientes fixos dos modelos de regressão vistos até aqui, que modelam a média da variável resposta, os efeitos aleatórios introduzem uma estrutura para a variância de $Y$, contemplando a correlação entre observações de um mesmo grupo e as diferentes variâncias de observações de diferentes grupos.

Em estudos de poluição do ar, modelos mistos são usados principalmente para controlar variações individuais em estudos epidemiológicos longitudinais. \cite{Liao1999}, por exemplo, avaliaram a resposta autonômica cardiovascular a variações diárias de material particulado 2.5 em 26 idosos durante 3 semanas consecutivas, mostrando que o aumento dos níveis de MP2.5 estão associados a um menor controle autonômico cardíaca. \cite{Coull2001} utilizaram um modelo aditivo misto para associar mudanças na concentração de poluentes atmosféricos com a função pulmonar de crianças ao longo de 109 dias. Eles mostraram que o modelo misto era o mais adequado para contemplar a heterogeneidade populacional da suscetibilidade à poluição. Já \cite{Chuang2011} avaliaram o uso de um componente aleatório para os graus de liberdade de um modelo aditivo Poisson em estudos epidemiológicos de poluição do ar. Os autores observaram que esse modelo gerava erros-padrão maiores do que o modelo aditivo usual e concluíram que um alisamento variável da função não-linear em conjunto com essa maior variabilidade poderia refletir melhor a realidade.

Mais informações sobre modelos mistos, como especificação do modelo, estimação e outros exemplos, podem ser encontradas em \cite{Singer2012} e \cite{Galecki2013}.

Na linguagem R, modelos mistos podem ser ajustados a partir das funções dos pacotes \texttt{nlme} e \texttt{lme4}.

\subsection{Modelos GARCH}

Os modelos para séries temporais apresentados até aqui são utilizados para modelar a média condicional de um processo quando a variância condicional (volatilidade) é constante. Em muitos problemas, contudo, a suposição de homoscedasticidade pode não ser verdadeira.

Os modelos autoregressivos com heteroscedasticidade condicional (ARCH), propostos por \cite{Engle1982}, foram desenvolvidos para contemplar mudanças da volatilidade da série. Se $\epsilon_t \sim N(0, 1)$, para $t = 1, \dots, n$, o modelo ARCH($q$) é definido por

\[
Y_t = f(\mathbf{X}, \mathbf{Y}) + \sigma_t\epsilon_t
\]
\begin{equation}	
	\sigma_t^2 = \alpha_0 + \alpha_1 \epsilon_{t-1}^2 + \dots \epsilon_q \epsilon_{t-q}^2,
	\label{mod:ARCH}
\end{equation}
com $\alpha_0 > 0$ e $\alpha_i \geq 0$, $i > 0$, sendo $f(\mathbf{X}, \mathbf{Y})$ uma função dos preditores $\{(X_{1i}, \dots, X_{pi}),  i \leq t\}$ e das variáveis defasadas $(Y_1, \dots, Y_{t-1})$. Repare a primeira expressão de (\ref{mod:ARCH}) permite o ajuste de diversas classes de modelo para a média condicional de $Y_t$, como modelos de regressão linear, modelos ARIMA e modelos de função de transferência, enquanto a segunda impõe um modelo autorregressivo de ordem $p$ para a volatilidade do processo.

\cite{Bollerslev1986} estendeu a classe ARCH, propondo os GARCH (\textit{generalized} ARCH). Essa nova classe permite o ajuste de um modelo ARMA para a variância do erro ($\sigma^2$), modelando a volatilidade da série com menos parâmetros que um modelo ARCH \citep{Morettin2004}. Esse modelo pode ser expresso por

\[
Y_t = f(\mathbf{X}, \mathbf{Y}) + \sigma_t\epsilon_t
\]
\begin{equation}
	\sigma_t^2 = \alpha_0 + \alpha_1 \epsilon_{t-1}^2 + \dots \alpha_q \epsilon_{t-q}^2 + \beta_1 \sigma^2_{t-1} + \dots + \beta_p \sigma^2_{t-p},
	\label{mod:GARCH}
\end{equation}
sendo $f(\mathbf{X}, \mathbf{Y})$ definida como antes. 

Por ser um modelo com muitos parâmetros, a especificação do modelo GARCH($p$, $q$), geralmente é dividida em três passos:

\begin{enumerate}
	\item Estimar o melhor modelo AR($q$):
	\[
	Y_t = a_0 + a_1 Y_{t-1} + \cdots a_q Y_{t-q} + \epsilon_t
	\]
	\item Calcular e construir o gráfico das autocorrelações de $\epsilon^2$, dadas por
	\[
	\rho_i = \frac{\sum_{t = i + 1}^T (\hat{\epsilon}_t^2 - \hat{\sigma}_t^2)(\hat{\epsilon}_{t-1}^2 - \hat{\sigma}_{t-1}^2)}{\sum_{t = 1}^T(\hat{\epsilon}_t^2 - \hat{\sigma}_t^2)^2},
	\]
	sendo $T$ o tamanho amostral.
	\item Avaliar valores de $\rho_i$ maiores que $1/\sqrt{T}$.
\end{enumerate}

A estimação desses modelos pode ser conduzida da mesma forma que para os modelos ARMA, discutida na Seção 3.6 de \cite{Shumway2006}. Na linguagem R, esse modelo pode ser ajustado usando a função \texttt{garch()} do pacote \texttt{tseries}.


Em estudos de poluição do ar, modelos GARCH são utilizados principalmente para aprimorar modelos de previsão, como feito por \cite{Kumar2010} para dados de ozônio em Bruxelas e Londres.

\subsection{Modelos dinâmicos}

Estudos de poluição atmosférica envolvem dados cuja coleta é naturalmente suscetível à omissão. A medição de poluentes e de variáveis meteorológicas, por exemplo, envolve equipamentos que estão sujeitos a imprecisões, falhas e precisam ser constantemente regulados. Esses dados geralmente são sustentados pela administração pública, cuja redução de verbas pode descontinuar ou reduzir os planos de coleta.

Às vezes, o próprio delineamento do estudo gera dados faltantes. Na análise feita por \cite{Salvo2014}, os autores descartaram da amostra os meses frios (julho à setembro), devido à menor formação de ozônio nesse período. Por causa da influência do tráfego no estudo, os feriados e fins de semanas também não foram considerados. Essas exclusões geraram uma série com "buracos", inviabilizando a aplicação de modelos que fazem a suposição de observações equidistantes, como os modelos ARIMA apresentados anteriormente.

Os modelos modelos lineares dinâmicos (ou espaço-estado ou filtros de Kalman), introduzidos por \cite{Kalman1960} e \cite{Kalman1961}, são uma alternativa nesses casos. Eles são caracterizados por duas suposições principais. A primeira afirma que a verdadeira variável sob estudo, $U_t$, é um fenômeno não-observável. Neste caso, o que realmente observamos é uma transformação linear desse fenômeno, $A_tU_t$, acrescida de um ruído, $v_t$. A segunda suposição diz respeito sobre o processo de geração de $U_t$. Mais precisamente, na sua forma mais básica, temos que $U_t$ é gerado por um processo autoregressivo de primeira ordem.

Dadas essas duas suposições, podemos escrever o modelo de espaço-estado da seguinte maneira

\begin{displaymath}
Y_t = a_tU_t + v_t
\end{displaymath}
\begin{equation}
U_t = \phi U_{t-1} + w_t,
\label{mod:espaco-estado}
\end{equation}
sendo $a_t$ e $\phi$ parâmetros do modelo e $w_t \sim N(0, \sigma_w)$. A primeira equação em (\ref{mod:espaco-estado}) é chamada de \textit{equação de estado}, enquanto a segunda é chamada de \textit{equação de observação}.

A aplicação desses modelos nos permite ajustar a série a partir de suas observações passadas, como nos modelos ARIMA, mas, a cada passo (instante), incorporamos informação de um processo externo, que pode ser tanto a informação de variáveis explicativas quanto de outros processos autorregressivos. Assim, valores omissos no instante $t$ são estimados a partir da informação contida em $1, \dots, t-1$, sendo uma maneira natural e integrada para lidar com os buracos da série.

\cite{Collet2008} apresentam um famoso uso da aplicação de modelos dinâmicos para previsão de consumo de energia elétrica na França. Mais informações sobre o modelo, no contexto de poluição do ar, podem ser encontradas no capítulo 12.3 de \cite{Zannetti1990}. 

%\textsc{\subsection{Outros tópicos de modelagem}

%Nesta seção, apresentaremos alguns tópicos adicionais sobre modelagem de dados de poluição.

%\subsubsection{Funções de impacto na saúde}
%\label{sec:health-impact-functions}

%\cite{Chang2017} utilizaram funções de dose-resposta para avaliar a associação da mortalidade com as %concentrações de material particulado próximas a estradas na região central da Carolina do Norte (EUA). %Eles concluíram que 72\% das mortes prematuras associadas à exposição de PM2.5 aconteciam em um raio de 1km %de grandes estradas, onde cerca de 50\% da população vivia.

%Referências de funções de impacto na saúde: \cite{Chang2017}.}