%% ------------------------------------------------------------------------- %%
\chapter{Técnicas de aprendizado estatístico}
\label{cap:aprendizado_estatistico}

\begin{flushright}
	\textcolor{gray}{
		\begin{tabular}{r}
			There are no routine statistical questions, \\
			only questionable statistical routines. \\
			--- Sir David Cox			
		\end{tabular} 
	}
	\vspace{5mm}
\end{flushright}

No Capítulo anterior, discutimos diversas classes de modelos úteis para o ajuste de séries temporais dentro do contexto de poluição do ar. A escolha do modelo, e tudo que envolve essa tarefa, faz parte de um \textit{framework} maior, muitas vezes chamado de \textit{aprendizado estatístico}. O aprendizado estatístico pode ser definido como um conjunto de técnicas utilizadas para extrair informação dos dados. Assim, além da modelagem, também podemos incluir a análise exploratória, discutida no Capítulo \ref{cap:analise-exploratoria}, e as ferramentas que sugerem se o modelo ajustado é ou não uma boa escolha para representar o fenômeno sob estudo. Essas técnicas serão o tema deste Capítulo.

Mais precisamente, discutiremos métodos de reamostragem, seleção de variáveis e regularização. Os métodos de reamostragem geralmente são utilizados para avaliar a performance de um modelo ou prover medidas de acurácia para as estimativas dos parâmetros. Dada uma classe de modelos, os métodos de seleção de variáveis e regularização permitem aumentarmos o poder preditivo e a interpretabilidade do modelo.

\section{Métodos de reamostragem}

- Introdução

	- O que é?
	
	- Para que serve?
	
- Falaremos de validação cruzada e bootstrap

\subsection{Validação cruzada}

Uma forma de avaliar a qualidade do ajuste de um modelo é observar o seu \textit{erro de teste}. O erro de teste é o erro médio resultante da utilização do modelo escolhido para prever a resposta em novas observações, que não foram utilizadas para estimar os parâmetros do modelo \citep{James2013}.

Como na maioria dos estudos não é possível obter facilmente novas observações, podemos calcular o erro de teste dividindo a amostra principal em duas partes: uma utilizada para o ajuste do modelo e a outra para o cálculo do erro de teste, como se fosse um conjunto de novas observações. Essa técnica é conhecida como \textit{validação cruzada} \citep{James2013}. Há diversos tipos de validação cruzada, que variam a depender da forma utilizada para dividir a amostra. Nesta seção, apresentaremos os principais tipos de validação cruzada e discutiremos as vantagens e desvantagens de cada uma.

%Quando precisamos escolher valores para hiperparâmetros do modelo, como o grau de suavização de um modelo aditivo generalizado (Seção \ref{sec:gam}) ou o $\lambda$ do LASSO (Seção \ref{sec:lasso}), podemos dividir a amostra em ainda mais uma parte: uma amostra de validação. Nesses casos, os modelos são treinados com a amostra de teste e, para diversos valores do hiperparâmetro, calculamos o erro de teste na amostra de validação. Escolhemos então o hiperparâmetro com menor erro de teste e utilizamos a amostra de teste para calcular o erro de teste do modelo final. Repare que a amostra de teste nunca é utilizada para ajustar o modelo.

\subsubsection{Amostra de validação}

A amostra de validação é a forma mais simples de validação cruzada. A estratégia aqui consiste em dividir aleatoriamente as observações em uma amostra de treino e uma amostra de validação. A proporção de observações em cada uma depende do tamanho amostral. Costuma-se utilizar 30\% da amostra para validação, mas esse número pode ser menor em amostras muito grandes (mais de 100 mil observações, por exemplo).

Conforme discutido em \cite{James2013}, a amostra de validação tem duas desvantagens em potencial:

\begin{itemize}
	\item a estimativa do erro de teste pode ter alta variância, dependendo de quais observações ficaram na amostra de treino e quais fiaram na amostra de validação;
	\item como a acurácia de modelos estatísticos é menor quando ajustados com menos observações e apenas parte das observações são utilizadas no modelo final, a estimativa do erro de teste pode superestimada. 
\end{itemize}

A seguir, apresentamos um tipo de validação cruzada que não possui essas limitações.

\subsubsection{LOOCV}

Considere uma amostra com $n$ observações. A validação cruzada \textit{leave-one-out} (LOOCV) consiste em rodar o modelo escolhido $n$ vezes, sendo que em cada ajuste nós deixamos a $i$-ésima observação de fora, $i = 1, \dots, n$, e a utilizamos para calcular o erro de teste. A estimativa final do erro de teste será então a média das $n$ estimativas parciais. 


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{figuras/cap-aprend-estat-loocv.png}
	\caption{Esquematização da validação cruzada \textit{leave-one-out}.}
	\label{cap-aprend-estat-loocv}
\end{figure}

Repare que com essa estratégia, todas as observações são utilizadas no ajuste do modelo, o que elimina as limitações da amostra de validação. No entanto, uma desvantagem aqui é a necessidade de ajustar o modelo $n$ vezes. Quando $n$ é muito grande, a LOOCV pode exigir um alto tempo de computação. Apresentamos a seguir uma generalização do LOOCV que é mais adequada para grandes amostras.

\subsubsection{K-fold}

Podemos generalizar a LOOCV dividindo a amostra em $k$ grupos com aproximadamente a mesma quantidade de observações e então ajustando o modelo $k$ vezes, sendo que em cada ajuste selecionamos um grupo diferente como amostra de validação. Essa abordagem é chamada de \textit{validação cruzada k-fold}. Note que a LOOCV é o caso especial em que $k = n$.

A maior vantagem da validação cruzada \textit{k-fold} sobre a LOOCV é computacional. Em vez de ajustarmos o modelo $n$ vezes, ajustamos apenas $k$, sendo que $k << n$. E como estamos utilizando todas as observações para treinar o modelo, não temos as limitações de se utilizar uma única amostra de validação.

Muitas vezes, quando a classe de modelos escolhida exige a escolha de hiperparâmetros, como o grau de suavização de um modelo aditivo generalizado (Seção \ref{sec:gam}) ou o $\lambda$ do LASSO (Seção \ref{sec:lasso}), é comum separarmos a amostra em três partes: uma amostra de treino, uma amostra de validação e uma amostra de teste. Nesse caso, os modelos são treinados com a amostra de treino e, para diversos valores do hiperparâmetro, calculamos o erro de teste na amostra de validação. Escolhemos então o hiperparâmetro que leva ao menor erro de teste e utilizamos a amostra de teste para calcular o erro de teste do modelo final. Geralmente utiliza-se LOOOCV ou \textit{k-fold} para a validação e uma amostra separada para o teste. Essa estratégia é utilizada para garantir que o erro de teste não seja calculado em observações utilizadas no ajuste do modelo e reflita o erro que obteríamos ao aplicá-lo no mundo real.

Como discutimos até agora, a validação cruzada utiliza reamostragem para avaliar a qualidade do ajuste realizado. A seguir, apresentaremos uma técnica de reamostragem para obter informações sobre as estimativas do modelo.

\subsection{Bootstrap}

\textit{Bootstrap} é uma poderosa ferramenta estatística utilizada para quantificar incertezas associadas a estimadores e modelos estatísticos. \cite{Salvo2014} e \cite{Salvo2017}, por exemplo, utilizaram essa técnica para estimar o erro-padrão dos coeficientes do modelo de regressão linear ajustado para associar a concentração de ozônio na cidade de São Paulo com a proporção estimada de veículos bicombustíveis rodando a gasolina. Segundo os autores, o bootstrap foi utilizado para contemplar a variação causada pelo erro de medida presente na estimação da proporção de carros rodando a gasolina e na medição das condições climáticas.

O bootstrap consiste em gerar novas amostras sorteando repetidamente novas observações do conjunto de dados original. Para cada amostra, podemos ajustar o modelo escolhido, obtendo assim uma amostra dos coeficientes. A estimativa de bootstrap do erro-padrão de cada coeficiente, por exemplo, é dada pelo desvio-padrão das estimativas obtidas.

Mais informações sobre o bootstrap podem ser encontradas em \cite{James2013}.

%Essa técnica pode ser utilizada, por exemplo, para estimar o erro-padrão dos coeficientes de um modelo de regressão.

\section{Seleção de variáveis}

É muito comum na modelagem estatística incluirmos no modelo preditores que, na verdade, não estão associados com o fenômeno de interesse. Essas variáveis irrelevantes geram uma complexidade desnecessária no modelo resultante, sendo apropriado retirá-las da análise para obtermos um modelo mais fácil de interpretar.

Uma forma de selecionar variáveis é, a partir de algum critério, definir qual subconjunto dos preditores deve permanecer no modelo. Nesta seção, apresentaremos estratégias para decidir quais variáveis devem ficar ou ser retiradas do modelo.

\subsection{Selecionando o melhor subconjunto de preditores}

Para selecionar o melhor subconjunto entre $p$ preditores, ajustamos o modelo para todos os subconjuntos possíveis de variáveis e escolhemos o melhor segundo alguma métrica, geralmente erro preditivo, R$^2$ ajustado, AIC ou BIC. Para mais informações sobre essas quantidades, consultar \cite{James2013}. Observe que devemos usar uma métrica que penalize o modelo pelo número de parâmetros, pois, caso contrário, selecionaríamos sempre o modelo com mais preditores.

Na Tabela \ref{tab:3-preditores-selecao-melhor-subconjunto} apresentamos os 7 modelos que precisaríamos ajustar no caso de 3 preditores, $X_1$, $X_2$ e$X_3$, e um modelo de regressão linear (Seção \ref{sec:modelo-linear}). Para um número relativamente pequeno de variáveis, selecionar o melhor subconjunto de preditores é uma estratégia conceitualmente simples e de fácil execução. No entanto, conforme $p$ cresce, essa técnica pode se tornar computacionalmente inviável. Para $p = 20$, por exemplo, precisaríamos rodar mais de um milhão de modelos. A seguir, apresentamos algumas estratégias computacionalmente eficientes para aplicarmos em problemas com muitos preditores.

\begin{table}[h!]
	\centering
	\caption{Modelos de regressão linear que devem ser ajustados para selecionar o melhor subconjunto de variáveis no caso com 3 preditores.}
	\begin{tabular}{c|c|c}
		\hline 
		Uma variável & Duas variáveis & Três variáveis \\ 
		\hline 
		$Y = \beta_0 + \beta_1 X_1 + \epsilon$ & $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$ &  \multirow{3}{5.5cm}{$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 +  \epsilon$} \\  
		$Y = \beta_0 + \beta_1 X_2 + \epsilon$ & $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_3 + \epsilon$  \\
		$Y = \beta_0 + \beta_1 X_3 + \epsilon$ & $Y = \beta_0 + \beta_1 X_2 + \beta_2 X_3 + \epsilon$ \\ 
		\hline 
	\end{tabular}
    \label{tab:3-preditores-selecao-melhor-subconjunto}	
\end{table}

\subsection{Stepwise}

Os métodos \textit{stepwise} são algoritmos de seleção de variáveis que visam encontrar o melhor sub-conjunto de preditores dentro de um conjunto restrito de combinações em vez de ajustar todos os 2$^p$ modelos possíveis. 

A diferença entre cada método \textit{stepwise} está em como as variáveis são adicionas ou retiradas do modelo em cada passo. Os mais utilizados são o \textit{foward stepwise} e o \textit{backward stepwise}.

O \textit{foward stepwise} consiste na execução dos seguintes passos:

\begin{enumerate}
	\item Ajuste o modelo nulo ($M_0$), sem preditores.
	\item Ajuste todos os $p$ modelos com 1 preditor e escolha o melhor\footnote{Maior $R^2$, por exemplo.} ($M_1$).
	\item Ajuste todos os $p-1$ modelos com 2 preditores que contenham o preditor selecionado no passo anterior e escolha o melhor ($M_2$).
	\item De forma análoga, ajuste os modelos com 3, 4, ..., $p$ preditores, mantendo sempre como base o modelo obtido anteriormente, e em cada passo escolha o melhor ($M_3$, $M_4$, \dots, $M_p$).
	\item Escolha o melhor modelo entre $M_0$, $M_1$, \dots, $M_p$ utilizando erro preditivo, AIC, BIC ou $R^2$ ajustado.
\end{enumerate}

Repare que o \textit{foward stepwise} diminui o número de modelos ajustados de $2^p$ para $1 + p(p + 1)/2$. Para $p = 20$, o número de modelos diminui de 1.048.576 para 211.  

A ideia do método \textit{backward stepwise} é parecida com a do \textit{foward}. A diferença é que começamos no passo 1 com o modelo completo ($M_p$), com todos os preditores, e nos passo seguintes retiramos cada um dos preditores e ajustamos os modelos correspondentes, selecionando sempre aquele com maior $R^2$ ($M_{p-1}, M_{p-2}, \dots, M_0$). Ao fim, escolhemos o melhor entre os modelos $M_0$, $M_1$, \dots, $M_p$ utilizando erro preditivo, AIC, BIC ou $R^2$ ajustado. O número de modelos ajustados nesse caso é igual ao do \textit{foward stepwise}.

Ainda existem métodos \textit{stepwise} híbridos, nos quais os preditores são adicionados sequencialmente, assim como no \textit{foward stepwise}, mas em cada etapa é avaliado se um dos preditores já incluídos deve ou não sair do modelo. Essa estratégia tenta considerar mais modelos, chegando mais próximo da seleção do melhor sub-conjunto discutida na seção anterior. Para mais informações, consultar \cite{Nelder1972}.

\section{Regularização}

Os métodos de seleção de sub-conjuntos de preditores apresentados nas seções anteriores envolvem o ajuste de diversos modelos e a escolha do melhor segundo alguma métrica. Uma alternativa, seria ajustar um único modelo contendo todos os preditores e utilizar técnicas que limita ou regulariza as estimativas dos coeficientes, encolhendo seus valores na direção do zero.

Essas técnicas são chamadas de técnicas de regularização e serão o tema dessa seção.

\subsection{Regressão ridge}
\label{sec:ridge}


\subsection{LASSO}
\label{sec:lasso}