%% ------------------------------------------------------------------------- %%
\chapter{Estratégias de aprendizado automático}
\label{cap:aprendizado_estatistico}

\begin{flushright}
	\textcolor{gray}{
		\begin{tabular}{r}
			There are no routine statistical questions, \\
			only questionable statistical routines. \\
			--- Sir David Cox			
		\end{tabular} 
	}
	\vspace{5mm}
\end{flushright}

Nos últimos anos, a abordagem conhecida como \textit{machine learning}\footnote{Também conhecida como modelagem preditiva, aprendizado estatístico, aprendizagem automática ou aprendizado de máquina.} se tornou muito popular, principalmente pela sua eficiência na resolução de problemas de predição, como detecção de imagens, transcrição de áudio e sistemas de recomendação de compras. Por trás de todo o marketing em volta desse termo, existe um conjunto de práticas e técnicas que visam gerar a predição mais precisa possível para um fenômeno.

No capítulo anterior, apresentamos diversas classes de modelos úteis para fazer inferência em estudos de poluição do ar. A utilização desses modelos depende de suposições sobre a forma com que as variáveis explicativas e a variável resposta estão relacionadas. De maneira geral, essas suposições são partes de um modelo probabilístico para a variável resposta $Y$, cuja parametrização dependerá de alguma função dos preditores\footnote{Muita da literatura sobre \textit{machine learning} vem da área da Ciência da Computação. Os computólogos, de uma maneira geral, denominam as variáveis respostas como variáveis de saída ou \textit{outputs} e os preditores como variáveis de entrada ou \textit{inputs}.} $\mathbf{X}$. O modelo de regressão linear (\ref{mod:linear}), por exemplo,supõe que:

\begin{itemize}
	\item a média de $Y$ depende das variáveis em $\mathbf{X}$ a partir da relação $\beta_0 + \beta_1X_1 + \cdots \beta_pX_p$ (linearidade e aditividade);
	\item a variância de $Y$, $\sigmatwo$, é constante para todas as observações na população.
\end{itemize}
Essas suposições, potencialmente restritivas, permitem que o modelo seja interpretável, isto é, ao estimarmos os coeficientes $\beta_0, \beta_1, \dots, \beta_p$, entendemos como a variável $Y$ é influenciada por cada preditor $X_1, \dots, X_p$.

As técnicas e modelos utilizados para \textit{machine learning} colocam a interpretabilidade em segundo plano e priorizam a produção de predições o mais precisas possível para a resposta de interesse. As estratégias dentro dessa abordagem enfrentam a dualidade entre flexibilidade e sobreajuste, isto é, buscam entre modelos complexos (normalmente não interpretáveis) aquele que melhor se ajuste aos dados, mas que ainda possa ser generalizado para além da amostra. Embora essa visão não seja adequada para inferência, muitas das práticas podem ser incorporadas em estudos inferenciais na tentativa de encontrar modelos mais bem ajustados.

Neste capítulo, discutiremos com mais detalhes o conceito de sobreajuste, apresentando métodos de reamostragem, seleção de variáveis e regularização. Em seguida, introduziremos alguns modelos de árvores, bastante utilizados para modelagem preditiva devido à sua alta precisão. Por fim, apresentaremos alguns métodos gráficos para interpretar modelos ``caixa-preta''.

\section{Sobreajuste e o balanço entre viés e variância}
\label{sec:trade-off}

Como discutido na introdução do Capítulo \ref{cap:regressao}, dificilmente vamos encontrar uma função $f(\cdot)$ que relacione perfeitamente a variável resposta $Y$ e os preditores $\mathbf{X}$, pois estamos sempre sujeitos a dois tipos de erros: um erro redutível e outro irredutível. O erro redutível indica o quanto o modelo escolhido não se adéqua bem ao fenômeno estudado e tem esse nome porque podemos sempre encontrar uma candidata de $f(\cdot)$ que se aproxime mais do processo gerador de $Y$. Reduzir ao máximo esse erro é o grande objetivo da modelagem.

No entanto, mesmo que conseguíssemos eliminar o erro redutível, nossas predições não seriam perfeitas devido ao erro irredutível. Esse erro representa a parte da variabilidade de $Y$ que não pode ser explicada pelos preditores em $\mathbf{X}$, comumente chamado de erro aleatório ou ruído. Assim, por construção, todo modelo estatístico tem um erro associado.

O sobreajuste ou \textit{overfitting} acontece quando, na tentativa de eliminar o erro redutível, acabamos eliminando também o erro irredutível. Conforme aumentamos a complexidade do modelo, podemos passar a explicar variações aleatórias, que não estão associadas aos preditores considerados. Assim, um modelo sobreajustado gera conclusões que os dados disponíveis não são capazes de sustentar em um contexto mais amplo.

Essa dualidade entre qualidade do ajuste e capacidade de generalização é o maior paradigma da modelagem preditiva, e deveria ter atenção especial em qualquer contexto de modelagem, pois queremos sempre generalizar os resultados do modelo para a população de interesse. Na prática, gostaríamos de encontrar o melhor ajuste que generalize bem os resultados para a população, e essa tarefa pode ser resumida na minimização de duas quantidades: o \textit{viés} e a \textit{variância}. Para entender melhor o que essas quantidades representam, imagine que precisamos ajustar um modelo para os dez pontos apresentados na Figura \ref{cap-aprend-estat-trade-off} (a). Podemos começar ajustando um modelo de regressão linear simples,

\begin{displaymath}
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i, \quad i = 1, \dots, 10,
\end{displaymath}
e calcular a raiz do erro quadrático médio (RMSE), definido na Seção \ref{sec:reg-quali-mod}, para avaliar o quanto a reta ajustada se afasta dos pontos. Uma forma de tentar melhorar o ajuste seria acrescentar um termo quadrático e verificar se a RMSE diminui. Podemos repetir esse procedimento acrescentando termos de graus cada vez maiores\footnote{Esses são os modelos polinomiais apresentados na Seção \ref{sec:linearidade}. O modelo de regressão linear simples é um modelo polinomial de grau 1.}, até encontrarmos o menor RMSE.

Na Tabela \ref{tab:cap-aprend-estat-trade-off-10-obs}, apresentamos a RMSE obtida para os modelos de regressão polinomial até o nono grau. Observe que, conforme aumentamos a complexidade do modelo (grau do polinômio), a RMSE diminui, até chegar em 0 para o polinômio de grau 9. Se utilizarmos apenas a RMSE como medida da performance do modelo, escolheríamos justamente esse polinômio como modelo final. No entanto, pela Figura \ref{cap-aprend-estat-trade-off} (b), observamos que esse modelo claramente não representa bem o comportamento dos pontos.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{figuras/cap-aprend-estat-trade-off.pdf}
	\caption{Exemplo do balanço entre viés e variância. (a) Conjunto de 10 pontos que gostaríamos de ajustar. (b) Modelo de regressão linear simples (vermelho), modelo de regressão polinomial de grau 2 (amarelo) e modelo de regressão polinomial de grau 9 (azul), ajustados aos 10 pontos. (c) Amostra de 100 novas observações plotadas juntas dos modelos polinomiais ajustados nas 10 observações iniciais. (d) Modelos de regressão polinomial de graus 1 (vermelho), 2 (amarelo) e 9 (azul) ajustados aos 100 novos pontos.}
	\label{cap-aprend-estat-trade-off}
\end{figure}

\begin{table}[h!]
	\centering
	\caption{Raiz do erro quadrático médio (RMSE) para os modelos polinomiais de grau 1 a 9 ajustados com 10 e 100 observações no exemplo da Figura \ref{cap-aprend-estat-trade-off}.}
	\begin{tabular}{M{2cm}|M{3cm}|M{3cm} N}
		\hline
		Grau do polinômio & RMSE (10 obs.) & RMSE (100 obs.) & \\
		\hline
		1 & 0.204 & 0.360 & \\[10pt]
		\hline
		2 & 0.149 & 0.226 & \\[10pt]
		\hline
		3 & 0.140 & 0.199 & \\[10pt]
		\hline
		4 & 0.140 & 0.198 & \\[10pt]
		\hline
		5 & 0.102 & 0.289 & \\[10pt]
		\hline
		6 & 0.086 & 0.360 & \\[10pt]
		\hline
		7 & 0.063 & 0.320 & \\[10pt]
		\hline
		8 & 0.031 & 1.152 & \\[10pt]
		\hline
		9 & 0.000 & 3.904 & \\[10pt]
		\hline
	\end{tabular}
	\label{tab:cap-aprend-estat-trade-off-10-obs}
\end{table}

Considere agora, nesse mesmo exemplo, que conseguimos uma nova amostra com mais 100 observações geradas pelo mesmo fenômeno que gerou as 10 primeiras. A Figura \ref{cap-aprend-estat-trade-off} (c) confirma o quanto o modelo polinomial de grau 9 se ajustou mal aos dados, enquanto os modelos de grau 1 e 2 parecem escolhas mais razoáveis. Podemos observar ainda na Tabela \ref{tab:cap-aprend-estat-trade-off-10-obs} que a RMSE do modelo polinomial de grau 9 calculada nas 100 novas observações\footnote{Aqui, os modelos não foram reajustados. Foram considerados os modelos ajustados apenas com as 10 primeiras observações} é o maior entre todos os candidatos. Por fim, observe na Figura \ref{cap-aprend-estat-trade-off} (d) como a curva desse modelo muda quando o ajustamos agora usando também as 100 novas observações.

Como enfatizado anteriormente, estamos sempre em busca de modelos que se ajustem bem à amostra, mas que também possam ser generalizados para a população. Assim, chamaremos de \textit{viés} o quanto o modelo ajustado está distante das observações da amostra e de \textit{variância} o quanto o modelo mudaria se o ajustássemos em uma nova amostra. O viés representa o erro induzido por aproximar um fenômeno real, que pode pode ser extremamente complicado, por um modelo muito mais simples. Já a \textit{variância} indica o quanto erraríamos se usássemos o modelo para predizer novas observações. Dizemos então que modelos mal ajustados apresentam alto viés e modelos com baixo poder de generalização apresentam alta variância. 

É muito comum utilizarmos estratégias que se preocupam apenas com a minimização do viés. Essas estratégias elegem como boas escolhas modelos complexos, visando um ajuste cada vez melhor aos dados, sem levar em conta o quanto isso será representativo na população. No exemplo anterior, isso fica claro com o ajuste de polinômios de grau cada vez maior aos dados. O polinômio de grau 9 ilustra o conceito de sobreajuste: baixo viés, mas alta variância, não sendo apropriado para representar o fenômeno de interesse. Controlar o balanço entre viés e variância é um dos maiores desafios da modelagem preditiva.

Na presença de muitos preditores, não é possível visualizar graficamente o sobreajuste, como mostrado no exemplo. Por isso, na prática, pode não ser trivial identificar um modelo sobreajustado. Para contornar esse problema, apresentaremos na próxima seção medidas utilizadas para quantificar o viés e a variância de um modelo.

\section{Estimando o desempenho do modelo}

Na Seção \ref{sec:reg-quali-mod}, vimos que o $R^2$ e a RMSE podem ser utilizados para avaliar a qualidade do ajuste de um modelo. Em alguns casos, podemos querer utilizar o erro absoluto médio (MAE, \textit{mean absolute error}), que, ao contrário da RMSE, não dá mais peso para erros em valores muito altos da variável resposta.

A escolha da métrica de performance vai depender sempre do objetivo do estudo. Independentemente da medida escolhida, ao calculá-la para as próprias observações utilizadas no ajuste, temos uma estimativa do viés do modelo, isto é, quanto o modelo escolhido se ajusta bem à amostra. Essa quantidade é chamada de \textit{erro de treino}. Para obtermos uma estimativa da variância, precisamos calculá-la para observações não utilizadas no ajuste, representando uma nova amostra do fenômeno sob estudo. Essa quantidade é chamada de \textit{erro de teste}.

Na prática, nem sempre teremos à disposição uma nova base de dados para a estimação do erro de teste. Uma alternativa nesses casos é utilizar métodos de reamostragem, que consistem em  separar a base disponível em observações utilizadas para \textit{treinar} o modelo e observações para estimar sua variância. Na próxima seção, apresentaremos dois métodos de reamostragem bastante utilizados: a \textit{validação cruzada} e o \textit{bootstrapping}.

\subsection{Validação cruzada}

Como na maioria dos estudos não é possível obter facilmente novas observações, podemos calcular o erro de teste, estimativa da variância do modelo, dividindo a amostra original em duas partes: uma utilizada para o ajuste do modelo (amostra de treino) e a outra para o cálculo do erro (amostra de teste), essa última agindo como se fosse um conjunto de novas observações. Essa técnica é conhecida como validação cruzada \citep{James2013}. Há diversos tipos de validação cruzada, que dependem da forma utilizada para dividir a amostra. Nesta seção, apresentaremos os principais tipos de validação cruzada e discutiremos as vantagens e desvantagens de cada um.

%Quando precisamos escolher valores para hiperparâmetros do modelo, como o grau de suavização de um modelo aditivo generalizado (Seção \ref{sec:gam}) ou o $\lambda$ do LASSO (Seção \ref{sec:lasso}), podemos dividir a amostra em ainda mais uma parte: uma amostra de validação. Nesses casos, os modelos são treinados com a amostra de teste e, para diversos valores do hiperparâmetro, calculamos o erro de teste na amostra de validação. Escolhemos então o hiperparâmetro com menor erro de teste e utilizamos a amostra de teste para calcular o erro de teste do modelo final. Repare que a amostra de teste nunca é utilizada para ajustar o modelo.

\subsubsection{Amostra de validação}

A utilização de uma amostra de validação é a forma mais simples de validação cruzada. A estratégia consiste em dividir aleatoriamente as observações em um conjunto de treino, usado para ajustar o modelo, e outro de teste, utilizado exclusivamente para estimar o erro de teste. A proporção de observações em cada uma depende do tamanho amostral. Costuma-se utilizar 30\% da amostra original no conjunto de teste, mas esse número pode ser menor para amostras muito grandes (mais de 100 mil observações, por exemplo).

As maiores vantagens dessa técnica são a sua simplicidade e a necessidade de se ajustar o modelo uma única vez. No entanto, conforme discutido em \cite{James2013}, a amostra de validação apresenta duas potenciais desvantagens:

\begin{itemize}
	\item a estimativa do erro de teste pode apresentar alta variabilidade, dependendo de quais observações ficaram na amostra de treino e quais ficaram na amostra de validação;
	\item como a acurácia de modelos estatísticos é menor quando ajustados com menos observações, e apenas parte das observações são utilizadas para treinar o modelo, o  erro de teste pode estar sendo superestimado. 
\end{itemize}

A seguir, apresentaremos a validação cruzada \textit{leave-one-out} (LOOCV), uma técnica de reamostragem que não possui essas limitações.

\subsubsection{Validação cruzada \textit{leave-one-out} (LOOCV)}

Considere uma amostra com $n$ observações. A LOOCV consiste em rodar o modelo escolhido $n$ vezes, sendo que, em cada ajuste, deixamos de fora a $i$-ésima observação, $i = 1, \dots, n$, e a utilizamos para calcular o erro de teste. A estimativa final do erro de teste será então a média das $n$ medidas parciais. Uma esquematização dessa técnica está representada na Figura \ref{cap-aprend-estat-loocv}.


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.5\linewidth]{figuras/cap-aprend-estat-esquema-loocv.png}
	\caption{Esquematização da validação cruzada \textit{leave-one-out}.}
	\label{cap-aprend-estat-loocv}
\end{figure}

Repare que, neste caso, todas as observações são utilizadas no ajuste do modelo e na estimativa do erro de teste, o que elimina as limitações incorridas ao utilizarmos a amostra de validação. No entanto, uma desvantagem aqui é a necessidade de ajustar o modelo $n$ vezes. Quando $n$ é muito grande, a LOOCV pode exigir muito esforço computacional, inviabilizando a sua utilização. 

Vale ressaltar que esse procedimento é utilizado para estimar as métricas de performance do modelo, sendo que ajuste do modelo final da análise contempla todas as observações da amostra. Dessa forma, ao fim desse procedimento, $n+1$ modelos são ajustados: as $n$ iterações da LOOCV e o ajuste do modelo com todas as observações.

A seguir, apresentamos validação cruzada \textit{k-fold}, uma generalização da LOOCV que não apresenta a contrapartida computacional.

\subsubsection{validação cruzada k-fold}

Podemos generalizar a LOOCV criando $k$ amostras de teste, com aproximadamente a mesma quantidade de observações e sem intersecção. Então ajustamos o modelo $k$ vezes, sendo que em cada ajuste selecionamos um conjunto diferente como amostra de teste e as observações restantes como a de treino. Essa abordagem é chamada de \textit{k-fold}. Note que a LOOCV é o caso especial em que $k = n$. Na prática, escolhemos valores de $k$ entre 3 e 10, sendo que $k = 5$ é bastante utilizado (Figura \ref{cap-aprend-estat-esquema-k-fold-cv}).

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{figuras/cap-aprend-estat-esquema-k-fold-cv.png}
	\caption{Esquematização da validação cruzada \textit{k-fold}, com $k = 5$.}
	\label{cap-aprend-estat-esquema-k-fold-cv}
\end{figure}

A maior vantagem da validação cruzada \textit{k-fold} sobre a LOOCV é computacional. Em vez de ajustarmos o modelo $n$ vezes, ajustamos apenas $k$, sendo que $k << n$. E como utilizamos todas as observações para treinar o modelo, não temos as limitações de se utilizar uma única amostra de validação.

Assim como na LOOCV, o objetivo desse procedimento é estimar o erro de predição. Ao fim, ajustamos o modelo utilizando todas as observações na amostra, que será considerado o modelo final. Assim, o modelo é ajustado $k+1$ vezes: as $k$ iterações da validação $k$-fold e o ajuste com todas as observações.

A validação cruzada é geralmente utilizada para avaliar a performance do modelo. A seguir, apresentaremos uma técnica de reamostragem muito utilizada também para o cálculo do erro-padrão das estimativas do modelo.

\subsection{Bootstrapping}

O \textit{bootstrapping} é uma poderosa ferramenta utilizada para quantificar incertezas associadas a estimadores e modelos estatísticos. Ela consiste em gerar $m$ novas amostras a partir de sorteios com reposição da amostra original. Para cada uma das amostras geradas, ajustamos o modelo escolhido e guardamos as estimativas dos parâmetros. Ao repetirmos esse processo para as $m$ amostras, teremos $m$ estimativas diferentes para cada parâmetro do modelo. Assim, para cada parâmetro, podemos, por exemplo, calcular o desvio-padrão dessas $m$ estimativas e utilizar essa medida como o erro-padrão associado ao coeficiente. Repare que os parâmetros do modelo devem ser estimados utilizando a amostra original. Nesse exemplo, o \textit{bootstrapping} seria usado apenas para estimar a variabilidade dos coeficientes.

Essa técnica é utilizada principalmente quando não conhecemos a distribuição dos estimadores do modelo ou quando precisamos controlar outras fontes de variabilidade. \cite{Salvo2014} e \cite{Salvo2017}, por exemplo, utilizaram o \textit{bootstrapping} para estimar o erro-padrão dos coeficientes do modelo de regressão linear ajustado para associar a concentração de ozônio na cidade de São Paulo com a proporção estimada de veículos bicombustíveis abastecidos com gasolina. Segundo os autores, essa estratégia foi utilizada para contemplar a variação causada pelo erro de medida presente na estimação da proporção de carros a gasolina e na medição das condições climáticas.

O \textit{bootstrapping} também pode ser utilizado para a estimação da performance do modelo. Neste caso, cada uma das $m$ amostras é utilizada como conjunto de treino e as observações que foram sorteadas em cada amostra são utilizada com conjunto de teste. Em geral, o tamanho de cada amostra de \textit{bootstrapping} tem o mesmo tamanho da base original.

Mais informações sobre o \textit{bootstrapping} podem ser encontradas em \cite{James2013}.

%Essa técnica pode ser utilizada, por exemplo, para estimar o erro-padrão dos coeficientes de um modelo de regressão.

\section{Seleção de variáveis}

Na especificação do modelo, muitas vezes incluímos variáveis que não são associadas com o fenômeno sob estudo. Isso acontece principalmente quando temos pouco conhecimento sobre o mecanismo gerador do fenômeno ou quando estamos justamente investigando quais fatores estão associados a ele. 

Como variáveis irrelevantes geram uma complexidade desnecessária no modelo, podemos pensar em estratégias para retirá-las da análise, aumentando a interpretabilidade dos resultados. Nesta seção, apresentaremos algumas técnicas de seleção de variáveis que podem ser utilizadas em qualquer classe de modelos estatísticos.

\subsection{Selecionando o melhor subconjunto de preditores}

A maneira mais simples para selecionarmos variáveis em um modelo é ajustar todas as possíveis combinações dos $p$ preditores e avaliar qual produz o melhor ajuste segundo alguma métrica de performance. Essa estratégia é chamada de seleção do melhor subconjunto de preditores (\textit{best subset selection}, em inglês) e seu procedimento de seleção pode ser resumido pelos passo abaixo:

\begin{enumerate}
	\item Ajustar o modelo nulo, sem nenhum preditor ($k = 0$).
	\item Para cada $k = 1, \dots, p$, ajustar todos os modelos possíveis com $k$ preditores e escolher o melhor entre eles, isto é, aquele com menor RSME ou maior $R^2$ por exemplo.
	\item Dentre os $p+1$ modelos selecionados em (1) e (2), selecionar o melhor usando o $R^2$ ajustado, RMSE calculada por validação cruzada (erro de teste), AIC ou BIC\footnote{O AIC e o BIC são medidas da qualidade do ajuste penalizadas pelo número de parâmetros do modelo. Mais informações, consultar \cite{James2013}}.
\end{enumerate}
Observe que a métrica utilizada para selecionar o modelo final deve ser penalizada pelo número de parâmetros, pois, caso contrário, escolheríamos sempre o modelo com mais preditores.

Para um número relativamente pequeno de variáveis, selecionar o melhor subconjunto de preditores é uma estratégia conceitualmente simples e de fácil execução. No entanto, conforme $p$ cresce, essa técnica pode se tornar computacionalmente inviável. Na Tabela \ref{tab:3-preditores-selecao-melhor-subconjunto} apresentamos os 7 modelos que precisaríamos ajustar se tivéssemos 3 preditores, $X_1$, $X_2$ e$X_3$, e um modelo de regressão linear (Seção \ref{sec:modelo-linear}). Para $p = 20$, por exemplo, precisaríamos rodar mais de um milhão de modelos, o que poderia inviabilizar a execução dessa estratégia.

\begin{table}[h!]
	\centering
	\caption{Modelos de regressão linear que devem ser ajustados para selecionar o melhor subconjunto de variáveis no caso com 3 preditores, além do modelo nulo.}
	\begin{tabular}{c|c|c}
		\hline 
		Uma variável & Duas variáveis & Três variáveis \\ 
		\hline 
		$Y = \beta_0 + \beta_1 X_1 + \epsilon$ & $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$ &  \multirow{3}{5.5cm}{$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 +  \epsilon$} \\  
		$Y = \beta_0 + \beta_1 X_2 + \epsilon$ & $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_3 + \epsilon$  \\
		$Y = \beta_0 + \beta_1 X_3 + \epsilon$ & $Y = \beta_0 + \beta_1 X_2 + \beta_2 X_3 + \epsilon$ \\ 
		\hline 
	\end{tabular}
    \label{tab:3-preditores-selecao-melhor-subconjunto}	
\end{table}

A seguir, apresentamos algumas estratégias computacionalmente eficientes para aplicarmos em problemas com muitos preditores.

\subsection{Métodos passo a passo}

Os métodos passo a passo (\textit{stepwise}) são algoritmos de seleção de variáveis que visam encontrar o melhor sub-conjunto de preditores dentro de um conjunto restrito de combinações em vez de ajustar todos os 2$^p$ modelos possíveis. 

A diferença entre cada método \textit{stepwise} está em como as variáveis são adicionas ou retiradas do modelo em cada passo. Os mais utilizados são o \textit{foward stepwise} e o \textit{backward stepwise}.

O \textit{foward stepwise} consiste na execução dos seguintes passos:

\begin{enumerate}
	\item Ajuste o modelo nulo ($M_0$), sem preditores.
	\item Ajuste todos os $p$ modelos com 1 preditor e escolha o melhor\footnote{Maior $R^2$, por exemplo.} ($M_1$).
	\item Ajuste todos os $p-1$ modelos com 2 preditores que contenham o preditor selecionado no passo anterior e escolha o melhor ($M_2$).
	\item De forma análoga, ajuste os modelos com 3, 4, ..., $p$ preditores, mantendo sempre como base o modelo obtido anteriormente, e em cada passo escolha o melhor ($M_3$, $M_4$, \dots, $M_p$).
	\item Escolha o melhor modelo entre $M_0$, $M_1$, \dots, $M_p$ utilizando erro preditivo, AIC, BIC ou $R^2$ ajustado.
\end{enumerate}

Repare que o \textit{foward stepwise} diminui o número de modelos ajustados de $2^p$ para $1 + p(p + 1)/2$. Para $p = 20$, o número de modelos diminui de 1.048.576 para 211.  

A ideia do método \textit{backward stepwise} é parecida com a do \textit{foward}. A diferença é que começamos no passo 1 com o modelo completo ($M_p$), com todos os preditores, e nos passos seguintes retiramos cada um dos preditores e ajustamos os modelos correspondentes, selecionando sempre aquele com maior $R^2$ ($M_{p-1}, M_{p-2}, \dots, M_0$). Ao fim, escolhemos o melhor entre os modelos $M_0$, $M_1$, \dots, $M_p$ utilizando erro preditivo, AIC, BIC ou $R^2$ ajustado. O número de modelos ajustados nesse caso é igual ao do \textit{foward stepwise}.

Ainda existem métodos \textit{stepwise} híbridos, nos quais os preditores são adicionados sequencialmente, assim como no \textit{foward stepwise}, mas em cada etapa avalia-se se um dos preditores já incluídos deve ou não sair do modelo. Essa estratégia tenta considerar mais modelos, chegando mais perto da seleção do melhor subconjunto discutida na seção anterior. Para mais informações, consultar \cite{Nelder1972}.

\section{Regularização}
\label{sec:regularizacao}

Os métodos de seleção de subconjuntos de preditores apresentados na seção anterior diminuem a complexidade do modelo eliminando variáveis que não contribuem significativamente com a diminuição do viés, potencialmente diminuindo a variância. As técnicas de \textit{regularização} apresentam uma ideia similar: diminuir a variância do modelo a partir de suavizações que introduzem um pouco de viés. Essas técnicas envolvem o ajuste de um único modelo e introduzem penalizações no processo de estimação que limitam as estimativas dos coeficientes, encolhendo seus valores em direção a zero.

A utilização da regularização pode levar a uma redução substancial da variância do modelo, sendo uma boa estratégia para evitar o sobreajuste. Apresentaremos nesta seção as formas mais utilizadas de regularização: a regressão \textit{ridge} e o LASSO (\textit{least absolute shrinkage and selection operator}). 

\subsubsection{Regressão Ridge}

De uma forma geral, o processo de estimação dos parâmetros de um modelo consiste na minimização de uma função de perda $L(y, f(x))$ que depende dos dados observados $(x, y)$ e do modelo escolhido $(f(\cdot))$. As técnicas de regularização consistem em adicionar uma penalidade nessa função de perda, de tal forma que os coeficientes dos preditores pouco associados à variável resposta sejam encolhidos em direção a zero.

No caso da regressão ridge \citep{James2013}, a função de perda penalizada é dada por

\begin{displaymath}
	L(y, f(x)) + \lambda \sum_{j=1}^p \beta_j^2,
\end{displaymath}
sendo $\beta_1, \dots, \beta_p$ os parâmetros do modelo $f(\cdot)$ e $\lambda$ um hiperparâmetro\footnote{Hiperparâmetros são parâmetros que não são estimados diretamente pelos dados.} que controla o impacto da penalização nas estimativas dos coeficientes. Quando $\lambda = 0$, o termo é anulado e as estimativas são calculadas sem penalização. Conforme $\lambda \longrightarrow \infty$, os coeficientes $\beta_j$ passam a ser penalizados, encolhendo seus valores em direção a zero. A vantagem disso está na potencial redução da variância do modelo, em troca de um pequeno aumento do viés, já que os coeficientes menos importantes recebem cada vez menos peso. Assim, a regularização é uma alternativa para lidarmos com o balanço entre viés e variância discutido na Seção \ref{sec:trade-off}.

No caso da regressão ridge, é possível mostrar que, para qualquer $i = 1, \dots, p$,  $\beta_i = 0$ apenas se $\lambda = \infty$. Isso significa que não estamos fazendo seleção de variáveis, isto é, o modelo ajustado sempre terá todos os preditores. Apesar de estarmos melhorando a performance do modelo diminuindo o peso dos preditores menos importantes, isso pode não ser o ideal quando quisermos de fato eliminar variáveis do modelo. Nesses casos, uma boa alternativa é utilizar o LASSO.

\subsubsection{Least absolute shrinkage and selection operator (LASSO)}

O LASSO (\textit{least absolute shrinkage and selection operator}) é uma técnica análoga à regressão ridge, mas com função de perda penalização dada por

\begin{displaymath}
	L(y, f(x)) +\lambda \sum_{j=1}^p |\beta_j|.
\end{displaymath}
Para $\lambda$ suficientemente grande, essa penalização força alguns dos coeficientes sejam estimados exatamente como 0 e os correspondentes preditores associados serão eliminados do ajuste. Assim, ao utilizarmos o LASSO, estamos ao mesmo tempo reduzindo a variância do modelo e executando seleção de variáveis.

Um ponto importante sobre a aplicação das técnicas de regularização é a escala dos preditores. A maioria dos processos de estimação usuais é invariante à escala em que os preditores foram medidos, isto é, ajustar o modelo usando o preditor $X_1$ ou $cX_1$, $c$ um número real, não mudará a interpretação dos resultados. No caso da regressão ridge e do LASSO, a escala dos preditores influencia não só a estimativa dos próprios coeficientes, mas também a estimativa dos outros parâmetros do modelo. Dessa forma, um passo importante anterior à aplicação dessas técnicas é a padronização dos preditores, de tal forma que todos fiquem com a mesma média e variância. Essa padronização pode ser feita a partir da expressão

\begin{equation}
	\tilde{X}_{ij} = \frac{X_{ij} - \bar{X}_j}{\sqrt{\frac{1}{n}\sum_{i =1}^n (X_{ij} - \bar{X}_j)^2}}, 	
\end{equation}
sendo o denominador dessa expressão a estimativa do desvio-padrão do $j$-ésimo preditor. Consequentemente, todos os preditores terão média 0 desvio-padrão igual a 1.

Embora haja muita discussão sobre a validade de testes de hipóteses do tipo $\beta = 0$ para o LASSO, já que o algoritmo zera automaticamente os coeficientes menos importantes, alguns trabalhos vêm surgindo nos últimos anos sobre o cálculo do erro-padrão e sobre o desenvolvimento de testes paras as estimativas \citep{Adel2014, Lockhart2014}. Uma boa alternativa para avaliar a variabilidade das estimativas dos coeficientes é utilizar o \textit{bootstrapping}.

Para uma discussão mais aprofundada sobre a interpretação da regressão \textit{ridge} e do LASSO, consulte o Capítulo 6 de \cite{James2013}. Para o desenvolvimento matemático dessas técnicas, o Capítulo 5 de \cite{Hastie2008} é uma ótima referência.

\section{Quantificando a importância dos preditores}

Nas últimas seções, discutimos técnicas para removermos do modelo as variáveis que não ajudam a explicar a variabilidade da variável resposta. Em alguns casos, também gostaríamos de saber, entre os preditores que permaneceram no modelo, quais são os mais importantes.

Os valores $p$ são amplamente utilizados para definir as variáveis estatisticamente significativas para explicar a variável resposta. Dada a estimativa de um coeficiente $\beta$, o valor $p$ associado representa uma medida de evidência a favor da hipótese $\beta = 0$ e pode ser utilizado tanto para seleção de variáveis quanto para quantificar a magnitude de uma associação. Mais precisamente, se o valor $p$ for muito baixo (próximo de zero), o cenário $\beta = 0$ se torna improvável e então rejeitamos a hipótese de que o coeficiente é nulo. Caso contrário, se o valor $p$ for alto, a estimativa obtida não é inverossímil dentro do cenário $\beta = 0$ e então não rejeitamos essa hipótese.   

Ao cálculo do valor $p$ está associado uma estatística de teste, que também pode ser usada para quantificar a importância dos preditores. No modelo de regressão linear, por exemplo, a estatística do teste $t$ pode ser utilizada, de tal forma que, quanto maior o valor absoluto da estatística, maior será a importância do preditor para explicar a variável resposta.

Em alguns casos, a variação no erro preditivo quando um preditor é eliminado do modelo é utilizada como medida de importância. Essa métrica mais geral é bastante utilizada em modelos de regressão que envolvem funções suavizadoras, como os modelos aditivos generalizados.

Já para para a regressão ridge ou o LASSO, em que padronizamos as variáveis explicativas, uma medida de importância pode ser o próprio valor do coeficiente.

As métricas de importância dependem do modelo utilizado. De uma forma geral, os programas estatísticos já possuem métricas de importância implementadas. No R, a função \texttt{varImp()} do pacote \texttt{caret} calcula uma medida de importância para a maioria dos modelos disponíveis.

\section{Modelos de árvores}

Modelos baseados em árvores \citep{Hastie1990,James2013} são bastante utilizados tanto para regressão quanto para classificação. Eles envolvem a segmentação do espaço gerado pelas variáveis explicativas em algumas regiões mais simples, onde a média ou a moda da variável resposta são utilizadas como predição.

As chamadas árvores de decisão são modelos conceitualmente e computacionalmente simples, bastante populares pela sua interpretabilidade, apesar da precisão inferior quando comparados com modelos mais complexos. Generalizações desse modelos, como as florestas aleatórias (\textit{random forests}), costumam apresentar alta precisão, mesmo quando comparadas a modelos lineares, porém são pouco interpretáveis.

Nesta seção, introduziremos os principais conceitos por trás das árvores de decisão e das \textit{random forests}.


\subsection{Árvores de decisão}

As árvores de decisão se baseiam no particionamento do espaço gerado pelas variáveis explicativas. As predições são dadas pela média das observações dentro de cada subespaço obtido\footnote{No caso de uma variável resposta quantitativa. No caso de classificação, variáveis respostas categóricas, a predição será a classe mais frequente dentro do subespaço.}, e a construção desses subespaços visa minimizar alguma medida de erro (como o RMSE).

Na Figura \ref{fig:cap-aprend-estat-arvore}, apresentamos uma árvore de decisão para explicar a variabilidade da concentração de ozônio pela temperatura (tp). Nesse exemplo, a temperatura foi particionada em 2 pontos: 26 e 29 graus Celsius. Assim, a média da concentração de ozônio dentro de cada uma das três regiões obtidas pela partição serão as predições dadas pela árvore. No caso, a estimativa da concentração de ozônio será 38 $\mu g/m^3$ para dias com temperatura menor que 26° C, 69 $\mu g/m^3$ para dias com temperatura entre 26 e 29° C  e 92 $\mu g/m^3$ para dias com temperatura maior ou igual a 29° C.

No caso geral, para $p$ variáveis explicativas $X_1, \dots, X_p$ e uma amostra de tamanho $n$, a construção da árvore pode ser resumida a partir do seguinte algoritmo:

\begin{enumerate}
	\item Todas as observações começam na mesma partição.
	\item Para cada variável explicativa $X_j$ e seus valores observados $x_{ji}$, $j = 1, \dots, p$ e $i = 1, \dots, n$, são testadas todas as partições $X_j < x_{ji}$ e escolhida aquela que minimize alguma medida de erro (em geral, RMSE).
	\item O passo 2 é repetido, agora dentro de cada subespaço obtido, até que um dos subespaços tenha menos de um número mínimo de observações pré-definido.
\end{enumerate}

Na Figura \ref{fig:cap-aprend-estat-arvore}, cada partição, chamada de nó, é representada pelos módulos verdes. Os nós na base da árvore são chamados de nós terminais e representam os subespaços obtidos pelo particionamento. A interpretação do diagrama começa pelo primeiro nó (no topo da figura) e, para cada nova observação, seguimos as regras de decisão até alcançar um nó terminal, que apresentam a predição para dessa observação e o número de observações da amostra dentro desse subespaço (absoluto e proporcional ao tamanho da amostra). Quando temos mais de um preditor, cada nó poderá indicar uma partição em um preditor diferente.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.7\linewidth]{figuras/cap-aprend-estat-arvore.pdf}
	\caption{Exemplo de uma árvore de decisão para a concentração de ozônio explicada pela temperatura.}
	\label{fig:cap-aprend-estat-arvore}
\end{figure}

Observe que no exemplo temos 3 nós finais, mas, teoricamente, poderíamos continuar particionando o intervalo de valores da temperatura até cada possível valor ter o seu próprio nó. Esse seria um caso de árvore de decisão sobreajustada, apresentando pouco poder de generalização. A escolha do número de nós finais é feita a partir de uma técnica conhecida como ``poda'', que consiste em usar validação cruzada para definir a melhor altura para a árvore.

Na linguagem R, árvores de decisão podem ser ajustadas a partir da função \texttt{rpart()} do pacote \texttt{rpart}.

As árvores de decisão imitam bem o processo de tomada de decisão do cérebro humano, e por isso são mais simples de interpretar até mesmo que o modelo de regressão linear. No entanto, elas apresentam baixo poder preditivo e raramente são utilizadas para descrever processos muito complexos. A seguir, apresentaremos as florestas aleatórias, que abrem mão da interpretabilidade em troca de um alto grau de precisão.

\subsection{Florestas aleatórias}
\label{sec:random-forest}

As predições obtidas a partir de árvores de decisão tendem a ter alta variância, isto é, se dividirmos nossa amostra em duas e aplicarmos o mesmo modelo de árvores de decisão em cada uma, há uma grande chance de obtermos divisões diferentes no espaço amostral e, consequentemente, diferentes predições. 

Uma maneira de lidar com esse problema é gerar diferentes amostras de \textit{bootstrapping}, ajustar o mesmo modelo em cada uma delas e utilizar predições médias como estimativas finais do modelo. Em um contexto geral, essa técnica é chamada de \textit{bootstrapping aggregation} ou \textit{bagging}. Ela pode ser utilizada para qualquer modelo e seu objetivo geral é diminuir a variância das estimativas.

As chamadas florestas aleatórias \textit{random forests} \citep{Hastie2008} são uma aplicação do \textit{bagging} para árvores de decisão. Neste modelo, além de utilizarmos uma amostra de \textit{boostraping} diferente para cada árvore ajustada, também fazemos uma seleção dos preditores para cada ajuste. Isso impede que as árvores ajustadas sejam muito correlacionadas e permite que preditores que seriam preteridos por serem ``menos importantes'' também ajudem a explicar a variação da variável resposta.

Em cada iteração do algoritmo,

\begin{enumerate}
	\item sorteamos $m$ dos $p$ preditores, $m \leq p$;
	\item geramos uma amostra de \textit{boostrapping} a partir da base completa;
	\item ajustamos uma árvore de decisão utilizando os $m$ preditores escolhidos em (1) e a amostra em (2).
\end{enumerate}

Repetindo esse procedimento $M$ vezes, teremos $M$ estimativas, e predição final será a média dessas $M$ estimativas (no caso de regressão) ou a classe mais frequente (no caso de classificação). Geralmente, $m$ é escolhido como $\sqrt{p}$, mas esse hiperparâmetro pode ser definido utilizando validação cruzada. $M$ por volta de 200 costuma ser suficiente para gerar bons resultados\footnote{Para valores de $M$ muito altos (maior que 200), a amostragem dos preditores passa a criar árvores mais parecidas com as já existentes, o que não gera maiores ganhos de precisão.}.

Florestas aleatórias podem ser ajustadas utilizando a função \texttt{train()} do pacote \texttt{caret} com \texttt{method = "ranger"}. Os principais hiperparâmetros que devem ser escolhidos são o número de sorteados em cada ajuste ($m$) e o número mínimo de observações nos nós finais de cada árvore.

\subsection{XGBoost}

Assim como o \textit{bagging}, apresentado na seção anterior, o algoritmo \textit{boosting} é uma abordagem geral aplicada a diversos modelos para aumentar seu poder preditivo. No contexto de árvores de decisão, a ideia aqui é construir árvores sequencialmente, sendo que cada árvore utiliza informações obtidas da árvore anterior. Ao contrário do \textit{bagging}, o algoritmo \textit{boosting} não envolve amostras de \textit{bootstraping}, sendo que cada passo do algoritmo utiliza uma versão modificada dos dados originais.

A ideia por trás do algoritmo \textit{boosting} é ``aprender devagar''. Em vez de tentarmos ajustar uma grande árvore com toda a base de dados, ajustamos árvores com poucos nós $\hat{f}^1(x), \hat{f}^2(x), \dots, \hat{f}^B(x)$ sequencialmente, de tal forma que a árvore $\hat{f}^i(x)$ é ajustada utilizando os resíduos da árvore $\hat{f}^{i-1}(x)$. Considere $\hat{f}(x)$ a função estimada do modelo e que, inicialmente, $\hat{f}(x) = 0$. Em cada passo do algoritmo, a nova árvore $\hat{f}^i(x)$ é somada à função estimada $\hat{f}(x)$ utilizando-se um parâmetro de encolhimento $\lambda > 0$, isto é, no passo $i$, $\hat{f}(x)$ passa a ser $\hat{f}(x) + \lambda\hat{f}^i(x)$. Assim, os resíduos são atualizados em cada passo e $\hat{f}(x)$  se torna mais precisa\footnote{Obtém melhores predições, segundo alguma métrica de erro.} lentamente em regiões onde sua performance era ruim. O parâmetro de encolhimento $\lambda$, o número de árvores $B$ e o tamanho de cada árvore (controlado pelo número de nós terminais) são os principais hiperparâmetros do modelo e podem ser escolhidos por validação cruzada.

O \textit{gradient boosting} generaliza o \textit{boosting} substituindo o ajuste dos resíduos pela minimização de uma função de custo $L(y, f(x))$. Em cada etapa, ajustamos uma árvore não mais aos resíduos, mas sim às quantidades

\begin{displaymath}
g_i = - \left[\frac{\delta L(y_i - f(x_i))}{\delta f(x_i)}\right]_{f = f^{i-1}}, \quad i = 1, \dots, n.
\end{displaymath}
As quantidades $g_1, \dots, g_n$ representam o gradiente da função de custo em relação a cada árvore $f^i(x)$. Portanto, em cada etapa, minimizamos lentamente a função de custo a partir da nova árvore ajustada. Esse é um algoritmo de minimização é chamado de \textit{gradient descent} \cite{Kiefer1952}.

Por fim, o \textit{XGBoost} (ou \textit{extreme gradient boosting}) é uma implementação eficiente do \textit{gradient boosting} que utiliza alguns ``truques'' para otimizar o processo de estimação, como penalização das árvores para controlar a velocidade de aprendizado, aleatorização dos parâmetros para diminuir a correlação entre as árvores e encolhimento dos nós terminais para diminuir o sobreajuste. Esse algoritmo é um dos mais utilizados em problemas preditivos complexos hoje em dia, estando entre os três modelos que mais ganharam competições de modelagem preditiva.

Na linguagem R, podemos ajustar o \textit{XGBoost} utilizando a função \texttt{train()} do pacote \texttt{caret} com o argumento \texttt{method = "xgbTree"} ou diretamente as funções do pacote \texttt{xgboost}.

Assim como as \textit{random forests}, esses modelos são não interpretáveis e demandam o uso de técnicas gráficas para a avaliação da associação entre as variáveis. Discutiremos esse tópico na próxima seção.


\section{Interpretando modelos caixa-preta}
\label{sec:iml}

Ao contrário dos modelos apresentados no capítulo anterior, modelos como o \textit{random forest} ou o \textit{XGboost} não são interpretáveis. Isso implica que não conseguimos avaliar diretamente como cada preditor está associado com a variável resposta, impossibilitando que esses modelos sejam usados para inferência ou criando muita desconfiança a respeito do que esses modelos estão fazendo por trás das cortinas.

Nesta seção, vamos apresentar algumas técnicas que, sob algumas suposições ou restrições, visam elucidar a relação entre os preditores e os valores preditos, abrindo um pouco a caixa-preta dos modelos de aprendizado automático.

\subsection{Gráfico de dependência parcial}

O objetivo do gráfico de dependência parcial é mostrar o efeito marginal de um preditor no valor predito pelo modelo. A partir dele, podemos investigar qual é a forma e o sentido da relação entre cada preditor e a variável resposta.

Dado um modelo não interpretável $f(X, Z)$, um preditor sob investigação $X$ e um vetor $Z$ representando os outros preditores do modelo, a ideia por trás da construção do gráfico de dependência parcial consiste em:

\begin{itemize}
	\item[1.] fixar alguns valores para $X$, digamos $x = (x_{1}, \dots, x_{M})$;
	\item[2.] para cada $x_i$, $i = 1, \dots, M$, repetir:
	\subitem a) para cada uma das $n$ observações na amostra, substituir o valor observado de $X$ por $x_i$ e calcular o valor predito do modelo, $\hat{f}(x_i, z_{.})$;
	\subitem b) calcular a média das $n$ predições, $\bar{f}_i(x_i) = \frac{1}{n}\sum_{j=1}^{n}\hat{f}(x_i, z_j)$;
	\item[3.] construir o gráfico dos valores $x$ contra as médias das predições $\bar{f}(x)$.
\end{itemize}

A função $\bar{f}(x)$ podem ser interpretada como uma estimativa da distribuição marginal das predições que depende apenas do preditor X, obtida calculando-se a média das predições sobre os outros preditores. Essa estratégia é razoável apenas se os preditores $X$ e $Z$ são não correlacionados. Caso contrário, o uso dessa média como estimativa da distribuição marginal $\bar{f}(x)$ pode colocar peso em regiões inverossímeis ou de probabilidade zero.

\subsection{Gráfico da esperança condicional individual}

O gráfico da esperança condicional individual é equivalente ao gráfico de dependência parcial, com a diferença que aqui traçamos uma curva para cada observação da amostra, sendo que o gráfico de dependência parcial é a média das curvas do gráfico da esperança condicional individual.

O algoritmo para construção desse gráfico consiste nos passos (1) e (2a) da seção anterior, sendo que plotamos as $n$ curvas formadas pelos pontos $(x_1, \hat{f}(x_1, z_{j})), \dots, (x_M, \hat{f}(x_M, z_{j}))$, $j = 1, \dots, n$. Além de ser ainda mais intuitivo que o gráfico de dependência parcial, a grande vantagem do gráfico da esperança condicional individual é mostrar relações heterogêneas geradas por interações entre os preditores.

\subsection{Gráfico de efeitos locais acumulados}

O gráfico de efeitos locais acumulados é uma alternativa ao gráfico de dependência parcial no caso de preditores correlacionados. A estratégia aqui é, para cada valor $x$ do preditor sob investigação $X$, estimar a distribuição marginal das predições $\hat{f}(x)$ utilizando apenas as observações da amostra similares a $x$. Esse gráfico mostra qual é a predição do modelo em uma pequena região ao redor de $x$ para observações da base dentro dessa região.

Dado o preditor $X$ de interesse, o algoritmo  para construção desse gráfico é dado por:

\begin{enumerate}
	\item particionar $X$ em $M$ intervalos (usualmente os quantis de $X$ são utilizados);
	\item para cada uma das $m_i$ observações dentro do $i$-ésimo intervalo, calcular
	\[
	\hat{f}_{ij}^d = \hat{f}(x_i^{+}, z_j) - \hat{f}(x^{-}_i, z_j), \quad j = 1, \dots, m_i,
	\]
	sendo $x_i^{+}$ e $x_i^{-}$, respectivamente, os limites superior e inferior do intervalo $i$.
	\item Para cada valor $x$ de $X$, calcular a média acumulada 
	\[
	\bar{f}_a(x) = \sum_{i = 1}^{k(x)} \frac{1}{m_i} \sum_{j=1}^{m_i} \hat{f}_{ij}^d,
	\]
	sendo $k(x)$ o índice do intervalo ao qual o valor $x$ pertence.
	\item Calcular $\bar{f}_{ac}(x)$, o valor centralizado de $\bar{f}_a(x)$ considerando todas as $n$ observações da amostra:
	\[
	\bar{f}_{ac}(x) = \bar{f}_a(x) - \frac{1}{n}\sum_{i = 1}^{n}\bar{f}_a(x_i).
	\]
	\item Para cada valor observado de $X$, plotar $\bar{f}_{ac}(x^c_.) \times x^c_.$, sendo $x^c_1, \dots, x^c_{M-1}$ os pontos de corte utilizados para construir os $M$ intervalos em (1). 
	
\end{enumerate}

A partir desse algoritmo, podemos fazer algumas considerações sobre o gráfico de efeitos locais acumulados:

\begin{itemize}
	\item As diferenças calculadas no passo (2) explicam o termo \textit{efeitos locais}. Para cada valor $x$ do preditor $X$, essas diferenças estimam o quanto a predição é alterada por pequenas mudanças no valor de $x$. 
	\item Calcular a média das diferenças das predições em cada intervalo (em vez de apenas a média das predições) também garante que o efeito estimado para o preditor $X$ não seja influenciado pelo efeito de algum outro preditor correlacionado com $X$.
	\item Como cada $\frac{1}{m_i} \sum_{1}^{m_i} \hat{f}_i^d$ representa a diferença média nas predições quando $X$ varia localmente, dentro do intervalo $i$, a soma acumulada no passo (3) é realizada para que $\bar{f}_a(x)$ represente o efeito do preditor $X$ nas predições.
	\item Ao contrário do gráfico de dependência parcial e do gráfico de esperança condicional individual, que apresentam no eixo $y$ o valor predito, o gráfico de efeitos locais acumulados consideram nesse eixo a diferença em relação ao valor predito médio.
\end{itemize}



\subsection{\textit{Local interpretable Model-Agnostic Explanations} (LIME)}
\label{sec:lime}

Uma outra técnica utilizada para interpretar modelos caixa-preta é o \textit{Local interpretable Model-Agnostic Explanations} (LIME) \citep{Ribeiro2016}. Ela vem sendo muito utilizada para detectar, explicar e corrigir problemas com modelos preditivos, pois permite avaliar, para cada observação da amostra, quais preditores mais influenciaram a sua própria predição. 

O LIME assume que todo modelo complexo prevê valores parecidos para duas observações muito próximas, sendo possível encontrar um modelo simples que seja uma boa aproximação do modelo complexo na vizinhança da observação que queremos explicar. Assim, a partir de um modelo interpretável podemos obter para cada observação uma explicação sobre a predição feita pelo modelo não-interpretável.

O algoritmo consiste em:

\begin{enumerate}
	\item Para cada predição a ser explicada, permutar as observações da amostra $n$ vezes.
	\item Predizer cada uma das observações permutadas usando o modelo complexo.
	\item Calcular uma medida de distância e similaridade entre as permutações e a observação original. Geralmente a distância de Gower é utilizada \citep{Gower1971}.
	\item Selecionar as $m$ variáveis mais importantes utilizadas pelo modelo complexo para explicar os dados permutados.
	\item Ajustar um modelo interpretável aos dados permutados, utilizando as predições do modelo complexo como variável resposta e as $m$ variáveis selecionadas no passo anterior como preditores, ponderando pela medida de similaridade com a observação original.
	\item Usar as estimativas desse modelo simples como as explicações para o comportamento local do modelo complexo.	 
\end{enumerate}

Embora o LIME possa ser utilizado para explicar o modelo globalmente, utilizando uma grande quantidade de pontos da base de treino, essa abordagem funciona bem para problemas \textit{bem comportados}, como classificação de imagens, em que cada preditor representa um pixel, e textos, em que cada preditor representa uma palavra. Em problemas muito complexos, principalmente de regressão (variável resposta numérica) com muitos preditores, pode ser difícil encontrar modelos simples que expliquem bem as predições localmente.

A implementação do LIME exige definir como as observações serão permutadas, qual medida de similaridade será usada, qual o valor de $m$ e qual modelo interpretável será utilizado. Uma boa discussão sobre esses pontos pode ser encontrada na \href{https://cran.r-project.org/web/packages/lime/vignettes/Understanding_lime.html}{documentação} do pacote \texttt{lime}, no qual a técnica foi implementada na linguagem R.

\subsection{Exemplo}

Para ilustrar a utilização desses métodos gráficos de interpretação, vamos considerar um problema simples no qual queremos associar médias diária de NO$_x$ com médias diárias de temperatura e umidade. Vamos então ajustar uma floresta aleatória para esses dados e tentar interpretar os resultados do modelo.

Na Figura \ref{fig:cap-aprend-estat-tp-hm-nox} apresentamos o gráfico de dispersão do NO$_x$ contra cada uma das variáveis explicativas. Embora os gráficos de dispersão não evidencie essa relação, podemos observar pela série temporal do NO$_x$ (Figura \ref{fig:cap-aprend-estat-series-tp-hm-nox}) que as maiores concentrações desse poluente tendem a acontecer no inverno, onde o clima\footnote{Dados para a cidade de São Paulo, de 2008 a 2011.} é mais frio e seco. 

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.7\linewidth]{figuras/cap-aprend-estat-tp-hm-nox.pdf}
	\caption{Gráficos de dispersão da média diária de NO$_x$ contra as médias diárias de temperatura (Celsius) e umidade relativa do ar (\%).}
	\label{fig:cap-aprend-estat-tp-hm-nox}
\end{figure}


\begin{figure}[!h]
	\centering
	\includegraphics[width=0.7\linewidth]{figuras/cap-aprend-estat-series-tp-hm-nox.pdf}
	\caption{Gráficos de dispersão da média diária de NO$_x$ contra as médias diárias de temperatura (Celsius) e umidade relativa do ar (\%).}
	\label{fig:cap-aprend-estat-series-tp-hm-nox}
\end{figure}

Inicialmente, ajustamos modelos de regressão linear para esses dados, pois eles são interpretáveis e podemos comparar seus resultados com os da \textit{random forest}. Focando o exemplo apenas na interpretação da umidade, ajustamos modelos considerando (1) apenas a umidade como preditor, (2) umidade e temperatura como preditores e (3) umidade, temperatura e a interação entre as duas variáveis como preditores. Na Figura \ref{fig:cap-aprend-estat-interp-lm}, apresentamos os gráficos do valor predito contra a umidade para cada um dos modelos. Repare que, para os modelos (2) e (3), avaliamos o efeito de umidade para diferentes temperaturas, que é constante no modelo sem interação e diminui conforme a temperatura aumenta no modelo com interação.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.7\linewidth]{figuras/cap-aprend-estat-interp-lm.pdf}
	\caption{Valores preditos do NO$_x$ para cada modelo em função da umidade e da temperatura.}
	\label{fig:cap-aprend-estat-interp-lm}
\end{figure}

Ajustamos então uma floresta aleatória para os mesmos dados e construímos os gráficos de dependência parcial (PDP), esperança condicional individual (ICE) e efeitos locais acumulados (ALE) para interpretar os resultados (Figura \ref{fig:cap-aprend-estat-graficos-iml}). Pelo PDP e ICE, observamos um efeito estranho nos preditores para valores baixos de umidade. Isso acontece porque a temperatura e a umidade são correlacionadas, como podemos observar pelo gráfico de dispersão na Figura \ref{fig:cap-aprend-estat-tp-hm}. Como há poucos dias com baixa umidade, esses gráficos acabam capturando o efeito da temperatura na concentração de NO$_x$. Nessa situação, o gráfico adequado para analisar o efeito da umidade seria o ALE, pois ele avalia localmente o efeito de cada valor da umidade na predição, desconsiderando regiões com poucas observações e eliminando a influência dos outros preditores.

O ALE também nos mostra que a relação entre umidade e concentração de NO$_x$ pode ser não-linear, dado o aumento na predição quando aumentamos a umidade de 85\% para 100\%. Embora essa relação possa estar apenas sendo induzida pela ausência de outros preditores importantes para explicar a variabilidade do NO$_\textnormal{x}$, é interessante notar que não conseguimos identificá-la a partir do modelo de regressão linear considerado.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.7\linewidth]{figuras/cap-aprend-estat-graficos-iml.pdf}
	\caption{Gráficos de dependência parcial (PDP), esperança condicional individual (ICE) e efeitos locais acumulados (ALE) para a \textit{random forest}. A curva amarela no ICE representa a média de todas as retas individuais, isto é, o PDP.}
	\label{fig:cap-aprend-estat-graficos-iml}
\end{figure}

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.7\linewidth]{figuras/cap-aprend-estat-tp-hm.pdf}
	\caption{Gráfico de dispersão entre as médias diárias da temperatura e umidade.}
	\label{fig:cap-aprend-estat-tp-hm}
\end{figure}

