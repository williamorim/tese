%% ------------------------------------------------------------------------- %%
\chapter{Estratégias de modelagem preditiva}
\label{cap:aprendizado_estatistico}

\begin{flushright}
	\textcolor{gray}{
		\begin{tabular}{r}
			There are no routine statistical questions, \\
			only questionable statistical routines. \\
			--- Sir David Cox			
		\end{tabular} 
	}
	\vspace{5mm}
\end{flushright}

No capítulo anterior, apresentamos diversas classes de modelos úteis para fazer inferência em estudos de poluição do ar. A utilização desses modelos depende de suposições sobre a forma como as variáveis explicativas e as variável resposta estão relacionadas. De uma forma geral, essas suposições são feitas a partir de um modelo probabilístico para $Y$, cuja parametrização dependerá de alguma função das variáveis $\mathbf{X}$. O modelo de regressão linear (\ref{mod:linear}), por exemplo, assume as seguintes hipóteses:

\begin{itemize}
	\item a média de $Y$ depende das variáveis $\mathbf{X}$ a partir da relação $\beta_0 + \beta_1X_1 + \dots \beta_pX_p$ (linearidade e aditividade);
	\item a variância de $Y$, $\sigmatwo$, é constante para todas as observações na população.
\end{itemize}
Essas suposições, embora potencialmente restritivas, permitem que o modelo seja interpretável, isto é, ao estimarmos os coeficientes $\beta_0, \beta_1, \dots, \beta_p$, podemos avaliar como a variável $Y$ é influenciada por cada preditor $X_1, \dots, X_p$.

Nos últimos anos, um novo \textit{framework} de análise de dados se tornou muito popular, principalmente pela sua eficiência na resolução de problemas de predição, como detecção de imagens, transcrição de áudio e sistemas de recomendação. A chamada \textit{modelagem preditiva}\footnote{Também conhecida como aprendizado estatístico, aprendizagem automática ou aprendizado de máquina \textit{machine learning}.} envolve um conjunto de técnicas que visam gerar a estimativa mais precisa possível para uma quantidade ou fenômeno. Embora esse \textit{framework} foque no ajuste de modelos preditivos, muitas das estratégias adotadas por ele também podem ser aplicadas em problemas de inferência, cujo interesse é entender a relação entre as variáveis.

%TODO: escrever sobre modelos preditivos (árvores e redes neurais)

Neste capítulo, discutiremos as principais técnicas dentro da modelagem preditiva, como o conceito de sobreajuste (\textit{over-fitting}), os métodos de reamostragem, seleção de variáveis e regularização. Em seguida, introduziremos alguns modelos mais utilizados dentro desse contexto.


%Os métodos de reamostragem geralmente são utilizados para avaliar a performance de um modelo ou prover medidas de acurácia para as estimativas dos parâmetros. Dada uma classe de modelos, os métodos de seleção de variáveis e regularização permitem aumentarmos o poder preditivo e a interpretabilidade do modelo.

\section{Sobreajuste e o \textit{trade-off} entre viés e variância}
\label{sec:trade-off}

Ao utilizarmos um modelo estatístico para predição, estamos sujeitos a dois tipos de erro: um erro redutível e outro irredutível. No contexto apresentado na introdução do Capítulo \ref{cap:regressao}, dificilmente vamos conseguir uma estimativa perfeita para a função $f$, e essa imprecisão introduz erro nas predições do modelo. Esse erro é chamado de \textit{redutível}, pois sempre podemos encontrar uma candidata $\hat{f}$ mais próxima da verdadeira $f$. No entanto, como $Y$ depende também do termo $\epsilon$ (\ref{mod:y-equal-X-e}), mesmo se pudêssemos estimar $f$ com 100\% de precisão, ainda teríamos um erro associado. Por construção, o termo $\epsilon$ representa a variação em $Y$ que não pode ser explicada pelos preditores $\mathbf{X}$, e como essa imprecisão não pode ser reduzida, independentemente de qual $\hat{f}$ nós escolhermos, esse erro é chamado de \textit{irredutível}.

O grande desafio na hora de ajustar um modelo aos dados é encontrar uma $\hat{f}$ que minimize o erro redutível, isto é, queremos encontrar um modelo que, utilizando os dados disponíveis, gere as estimativas o mais precisas possível sobre o fenômeno sob estudo. Dentro da modelagem preditiva, essa tarefa é equivalente a minimizar duas quantidades do modelo: o \textit{viés} e a \textit{variância}.

Para entender melhor o que essas quantidades representam, imagine que precisamos ajustar um modelo para os dez pontos apresentados na Figura \ref{cap-aprend-estat-trade-off} (a). Podemos começar ajustando um modelo de regressão linear simples,

\begin{displaymath}
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i, \quad i = 1, \dots, 10,
\end{displaymath}
e calcular a raiz do erro quadrático médio (definido na Seção \ref{sec:reg-quali-mod}) para avaliar o quanto a reta ajustada se afasta dos pontos. Uma forma de tentar melhorar o ajuste seria acrescentar um termo quadrático e verificar se o RMSE diminui. Podemos repetir esse procedimento acrescentando termos de graus cada vez maior\footnote{Esses são os modelos polinomiais apresentados na Seção \ref{sec:linearidade}.}, até encontrarmos o menor RMSE.

Na Tabela \ref{tab:cap-aprend-estat-trade-off-10-obs}, apresentamos o RMSE obtido para os modelos de regressão polinomial até o nono grau\footnote{O modelo de regressão linear simples é um modelo polinomial de grau 1.}. Observe que, conforme aumentamos a complexidade do modelo (grau do polinômio), o RMSE diminui, até chegar em 0 para o polinômio de grau 9. Se utilizarmos puramente o RMSE como medida da performance do modelo, escolheríamos justamente esse polinômio como modelo final. No entanto, pela Figura \ref{cap-aprend-estat-trade-off} (b), observamos que esse modelo está claramente mal ajustado aos dados.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{figuras/cap-aprend-estat-trade-off.pdf}
	\caption{Exemplo do \textit{trade-off} entre viés e variância. (a) Conjunto de 10 pontos que gostaríamos de ajustar. (b) Modelo de regressão linear simples (vermelho), modelo de regressão polinomial de grau 2 (amarelo) e modelo de regressão polinomial de grau 9 (azul), ajustados aos 10 pontos. (c) Amostra de 100 novas observações plotadas juntas dos modelos polinomiais ajustados nas 10 observações iniciais. (d) Modelos de regressão polinomial de graus 1 (vermelho), 2 (amarelo) e 9 (azul) ajustados aos 100 novos pontos.}
	\label{cap-aprend-estat-trade-off}
\end{figure}

\begin{table}[h!]
	\centering
	\caption{Raiz do erro quadrático médio (RMSE) para os modelos polinomiais de grau 1 a 9 ajustados com 10 e 110 observações no exemplo da Figura \ref{cap-aprend-estat-trade-off}.}
	\begin{tabular}{M{2cm}|M{3cm}|M{3cm} N}
		\hline
		Grau do polinômio & RMSE (10 obs.) & RMSE (100 obs.) & \\
		\hline
		1 & 0.204 & 0.360 & \\[10pt]
		\hline
		2 & 0.149 & 0.226 & \\[10pt]
		\hline
		3 & 0.140 & 0.199 & \\[10pt]
		\hline
		4 & 0.140 & 0.198 & \\[10pt]
		\hline
		5 & 0.102 & 0.289 & \\[10pt]
		\hline
		6 & 0.086 & 0.360 & \\[10pt]
		\hline
		7 & 0.063 & 0.320 & \\[10pt]
		\hline
		8 & 0.031 & 1.152 & \\[10pt]
		\hline
		9 & 0.000 & 3.904 & \\[10pt]
		\hline
	\end{tabular}
	\label{tab:cap-aprend-estat-trade-off-10-obs}
\end{table}

Considere agora, nesse mesmo exemplo, que conseguimos uma nova amostra com mais 100 observações geradas pelo mesmo fenômeno que gerou as 10 primeiras. A Figura \ref{cap-aprend-estat-trade-off} (c) ratifica o quanto o modelo polinomial de grau 9 se ajustou mal aos dados, enquanto os modelos de grau 1 e 2 parecem escolhas mais razoáveis. Podemos observar ainda na Tabela \ref{tab:cap-aprend-estat-trade-off-10-obs} que o RMSE do modelo polinomial de grau nove calculado nas 100 novas observações\footnote{Aqui, os modelos não foram reajustados. Foram considerados os modelos ajustados apenas com as 10 primeiras observações} é o maior entre todos os candidatos. Por fim, observe na Figura \ref{cap-aprend-estat-trade-off} (d) como a curva desse modelo muda quando o ajustamos agora usando as 100 novas observações.

Durante a modelagem, estamos sempre em busca de modelos que se ajustem bem à amostra, mas que também possam ser generalizados para a população. Nesse sentido, chamamos de \textit{viés} o quanto o modelo ajustado está distante dos dados da amostra e de \textit{variância} o quanto o modelo erra ao utilizarmos para prever novas observações. O viés representa o erro induzido por aproximar um fenômeno real, que pode pode ser extremamente complicado, por um modelo muito mais simples e a \textit{variância} o quanto as estimativas dos parâmetros do modelo mudariam se nós tivéssemos usado uma amostra diferente. Assim, dizemos que modelos mal ajustados apresentam alto viés e modelos que erram muito na predição de novas observações apresentam alta variância. 

É muito comum utilizarmos estratégias que se preocupam apenas com a minimização do viés. Essas estratégias geram modelos mais complexos, visando um ajuste cada vez melhor aos dados, sem levar em conta o quanto isso será representativo em um contexto mais geral. No exemplo anterior, isso fica claro com o ajuste de polinômios de grau cada vez maior aos dados. Além disso, o polinômio de grau 9 ilustra, de forma bem simplificada, o conceito de sobreajuste, que ocorre quando o modelo absorve de forma inadequada comportamentos da amostra que não são generalizáveis para a população. Modelos sobreajustados apresentam baixo viés, mas alta variância, não sendo apropriados para representar o fenômeno de interesse. Controlar o \textit{trade-off} entre o viés e a variância é um dos maiores desafios da modelagem preditiva.

Na presença de muitos preditores, não é possível visualizar graficamente o sobreajuste, como mostrado no exemplo. Por isso, na prática, nem sempre é trivial identificar um modelo sobreajustado. Para contornar esse problema, apresentaremos na próxima seção medidas utilizadas para quantificar o viés e a variância de um modelo.

\section{Estimando a performance do modelo}

Na Seção \ref{sec:reg-quali-mod}, vimos que o R$^2$ e a raiz do erro quadrático médio (RMSE) podem ser utilizados para avaliar a qualidade do ajuste de um modelo de regressão linear. Embora o $R^2$ seja utilizado apenas nessa classe de modelos, o RMSE pode ser calculado em qualquer contexto. Em alguns casos, podemos querer utilizar o erro absoluto médio (MAE), que dá menos peso para erros em valores muito altos da variável resposta. Em outros, pode ser razoável penalizar mais justamente os valores mais altos, e então construímos uma medida que distribui os pesos dessa maneira. 

A escolha da métrica de performance vai depender sempre do objetivo do estudo. Independentemente da medida escolhida, ao calculá-la para as próprias observações utilizadas no ajuste, temos uma estimativa do viés do modelo, isto é, o quanto o modelo escolhido se ajusta bem à amostra. Essa quantidade é chamada de \textit{erro de treino}. Para obtermos uma estimativa da variância, precisamos calcular a medida de performance para observações não utilizadas no ajuste, que representem uma nova amostra do fenômeno sob estudo. Essa quantidade é chamada de \textit{erro de teste}.

Na prática, nem sempre teremos à disposição uma nova base de dados para a estimação da variância. Uma alternativa nesses casos é utilizar técnicas de validação cruzada, que permite separar a base em observações utilizadas para \textit{treinar} o modelo e observações para estimar sua performance. Essas técnicas serão o tema da próxima seção.

\section{Métodos de reamostragem}

A reamostragem consiste na técnica de gerar novas amostras a partir de uma base principal. As técnicas de reamostragem mais utilizadas são a validação cruzada e o \textit{bootstraping}. 


\subsection{Validação cruzada}

Como na maioria dos estudos não é possível obter facilmente novas observações, podemos calcular o erro de teste, a estimativa da variância do modelo, dividindo a amostra original em duas partes: uma utilizada para o ajuste do modelo (amostra de treino) e a outra para o cálculo do erro (amostra de teste), essa última agindo como se fosse um conjunto de novas observações. Essa técnica é conhecida como \textit{validação cruzada} \citep{James2013}. Há diversos tipos de validação cruzada, que variam a depender da forma utilizada para dividir a amostra. Nesta seção, apresentaremos os principais tipos de validação cruzada e discutiremos as vantagens e desvantagens de cada um.

%Quando precisamos escolher valores para hiperparâmetros do modelo, como o grau de suavização de um modelo aditivo generalizado (Seção \ref{sec:gam}) ou o $\lambda$ do LASSO (Seção \ref{sec:lasso}), podemos dividir a amostra em ainda mais uma parte: uma amostra de validação. Nesses casos, os modelos são treinados com a amostra de teste e, para diversos valores do hiperparâmetro, calculamos o erro de teste na amostra de validação. Escolhemos então o hiperparâmetro com menor erro de teste e utilizamos a amostra de teste para calcular o erro de teste do modelo final. Repare que a amostra de teste nunca é utilizada para ajustar o modelo.

\subsubsection{Amostra de validação}

A amostra de validação é a forma mais simples de validação cruzada. A estratégia consiste em dividir aleatoriamente as observações em um conjunto de treino, usado para ajustar o modelo, e outro de teste, utilizado exclusivamente para estimar o erro de teste. 

A proporção de observações em cada uma depende do tamanho amostral. Costuma-se utilizar 30\% da amostra original no conjunto de teste, mas esse número pode ser menor para amostras muito grandes (mais de 100 mil observações, por exemplo).

As maiores vantagens dessa técnica é a sua simplicidade e a necessidade de se ajustar o modelo uma única vez. No entanto, conforme discutido em \cite{James2013}, a amostra de validação apresenta duas potenciais desvantagens:

\begin{itemize}
	\item a estimativa do erro de teste pode apresentar alta variabilidade, dependendo de quais observações ficaram na amostra de treino e quais fiaram na amostra de validação;
	\item como a acurácia de modelos estatísticos é menor quando ajustados com menos observações, e apenas parte das observações são utilizadas para treinar o modelo, o  erro de teste pode estar sendo superestimado. 
\end{itemize}

A seguir, apresentaremos o LOOCV, um método de validação cruzada que não possui essas limitações.

\subsubsection{LOOCV}

Considere uma amostra com $n$ observações. A validação cruzada \textit{leave-one-out} (LOOCV) consiste em rodar o modelo escolhido $n$ vezes, sendo que, em cada ajuste, deixamos de fora a $i$-ésima observação, $i = 1, \dots, n$, e a utilizamos para calcular o erro de teste. A estimativa final do erro de teste será então a média das $n$ medidas parciais. Uma esquematização dessa técnica está representada na Figura \ref{cap-aprend-estat-loocv}.


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.5\linewidth]{figuras/cap-aprend-estat-esquema-loocv.png}
	\caption{Esquematização da validação cruzada \textit{leave-one-out}.}
	\label{cap-aprend-estat-loocv}
\end{figure}

Repare que, neste caso, todas as observações são utilizadas no ajuste do modelo e na estimativa do erro de teste, o que elimina as limitações da amostra de validação. No entanto, uma desvantagem aqui é a necessidade de ajustar o modelo $n$ vezes. Quando $n$ é muito grande, a LOOCV pode exigir muito esforço computacional, inviabilizando a sua utilização em muitos casos. A seguir, apresentamos validação cruzada \textit{k-fold}, uma generalização da LOOCV que não possui essa contrapartida computacional.

\subsubsection{K-fold}

Podemos generalizar a LOOCV dividindo a amostra original aleatoriamente em $k$ grupos com aproximadamente a mesma quantidade de observações. Então ajustando o modelo $k$ vezes, sendo que em cada ajuste selecionamos um grupo diferente como amostra de teste. Essa abordagem é chamada de \textit{k-fold}. Note que a LOOCV é o caso especial em que $k = n$. Na prática, escolhemos valores de $k$ entre 3 e 10, sendo que $k = 5$ é o mais utilizado (Figura \ref{cap-aprend-estat-esquema-k-fold-cv}).

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{figuras/cap-aprend-estat-esquema-k-fold-cv.png}
	\caption{Esquematização da validação cruzada \textit{k-fold}, com $k = 5$.}
	\label{cap-aprend-estat-esquema-k-fold-cv}
\end{figure}

A maior vantagem da validação cruzada \textit{k-fold} sobre a LOOCV é computacional. Em vez de ajustarmos o modelo $n$ vezes, ajustamos apenas $k$, sendo que $k << n$. E como estamos utilizando todas as observações para treinar o modelo, não temos as limitações de se utilizar uma única amostra de validação.

Como discutimos até agora, a validação cruzada é geralmente utilizada para avaliar a performance do modelo. A seguir, apresentaremos uma técnica de reamostragem muito utilizada também para a estimação de quantidades acerca dos parâmetros do modelo.

\subsection{Bootstrapping}

O \textit{bootstrapping} é uma poderosa ferramenta estatística utilizada para quantificar incertezas associadas a estimadores e modelos estatísticos. Ela consiste em gerar $m$ novas amostras a partir de sorteios com reposição da amostra original. Para cada uma das amostras geradas, ajustamos o modelo escolhido e guardamos as estimativas dos parâmetros. Ao repetirmos esse processo para as $m$ amostras, teremos $m$ estimativas diferentes para cada parâmetro do modelo. Assim, para cada parâmetro, podemos, por exemplo, calcular o desvio-padrão dessas $m$ estimativas e utilizar essa medida como o erro-padrão associado ao coeficiente. Repare que as estimativas dos parâmetros do modelo devem ser estimadas usualmente, utilizando a amostra original. Nesse exemplo, o \textit{bootstrapping} seria usado apenas para estimar a variabilidade dos coeficientes.

Essa técnica é utilizada principalmente quando não conhecemos a distribuição dos estimadores do modelo ou quando precisamos controlar outras fontes de variabilidade. \cite{Salvo2014} e \cite{Salvo2017}, por exemplo, utilizaram o \textit{bootstrapping} para estimar o erro-padrão dos coeficientes do modelo de regressão linear ajustado para associar a concentração de ozônio na cidade de São Paulo com a proporção estimada de veículos bicombustíveis rodando a gasolina. Segundo os autores, essa estratégia foi utilizada para contemplar a variação causada pelo erro de medida presente na estimação da proporção de carros rodando a gasolina e na medição das condições climáticas.

O \textit{bootstrapping} também pode ser para a estimação da performance do modelo. Neste caso, cada uma das $m$ amostras é utilizada como conjunto de treino e as observações que foram sorteadas em cada amostra é utilizada com conjunto de teste. Assim como na LOOCV, se $m$ for muito grande, essa estratégia pode gerar um esforço computacional muito alto.

Mais informações sobre o bootstrap podem ser encontradas em \cite{James2013}.

%Essa técnica pode ser utilizada, por exemplo, para estimar o erro-padrão dos coeficientes de um modelo de regressão.

\section{Seleção de variáveis}

Muitas vezes, na construção do modelo, incluímos variáveis que não são associadas com o fenômeno sob estudo. Isso acontece principalmente quando temos pouco conhecimento sobre o mecanismo gerador do fenômeno ou quando estamos justamente investigando quais fatores estão associados a ele. 

Como essas variáveis irrelevantes geram uma complexidade desnecessária no modelo, é apropriado pesarmos em estratégias para retirá-las da análise, aumentando assim a interpretabilidade dos resultados.

Nesta seção, apresentaremos algumas técnicas de seleção de variáveis que podem ser utilizadas em qualquer classe de modelos estatísticos.

\subsection{Selecionando o melhor subconjunto de preditores}

A maneira mais simples que podemos pensar para selecionar variáveis em um modelo é ajustar todas as possíveis combinações dos $p$ preditores e avaliar qual produz o melhor ajuste segundo alguma métrica. Essa estratégia é chamada de \textit{melhor subconjunto de preditores} (\textit{best subset selection}, em inglês) e seu procedimento de seleção pode ser resumido pelos passo abaixo:

\begin{enumerate}
	\item Ajustar o modelo nulo, sem nenhum preditor.
	\item Para $k = 1, \dots, p$, ajustar todos os modelos com k preditores e escolher o melhor entre eles, isto é, aquele com menor RSME ou maior $R^2$ por exemplo.
	\item Para cada um dos $p+1$ modelos escolhidos, selecionar o melhor usando o $R^2$ ajustado, RMSE calculado por validação cruzada (erro de teste), AIC ou BIC\footnote{O AIC e o BIC são medidas da qualidade do ajuste penalizadas pelo número de parâmetros do modelo. Mais informações, consultar \cite{James2013}}.
\end{enumerate}
Observe que a métrica utilizada para selecionar o modelo final deve ser penalizada pelo número de parâmetros, pois, caso contrário, escolheríamos sempre o modelo com mais preditores.

Para um número relativamente pequeno de variáveis, selecionar o melhor subconjunto de preditores é uma estratégia conceitualmente simples e de fácil execução. No entanto, conforme $p$ cresce, essa técnica pode se tornar computacionalmente inviável. Na Tabela \ref{tab:3-preditores-selecao-melhor-subconjunto} apresentamos os 7 modelos que precisaríamos ajustar se tivéssemos 3 preditores, $X_1$, $X_2$ e$X_3$, e um modelo de regressão linear (Seção \ref{sec:modelo-linear}). Para $p = 20$, por exemplo, precisaríamos rodar mais de um milhão de modelos, o que poderia inviabilizar a execução dessa estratégia.

\begin{table}[h!]
	\centering
	\caption{Modelos de regressão linear que devem ser ajustados para selecionar o melhor subconjunto de variáveis no caso com 3 preditores.}
	\begin{tabular}{c|c|c}
		\hline 
		Uma variável & Duas variáveis & Três variáveis \\ 
		\hline 
		$Y = \beta_0 + \beta_1 X_1 + \epsilon$ & $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$ &  \multirow{3}{5.5cm}{$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 +  \epsilon$} \\  
		$Y = \beta_0 + \beta_1 X_2 + \epsilon$ & $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_3 + \epsilon$  \\
		$Y = \beta_0 + \beta_1 X_3 + \epsilon$ & $Y = \beta_0 + \beta_1 X_2 + \beta_2 X_3 + \epsilon$ \\ 
		\hline 
	\end{tabular}
    \label{tab:3-preditores-selecao-melhor-subconjunto}	
\end{table}

A seguir, apresentamos algumas estratégias computacionalmente eficientes para aplicarmos em problemas com muitos preditores.

\subsection{Stepwise}

Os métodos \textit{stepwise} são algoritmos de seleção de variáveis que visam encontrar o melhor sub-conjunto de preditores dentro de um conjunto restrito de combinações em vez de ajustar todos os 2$^p$ modelos possíveis. 

A diferença entre cada método \textit{stepwise} está em como as variáveis são adicionas ou retiradas do modelo em cada passo. Os mais utilizados são o \textit{foward stepwise} e o \textit{backward stepwise}.

O \textit{foward stepwise} consiste na execução dos seguintes passos:

\begin{enumerate}
	\item Ajuste o modelo nulo ($M_0$), sem preditores.
	\item Ajuste todos os $p$ modelos com 1 preditor e escolha o melhor\footnote{Maior $R^2$, por exemplo.} ($M_1$).
	\item Ajuste todos os $p-1$ modelos com 2 preditores que contenham o preditor selecionado no passo anterior e escolha o melhor ($M_2$).
	\item De forma análoga, ajuste os modelos com 3, 4, ..., $p$ preditores, mantendo sempre como base o modelo obtido anteriormente, e em cada passo escolha o melhor ($M_3$, $M_4$, \dots, $M_p$).
	\item Escolha o melhor modelo entre $M_0$, $M_1$, \dots, $M_p$ utilizando erro preditivo, AIC, BIC ou $R^2$ ajustado.
\end{enumerate}

Repare que o \textit{foward stepwise} diminui o número de modelos ajustados de $2^p$ para $1 + p(p + 1)/2$. Para $p = 20$, o número de modelos diminui de 1.048.576 para 211.  

A ideia do método \textit{backward stepwise} é parecida com a do \textit{foward}. A diferença é que começamos no passo 1 com o modelo completo ($M_p$), com todos os preditores, e nos passo seguintes retiramos cada um dos preditores e ajustamos os modelos correspondentes, selecionando sempre aquele com maior $R^2$ ($M_{p-1}, M_{p-2}, \dots, M_0$). Ao fim, escolhemos o melhor entre os modelos $M_0$, $M_1$, \dots, $M_p$ utilizando erro preditivo, AIC, BIC ou $R^2$ ajustado. O número de modelos ajustados nesse caso é igual ao do \textit{foward stepwise}.

Ainda existem métodos \textit{stepwise} híbridos, nos quais os preditores são adicionados sequencialmente, assim como no \textit{foward stepwise}, mas em cada etapa é avaliado se um dos preditores já incluídos deve ou não sair do modelo. Essa estratégia tenta considerar mais modelos, chegando mais próximo da seleção do melhor sub-conjunto discutida na seção anterior. Para mais informações, consultar \cite{Nelder1972}.

\section{Regularização}

Os métodos de seleção de sub-conjuntos de preditores apresentados nas seções anteriores envolvem o ajuste de diversos modelos e a escolha do melhor segundo alguma métrica. Uma outra forma de selecionar variáveis é a partir das \textit{técnicas de regularização}. Essas técnicas, a princípio, envolvem o ajuste de um único modelo e introduzem no processo de estimação penalizações que limitam as estimativas dos coeficientes, as encolhendo na direção do zero.

A utilização da regularização pode levar a uma redução substancial da variância do modelo ao custo de um pequeno aumento no viés. Apresentaremos nesta seção as formas mais utilizadas de regularização: a regressão ridge e o LASSO. 

\subsubsection{Regressão Ridge}

De uma forma geral, o processo de estimação dos parâmetros de um modelo consiste na minimização de uma função de perda $L(y, f(x))$ que depende dos dados observados $(x, y)$ e do modelo escolhido $(f(\cdot))$. As técnicas de regularização consistem em adicionar uma penalidade nessa função de perda, de tal forma que os coeficientes dos preditores pouco associados a variável resposta sejam encolhidos na direção do zero.

No caso da regressão ridge \citep{James2013}, essa penalização é dada por

\begin{displaymath}
	L(y, f(x)) + \lambda \sum_{j=1}^p \beta_j^2,
\end{displaymath}
sendo $\beta_1, \dots, \beta_p$ os parâmetros do modelo $f(\cdot)$ e $\lambda$ um hiperparâmetro\footnote{Hiperparâmetros são parâmetros que não são estimados diretamente pelos dados.} que controla o impacto da penalização nas estimativas dos coeficientes. Quando $\lambda = 0$, o termo é anulado e as estimativas são calculadas sem penalização. Conforme $\lambda \longrightarrow \infty$, os coeficientes $\beta_j$ passam a ser penalizados, encolhendo seus valores na direção do zero. A vantagem desse comportamento está na potencial redução da variância do modelo, em troca de um pequeno aumento do viés, já que os coeficientes menos importantes recebem cada vez menos peso. Assim, a regularização é uma alternativa para lidarmos com o \textit{trade-off} entre viés e variância discutido na Seção \ref{sec:trade-off}.

No caso da regressão ridge, é possível mostrar que, para qualquer $i = 1, \dots, p$,  $\beta_i = 0$ apenas se $\lambda = \infty$. Isso significa que não estamos fazendo seleção de variáveis, isto é, o modelo ajustado sempre terá todos os preditores. Apesar de estarmos melhorando a performance do modelo diminuindo o peso dos preditores menos importantes, isso pode não ser o ideal quando quisermos de fato eliminar variáveis do modelo. Nesses casos, uma boa alternativa é utilizar o LASSO.

\subsubsection{LASSO}

O LASSO (\textit{least absolute shrinkage and selection operator}) é uma técnica análoga à regressão ridge, mas com penalização dada por

\begin{displaymath}
	L(y, f(x)) +\lambda \sum_{j=1}^p |\beta_j|.
\end{displaymath}
Para $\lambda$ grande o suficiente, essa penalização força que alguns dos coeficientes sejam estimados exatamente como 0 e os preditores associados serão eliminados do ajuste. Assim, ao utilizarmos o LASSO, estamos ao mesmo tempo reduzindo a variância do modelo e executando seleção de variáveis.

Um ponto importante sobre a aplicação das técnicas de regularização é a escala dos preditores. A maioria dos processos de estimação usuais são invariantes à escala em que os preditores foram medidos, isto é, ajustar o modelo usando o preditor $X_1$ ou $cX_1$, $c$ uma constante qualquer, não mudará a interpretação dos resultados. No caso da regressão ridge e do LASSO, a escala dos preditores influenciam não só a estimativa dos próprios coeficientes, mas também a estimativa dos outros parâmetros do modelo. Dessa forma, um passo importante anterior à aplicação dessas técnicas é a padronização dos preditores, de tal forma que todos fiquem com a mesma média e variância. Essa padronização pode ser feita a partir da fórmula

\begin{equation}
	\tilde{X}_{ij} = \frac{X_{ij} - \bar{X}_j}{\sqrt{\frac{1}{n}\sum_{i =1}^n (X_{ij} - \bar{X}_j)^2}}, 	
\end{equation}
sendo o denominador dessa expressão a estimativa do desvio-padrão do $j$-ésimo preditor. Consequentemente, todos os preditores terão média 0 desvio-padrão igual a 1.

Outra questão sobre a utilização da regularização é a estimação do erro-padrão dos coeficientes. Ainda há pouca literatura sobre como essas quantidades devem ser estimadas. \REF Uma boa alternativa nesse caso é calcular o erro-padrão por meio do \textit{bootstrapping}.

Para uma discussão mais aprofundada sobre a interpretação da regressão ridge e do LASSO, consulte o Capítulo 6 de \cite{James2013}. Para o desenvolvimento matemático dessas técnicas, o Capítulo 5 de \cite{Hastie2008} é uma ótima referência.
