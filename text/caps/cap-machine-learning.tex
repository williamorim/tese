%% ------------------------------------------------------------------------- %%
\chapter{Estratégias de modelagem preditiva}
\label{cap:aprendizado_estatistico}

\begin{flushright}
	\textcolor{gray}{
		\begin{tabular}{r}
			There are no routine statistical questions, \\
			only questionable statistical routines. \\
			--- Sir David Cox			
		\end{tabular} 
	}
	\vspace{5mm}
\end{flushright}

Nos últimos anos, uma nova abordagem de análise de dados se tornou muito popular, principalmente pela sua eficiência na resolução de problemas de predição, como detecção de imagens, transcrição de áudio e sistemas de recomendação. A chamada \textit{modelagem preditiva}\footnote{Também conhecida como aprendizado estatístico, aprendizagem automática ou aprendizado de máquina \textit{machine learning}.} envolve um conjunto de técnicas que visam gerar a estimativa mais precisa possível para uma quantidade ou fenômeno.

No capítulo anterior, apresentamos diversas classes de modelos úteis para fazer inferência em estudos de poluição do ar. A utilização desses modelos depende de suposições sobre a forma como as variáveis explicativas e as variável resposta estão relacionadas. De uma forma geral, essas suposições são feitas a partir de um modelo probabilístico para a variável resposta $Y$, cuja parametrização dependerá de alguma função dos preditores\footnote{Muita da literatura sobre modelagem preditiva vem da área da Ciência da Computação. Os computólogos, de uma maneira geral, denominam as variáveis respostas como variáveis de saída ou \textit{outputs} e os preditores como variáveis de entrada ou \textit{inputs}.} $\mathbf{X}$. O modelo de regressão linear (\ref{mod:linear}), por exemplo, assume as seguintes hipóteses:

\begin{itemize}
	\item a média de $Y$ depende das variáveis $\mathbf{X}$ a partir da relação $\beta_0 + \beta_1X_1 + \dots \beta_pX_p$ (linearidade e aditividade);
	\item a variância de $Y$, $\sigmatwo$, é constante para todas as observações na população.
\end{itemize}
Essas suposições, embora potencialmente restritivas, permitem que o modelo seja interpretável, isto é, ao estimarmos os coeficientes $\beta_0, \beta_1, \dots, \beta_p$, podemos avaliar como a variável $Y$ é influenciada por cada preditor $X_1, \dots, X_p$.

As técnicas e modelos utilizados para modelagem preditiva deixam a interpretabilidade parcilamente de lado para focar na precisão dos modelos ajustados. Além disso, as estratégias dentro dessa abordagem têm uma preocupação especial com o sobreajuste, que representa o quanto o modelo pode ser generalizado para além da amostra. Embora essa abordagem esteja focada no ajuste de modelos preditivos, muitas das estratégias adotadas por ele também podem ser aplicadas em problemas de inferência.

%TODO: escrever sobre modelos preditivos (árvores e redes neurais)

Neste capítulo, discutiremos com mais detalhes o conceito de sobreajuste (\textit{over-fitting}) e apresentaremos métodos de reamostragem, seleção de variáveis e regularização. Em seguida, introduziremos alguns modelos bastante utilizados dentro desse contexto.

%Os métodos de reamostragem geralmente são utilizados para avaliar a performance de um modelo ou prover medidas de acurácia para as estimativas dos parâmetros. Dada uma classe de modelos, os métodos de seleção de variáveis e regularização permitem aumentarmos o poder preditivo e a interpretabilidade do modelo.

\section{Sobreajuste e o balanço entre viés e variância}
\label{sec:trade-off}

Ao utilizarmos um modelo estatístico para predição, estamos sujeitos a dois tipos de erros: um erro redutível e outro irredutível. No contexto apresentado na introdução do Capítulo \ref{cap:regressao}, dificilmente vamos conseguir uma estimativa perfeita para a função $f$, e essa imprecisão introduz erro nas predições do modelo. Esse erro é chamado de \textit{redutível}, pois sempre podemos encontrar uma candidata $\hat{f}$ mais próxima da verdadeira $f$. No entanto, como $Y$ depende também do termo $\epsilon$ (\ref{mod:y-equal-X-e}), mesmo se pudêssemos estimar $f$ com 100\% de precisão, ainda teríamos um erro associado. Por construção, o termo $\epsilon$ representa a variação em $Y$ que não pode ser explicada pelos preditores $\mathbf{X}$, e como essa imprecisão não pode ser reduzida, independentemente de qual $\hat{f}$ nós escolhermos, esse erro é chamado de \textit{irredutível}.

O grande desafio na hora de ajustar um modelo aos dados é encontrar uma $\hat{f}$ que minimize o erro redutível, isto é, queremos encontrar um modelo que, utilizando os dados disponíveis, gere as estimativas o mais precisas possível sobre o fenômeno sob estudo. Dentro da modelagem preditiva, essa tarefa é equivalente a minimizar duas quantidades do modelo: o \textit{viés} e a \textit{variância}.

Para entender melhor o que essas quantidades representam, imagine que precisamos ajustar um modelo para os dez pontos apresentados na Figura \ref{cap-aprend-estat-trade-off} (a). Podemos começar ajustando um modelo de regressão linear simples,

\begin{displaymath}
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i, \quad i = 1, \dots, 10,
\end{displaymath}
e calcular a raiz do erro quadrático médio (definido na Seção \ref{sec:reg-quali-mod}) para avaliar o quanto a reta ajustada se afasta dos pontos. Uma forma de tentar melhorar o ajuste seria acrescentar um termo quadrático e verificar se o RMSE diminui. Podemos repetir esse procedimento acrescentando termos de graus cada vez maior\footnote{Esses são os modelos polinomiais apresentados na Seção \ref{sec:linearidade}.}, até encontrarmos o menor RMSE.

Na Tabela \ref{tab:cap-aprend-estat-trade-off-10-obs}, apresentamos o RMSE obtido para os modelos de regressão polinomial até o nono grau\footnote{O modelo de regressão linear simples é um modelo polinomial de grau 1.}. Observe que, conforme aumentamos a complexidade do modelo (grau do polinômio), o RMSE diminui, até chegar em 0 para o polinômio de grau 9. Se utilizarmos puramente o RMSE como medida da performance do modelo, escolheríamos justamente esse polinômio como modelo final. No entanto, pela Figura \ref{cap-aprend-estat-trade-off} (b), observamos que esse modelo está claramente mal ajustado aos dados.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{figuras/cap-aprend-estat-trade-off.pdf}
	\caption{Exemplo do \textit{trade-off} entre viés e variância. (a) Conjunto de 10 pontos que gostaríamos de ajustar. (b) Modelo de regressão linear simples (vermelho), modelo de regressão polinomial de grau 2 (amarelo) e modelo de regressão polinomial de grau 9 (azul), ajustados aos 10 pontos. (c) Amostra de 100 novas observações plotadas juntas dos modelos polinomiais ajustados nas 10 observações iniciais. (d) Modelos de regressão polinomial de graus 1 (vermelho), 2 (amarelo) e 9 (azul) ajustados aos 100 novos pontos.}
	\label{cap-aprend-estat-trade-off}
\end{figure}

\begin{table}[h!]
	\centering
	\caption{Raiz do erro quadrático médio (RMSE) para os modelos polinomiais de grau 1 a 9 ajustados com 10 e 110 observações no exemplo da Figura \ref{cap-aprend-estat-trade-off}.}
	\begin{tabular}{M{2cm}|M{3cm}|M{3cm} N}
		\hline
		Grau do polinômio & RMSE (10 obs.) & RMSE (100 obs.) & \\
		\hline
		1 & 0.204 & 0.360 & \\[10pt]
		\hline
		2 & 0.149 & 0.226 & \\[10pt]
		\hline
		3 & 0.140 & 0.199 & \\[10pt]
		\hline
		4 & 0.140 & 0.198 & \\[10pt]
		\hline
		5 & 0.102 & 0.289 & \\[10pt]
		\hline
		6 & 0.086 & 0.360 & \\[10pt]
		\hline
		7 & 0.063 & 0.320 & \\[10pt]
		\hline
		8 & 0.031 & 1.152 & \\[10pt]
		\hline
		9 & 0.000 & 3.904 & \\[10pt]
		\hline
	\end{tabular}
	\label{tab:cap-aprend-estat-trade-off-10-obs}
\end{table}

Considere agora, nesse mesmo exemplo, que conseguimos uma nova amostra com mais 100 observações geradas pelo mesmo fenômeno que gerou as 10 primeiras. A Figura \ref{cap-aprend-estat-trade-off} (c) ratifica o quanto o modelo polinomial de grau 9 se ajustou mal aos dados, enquanto os modelos de grau 1 e 2 parecem escolhas mais razoáveis. Podemos observar ainda na Tabela \ref{tab:cap-aprend-estat-trade-off-10-obs} que o RMSE do modelo polinomial de grau 9 calculado nas 100 novas observações\footnote{Aqui, os modelos não foram reajustados. Foram considerados os modelos ajustados apenas com as 10 primeiras observações} é o maior entre todos os candidatos. Por fim, observe na Figura \ref{cap-aprend-estat-trade-off} (d) como a curva desse modelo muda quando o ajustamos agora usando as 100 novas observações.

Durante a modelagem, estamos sempre em busca de modelos que se ajustem bem à amostra, mas que também possam ser generalizados para a população. Nesse sentido, chamamos de \textit{viés} o quanto o modelo ajustado está distante dos dados da amostra e de \textit{variância} o quanto o modelo erra ao o utilizarmos para prever novas observações. O viés representa o erro induzido por aproximar um fenômeno real, que pode pode ser extremamente complicado, por um modelo muito mais simples e a \textit{variância} o quanto as estimativas dos parâmetros do modelo mudariam se nós tivéssemos usado uma amostra diferente. Assim, dizemos que modelos mal ajustados apresentam alto viés e modelos que erram muito na predição de novas observações apresentam alta variância. 

É muito comum utilizarmos estratégias que se preocupam apenas com a minimização do viés. Essas estratégias geram modelos mais complexos, visando um ajuste cada vez melhor aos dados, sem levar em conta o quanto isso será representativo em um contexto mais geral. No exemplo anterior, isso fica claro com o ajuste de polinômios de grau cada vez maior aos dados. Além disso, o polinômio de grau 9 ilustra, de forma bem simplificada, o conceito de sobreajuste, que ocorre quando o modelo absorve de forma inadequada comportamentos da amostra que não são generalizáveis para a população. Modelos sobreajustados apresentam baixo viés, mas alta variância, não sendo apropriados para representar o fenômeno de interesse. Controlar o balanço entre o viés e a variância é um dos maiores desafios da modelagem preditiva.

Na presença de muitos preditores, não é possível visualizar graficamente o sobreajuste, como mostrado no exemplo. Por isso, na prática, nem sempre é trivial identificar um modelo sobreajustado. Para contornar esse problema, apresentaremos na próxima seção medidas utilizadas para quantificar o viés e a variância de um modelo.

\section{Estimando a performance do modelo}

Na Seção \ref{sec:reg-quali-mod}, vimos que o $R^2$ e a raiz do erro quadrático médio (RMSE) podem ser utilizados para avaliar a qualidade do ajuste de um modelo de regressão linear. Embora o $R^2$ seja utilizado apenas nessa classe de modelos, o RMSE pode ser calculado em qualquer contexto. Em alguns casos, podemos querer utilizar o erro absoluto médio (MAE), que dá menos peso para erros em valores muito altos da variável resposta. Em outros, pode ser razoável penalizar mais justamente os valores mais altos, e então construímos uma medida que distribui os pesos dessa maneira. 

A escolha da métrica de performance vai depender sempre do objetivo do estudo. Independentemente da medida escolhida, ao calculá-la para as próprias observações utilizadas no ajuste, temos uma estimativa do viés do modelo, isto é, o quanto o modelo escolhido se ajusta bem à amostra. Essa quantidade é chamada de \textit{erro de treino}. Para obtermos uma estimativa da variância, precisamos calcular a medida de performance para observações não utilizadas no ajuste, que representem uma nova amostra do fenômeno sob estudo. Essa quantidade é chamada de \textit{erro de teste}.

Na prática, nem sempre teremos à disposição uma nova base de dados para a estimação da variância. Uma alternativa nesses casos é utilizar técnicas de \textit{validação cruzada}, que permite separar a base em observações utilizadas para \textit{treinar} o modelo e observações para estimar sua performance. Essas técnicas serão o tema da próxima seção.

\section{Métodos de reamostragem}

A reamostragem consiste na técnica de gerar novas amostras a partir de uma base principal. As técnicas de reamostragem mais utilizadas são a \textit{validação cruzada} e o \textit{bootstrapping}. 


\subsection{Validação cruzada}

Como na maioria dos estudos não é possível obter facilmente novas observações, podemos calcular o erro de teste, a estimativa da variância do modelo, dividindo a amostra original em duas partes: uma utilizada para o ajuste do modelo (amostra de treino) e a outra para o cálculo do erro (amostra de teste), essa última agindo como se fosse um conjunto de novas observações. Essa técnica é conhecida como validação cruzada \citep{James2013}. Há diversos tipos de validação cruzada, que variam a depender da forma utilizada para dividir a amostra. Nesta seção, apresentaremos os principais tipos de validação cruzada e discutiremos as vantagens e desvantagens de cada um.

%Quando precisamos escolher valores para hiperparâmetros do modelo, como o grau de suavização de um modelo aditivo generalizado (Seção \ref{sec:gam}) ou o $\lambda$ do LASSO (Seção \ref{sec:lasso}), podemos dividir a amostra em ainda mais uma parte: uma amostra de validação. Nesses casos, os modelos são treinados com a amostra de teste e, para diversos valores do hiperparâmetro, calculamos o erro de teste na amostra de validação. Escolhemos então o hiperparâmetro com menor erro de teste e utilizamos a amostra de teste para calcular o erro de teste do modelo final. Repare que a amostra de teste nunca é utilizada para ajustar o modelo.

\subsubsection{Amostra de validação}

A amostra de validação é a forma mais simples de validação cruzada. A estratégia consiste em dividir aleatoriamente as observações em um conjunto de treino, usado para ajustar o modelo, e outro de teste, utilizado exclusivamente para estimar o erro de teste. 

A proporção de observações em cada uma depende do tamanho amostral. Costuma-se utilizar 30\% da amostra original no conjunto de teste, mas esse número pode ser menor para amostras muito grandes (mais de 100 mil observações, por exemplo).

As maiores vantagens dessa técnica é a sua simplicidade e a necessidade de se ajustar o modelo uma única vez. No entanto, conforme discutido em \cite{James2013}, a amostra de validação apresenta duas potenciais desvantagens:

\begin{itemize}
	\item a estimativa do erro de teste pode apresentar alta variabilidade, dependendo de quais observações ficaram na amostra de treino e quais ficaram na amostra de validação;
	\item como a acurácia de modelos estatísticos é menor quando ajustados com menos observações, e apenas parte das observações são utilizadas para treinar o modelo, o  erro de teste pode estar sendo superestimado. 
\end{itemize}

A seguir, apresentaremos o LOOCV, um método de validação cruzada que não possui essas limitações.

\subsubsection{Validação cruzada \textit{leave-one-out} (LOOCV)}

Considere uma amostra com $n$ observações. A validação cruzada \textit{leave-one-out} (LOOCV) consiste em rodar o modelo escolhido $n$ vezes, sendo que, em cada ajuste, deixamos de fora a $i$-ésima observação, $i = 1, \dots, n$, e a utilizamos para calcular o erro de teste. A estimativa final do erro de teste será então a média das $n$ medidas parciais. Uma esquematização dessa técnica está representada na Figura \ref{cap-aprend-estat-loocv}.


\begin{figure}[h!]
	\centering
	\includegraphics[width=0.5\linewidth]{figuras/cap-aprend-estat-esquema-loocv.png}
	\caption{Esquematização da validação cruzada \textit{leave-one-out}.}
	\label{cap-aprend-estat-loocv}
\end{figure}

Repare que, neste caso, todas as observações são utilizadas no ajuste do modelo e na estimativa do erro de teste, o que elimina as limitações da amostra de validação. No entanto, uma desvantagem aqui é a necessidade de ajustar o modelo $n$ vezes. Quando $n$ é muito grande, a LOOCV pode exigir muito esforço computacional, inviabilizando a sua utilização em muitos casos. 

Vale ressaltar que esse procedimento é utilizado para estimar as métricas de performance do modelo, sendo que ajuste do modelo final da análise contempla todas as observações da amostra. Dessa forma, ao fim desse procedimento, $n+1$ modelos são ajustados: as $n$ interações da LOOCV e o ajuste do modelo com todas as observações.

A seguir, apresentamos validação cruzada \textit{k-fold}, uma generalização da LOOCV que não possui a contrapartida computacional.

\subsubsection{K-fold}

Podemos generalizar a LOOCV dividindo a amostra original aleatoriamente em $k$ grupos com aproximadamente a mesma quantidade de observações. Então ajustamos o modelo $k$ vezes, sendo que em cada ajuste selecionamos um grupo diferente como amostra de teste. Essa abordagem é chamada de \textit{k-fold}. Note que a LOOCV é o caso especial em que $k = n$. Na prática, escolhemos valores de $k$ entre 3 e 10, sendo que $k = 5$ é o mais utilizado (Figura \ref{cap-aprend-estat-esquema-k-fold-cv}).

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{figuras/cap-aprend-estat-esquema-k-fold-cv.png}
	\caption{Esquematização da validação cruzada \textit{k-fold}, com $k = 5$.}
	\label{cap-aprend-estat-esquema-k-fold-cv}
\end{figure}

A maior vantagem da validação cruzada \textit{k-fold} sobre a LOOCV é computacional. Em vez de ajustarmos o modelo $n$ vezes, ajustamos apenas $k$, sendo que $k << n$. E como estamos utilizando todas as observações para treinar o modelo, não temos as limitações de se utilizar uma única amostra de validação.

Assim como na LOOCV, o objetivo desse procedimento é estimar o erro de predição. Ao fim, ajustamos o modelo utilizando todas as observações na amostra, que será considerado o modelo final. Assim, o modelo é ajustado $k+1$ vezes: as $k$ iterações da validação $k$-fold e o ajuste do modelo com todas as observações.

Como discutimos até agora, a validação cruzada é geralmente utilizada para avaliar a performance do modelo. A seguir, apresentaremos uma técnica de reamostragem muito utilizada também para a estimação de quantidades acerca dos parâmetros do modelo.

\subsection{Bootstrapping}

O \textit{bootstrapping} é uma poderosa ferramenta estatística utilizada para quantificar incertezas associadas a estimadores e modelos estatísticos. Ela consiste em gerar $m$ novas amostras a partir de sorteios com reposição da amostra original. Para cada uma das amostras geradas, ajustamos o modelo escolhido e guardamos as estimativas dos parâmetros. Ao repetirmos esse processo para as $m$ amostras, teremos $m$ estimativas diferentes para cada parâmetro do modelo. Assim, para cada parâmetro, podemos, por exemplo, calcular o desvio-padrão dessas $m$ estimativas e utilizar essa medida como o erro-padrão associado ao coeficiente. Repare que os parâmetros do modelo devem ser estimados utilizando a amostra original. Nesse exemplo, o \textit{bootstrapping} seria usado apenas para estimar a variabilidade dos coeficientes.

Essa técnica é utilizada principalmente quando não conhecemos a distribuição dos estimadores do modelo ou quando precisamos controlar outras fontes de variabilidade. \cite{Salvo2014} e \cite{Salvo2017}, por exemplo, utilizaram o \textit{bootstrapping} para estimar o erro-padrão dos coeficientes do modelo de regressão linear ajustado para associar a concentração de ozônio na cidade de São Paulo com a proporção estimada de veículos bicombustíveis rodando a gasolina. Segundo os autores, essa estratégia foi utilizada para contemplar a variação causada pelo erro de medida presente na estimação da proporção de carros rodando a gasolina e na medição das condições climáticas.

O \textit{bootstrapping} também pode ser utilizado para a estimação da performance do modelo. Neste caso, cada uma das $m$ amostras é utilizada como conjunto de treino e as observações que foram sorteadas em cada amostra é utilizada com conjunto de teste. Assim como na LOOCV, se $m$ for muito grande, essa estratégia pode gerar um esforço computacional muito alto.

Mais informações sobre o \textit{bootstrapping} podem ser encontradas em \cite{James2013}.

%Essa técnica pode ser utilizada, por exemplo, para estimar o erro-padrão dos coeficientes de um modelo de regressão.

\section{Seleção de variáveis}

Muitas vezes, na construção do modelo, incluímos variáveis que não são associadas com o fenômeno sob estudo. Isso acontece principalmente quando temos pouco conhecimento sobre o mecanismo gerador do fenômeno ou quando estamos justamente investigando quais fatores estão associados a ele. 

Como variáveis irrelevantes geram uma complexidade desnecessária no modelo, é apropriado pesarmos em estratégias para retirá-las da análise, aumentando assim a interpretabilidade dos resultados.

Nesta seção, apresentaremos algumas técnicas de seleção de variáveis que podem ser utilizadas em qualquer classe de modelos estatísticos.

\subsection{Selecionando o melhor subconjunto de preditores}

A maneira mais simples que podemos pensar para selecionar variáveis em um modelo é ajustar todas as possíveis combinações dos $p$ preditores e avaliar qual produz o melhor ajuste segundo alguma métrica. Essa estratégia é chamada de \textit{melhor subconjunto de preditores} (\textit{best subset selection}, em inglês) e seu procedimento de seleção pode ser resumido pelos passo abaixo:

\begin{enumerate}
	\item Ajustar o modelo nulo, sem nenhum preditor.
	\item Para $k = 1, \dots, p$, ajustar todos os modelos com $k$ preditores e escolher o melhor entre eles, isto é, aquele com menor RSME ou maior $R^2$ por exemplo.
	\item Para cada um dos $p+1$ modelos escolhidos, selecionar o melhor usando o $R^2$ ajustado, RMSE calculado por validação cruzada (erro de teste), AIC ou BIC\footnote{O AIC e o BIC são medidas da qualidade do ajuste penalizadas pelo número de parâmetros do modelo. Mais informações, consultar \cite{James2013}}.
\end{enumerate}
Observe que a métrica utilizada para selecionar o modelo final deve ser penalizada pelo número de parâmetros, pois, caso contrário, escolheríamos sempre o modelo com mais preditores.

Para um número relativamente pequeno de variáveis, selecionar o melhor subconjunto de preditores é uma estratégia conceitualmente simples e de fácil execução. No entanto, conforme $p$ cresce, essa técnica pode se tornar computacionalmente inviável. Na Tabela \ref{tab:3-preditores-selecao-melhor-subconjunto} apresentamos os 7 modelos que precisaríamos ajustar se tivéssemos 3 preditores, $X_1$, $X_2$ e$X_3$, e um modelo de regressão linear (Seção \ref{sec:modelo-linear}). Para $p = 20$, por exemplo, precisaríamos rodar mais de um milhão de modelos, o que poderia inviabilizar a execução dessa estratégia.

\begin{table}[h!]
	\centering
	\caption{Modelos de regressão linear que devem ser ajustados para selecionar o melhor subconjunto de variáveis no caso com 3 preditores.}
	\begin{tabular}{c|c|c}
		\hline 
		Uma variável & Duas variáveis & Três variáveis \\ 
		\hline 
		$Y = \beta_0 + \beta_1 X_1 + \epsilon$ & $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$ &  \multirow{3}{5.5cm}{$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 +  \epsilon$} \\  
		$Y = \beta_0 + \beta_1 X_2 + \epsilon$ & $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_3 + \epsilon$  \\
		$Y = \beta_0 + \beta_1 X_3 + \epsilon$ & $Y = \beta_0 + \beta_1 X_2 + \beta_2 X_3 + \epsilon$ \\ 
		\hline 
	\end{tabular}
    \label{tab:3-preditores-selecao-melhor-subconjunto}	
\end{table}

A seguir, apresentamos algumas estratégias computacionalmente eficientes para aplicarmos em problemas com muitos preditores.

\subsection{Stepwise}

Os métodos \textit{stepwise} são algoritmos de seleção de variáveis que visam encontrar o melhor sub-conjunto de preditores dentro de um conjunto restrito de combinações em vez de ajustar todos os 2$^p$ modelos possíveis. 

A diferença entre cada método \textit{stepwise} está em como as variáveis são adicionas ou retiradas do modelo em cada passo. Os mais utilizados são o \textit{foward stepwise} e o \textit{backward stepwise}.

O \textit{foward stepwise} consiste na execução dos seguintes passos:

\begin{enumerate}
	\item Ajuste o modelo nulo ($M_0$), sem preditores.
	\item Ajuste todos os $p$ modelos com 1 preditor e escolha o melhor\footnote{Maior $R^2$, por exemplo.} ($M_1$).
	\item Ajuste todos os $p-1$ modelos com 2 preditores que contenham o preditor selecionado no passo anterior e escolha o melhor ($M_2$).
	\item De forma análoga, ajuste os modelos com 3, 4, ..., $p$ preditores, mantendo sempre como base o modelo obtido anteriormente, e em cada passo escolha o melhor ($M_3$, $M_4$, \dots, $M_p$).
	\item Escolha o melhor modelo entre $M_0$, $M_1$, \dots, $M_p$ utilizando erro preditivo, AIC, BIC ou $R^2$ ajustado.
\end{enumerate}

Repare que o \textit{foward stepwise} diminui o número de modelos ajustados de $2^p$ para $1 + p(p + 1)/2$. Para $p = 20$, o número de modelos diminui de 1.048.576 para 211.  

A ideia do método \textit{backward stepwise} é parecida com a do \textit{foward}. A diferença é que começamos no passo 1 com o modelo completo ($M_p$), com todos os preditores, e nos passos seguintes retiramos cada um dos preditores e ajustamos os modelos correspondentes, selecionando sempre aquele com maior $R^2$ ($M_{p-1}, M_{p-2}, \dots, M_0$). Ao fim, escolhemos o melhor entre os modelos $M_0$, $M_1$, \dots, $M_p$ utilizando erro preditivo, AIC, BIC ou $R^2$ ajustado. O número de modelos ajustados nesse caso é igual ao do \textit{foward stepwise}.

Ainda existem métodos \textit{stepwise} híbridos, nos quais os preditores são adicionados sequencialmente, assim como no \textit{foward stepwise}, mas em cada etapa é avaliado se um dos preditores já incluídos deve ou não sair do modelo. Essa estratégia tenta considerar mais modelos, chegando mais próximo da seleção do melhor sub-conjunto discutida na seção anterior. Para mais informações, consultar \cite{Nelder1972}.

\section{Regularização}
\label{sec:regularizacao}

Os métodos de seleção de sub-conjuntos de preditores apresentados nas seções anteriores envolvem o ajuste de diversos modelos e a escolha do melhor segundo alguma métrica. Uma outra forma de selecionar variáveis é a partir das \textit{técnicas de regularização}. Essas técnicas, a princípio, envolvem o ajuste de um único modelo e introduzem no processo de estimação penalizações que limitam as estimativas dos coeficientes, encolhendo seus valores na direção do zero.

A utilização da regularização pode levar a uma redução substancial da variância do modelo ao custo de um pequeno aumento no viés. Apresentaremos nesta seção as formas mais utilizadas de regularização: a \textit{regressão ridge} e o LASSO. 

\subsubsection{Regressão Ridge}

De uma forma geral, o processo de estimação dos parâmetros de um modelo consiste na minimização de uma função de perda $L(y, f(x))$ que depende dos dados observados $(x, y)$ e do modelo escolhido $(f(\cdot))$. As técnicas de regularização consistem em adicionar uma penalidade nessa função de perda, de tal forma que os coeficientes dos preditores pouco associados à variável resposta sejam encolhidos na direção do zero.

No caso da regressão ridge \citep{James2013}, essa penalização é dada por

\begin{displaymath}
	L(y, f(x)) + \lambda \sum_{j=1}^p \beta_j^2,
\end{displaymath}
sendo $\beta_1, \dots, \beta_p$ os parâmetros do modelo $f(\cdot)$ e $\lambda$ um hiperparâmetro\footnote{Hiperparâmetros são parâmetros que não são estimados diretamente pelos dados.} que controla o impacto da penalização nas estimativas dos coeficientes. Quando $\lambda = 0$, o termo é anulado e as estimativas são calculadas sem penalização. Conforme $\lambda \longrightarrow \infty$, os coeficientes $\beta_j$ passam a ser penalizados, encolhendo seus valores na direção do zero. A vantagem desse comportamento está na potencial redução da variância do modelo, em troca de um pequeno aumento do viés, já que os coeficientes menos importantes recebem cada vez menos peso. Assim, a regularização é uma alternativa para lidarmos com o balanço entre viés e variância discutido na Seção \ref{sec:trade-off}.

No caso da regressão ridge, é possível mostrar que, para qualquer $i = 1, \dots, p$,  $\beta_i = 0$ apenas se $\lambda = \infty$. Isso significa que não estamos fazendo seleção de variáveis, isto é, o modelo ajustado sempre terá todos os preditores. Apesar de estarmos melhorando a performance do modelo diminuindo o peso dos preditores menos importantes, isso pode não ser o ideal quando quisermos de fato eliminar variáveis do modelo. Nesses casos, uma boa alternativa é utilizar o LASSO.

\subsubsection{Least absolute shrinkage and selection operator (LASSO)}

O LASSO (\textit{least absolute shrinkage and selection operator}) é uma técnica análoga à regressão ridge, mas com penalização dada por

\begin{displaymath}
	L(y, f(x)) +\lambda \sum_{j=1}^p |\beta_j|.
\end{displaymath}
Para $\lambda$ grande o suficiente, essa penalização força que alguns dos coeficientes sejam estimados exatamente como 0 e os correspondentes preditores associados serão eliminados do ajuste. Assim, ao utilizarmos o LASSO, estamos ao mesmo tempo reduzindo a variância do modelo e executando seleção de variáveis.

Um ponto importante sobre a aplicação das técnicas de regularização é a escala dos preditores. A maioria dos processos de estimação usuais são invariantes à escala em que os preditores foram medidos, isto é, ajustar o modelo usando o preditor $X_1$ ou $cX_1$, $c$ uma constante qualquer, não mudará a interpretação dos resultados. No caso da regressão ridge e do LASSO, a escala dos preditores influenciam não só a estimativa dos próprios coeficientes, mas também a estimativa dos outros parâmetros do modelo. Dessa forma, um passo importante anterior à aplicação dessas técnicas é a padronização dos preditores, de tal forma que todos fiquem com a mesma média e variância. Essa padronização pode ser feita a partir da fórmula

\begin{equation}
	\tilde{X}_{ij} = \frac{X_{ij} - \bar{X}_j}{\sqrt{\frac{1}{n}\sum_{i =1}^n (X_{ij} - \bar{X}_j)^2}}, 	
\end{equation}
sendo o denominador dessa expressão a estimativa do desvio-padrão do $j$-ésimo preditor. Consequentemente, todos os preditores terão média 0 desvio-padrão igual a 1.

Embora haja muita discussão sobre a validade de testes de hipóteses do tipo $\beta = 0$ para o LASSO, já que o algoritmo zera automaticamente os coeficientes menos importantes, alguns trabalhos vêm surgindo nos últimos anos sobre o calculo do erro-padrão e o desenvolvimento de testes paras as estimativas \citep{Adel2014, Lockhart2014}. Uma boa alternativa para avaliar a variabilidade das estimativas dos coeficientes é utilizar o \textit{bootstrapping}.

Para uma discussão mais aprofundada sobre a interpretação da regressão ridge e do LASSO, consulte o Capítulo 6 de \cite{James2013}. Para o desenvolvimento matemático dessas técnicas, o Capítulo 5 de \cite{Hastie2008} é uma ótima referência.

\section{Quantificando a importância dos preditores}

Nas últimas seções, discutimos técnicas para removermos do modelo as variáveis que não ajudam a explicar a variabilidade da variável resposta. Em alguns casos, também gostaríamos de saber, entre os preditores que permaneceram no modelo, quais são os mais importantes.

Os valores $p$ são amplamente utilizados para definir as variáveis estatisticamente significantes para explicar a variável resposta. Dado um coeficiente $\beta$, o valor $p$ associado representa uma medida de evidência a favor da hipótese $\beta = 0$ e pode ser utilizado tanto para seleção de variáveis quanto para quantificar a magnitude de uma associação. Podemos interpretar o valor $p$ como quanto a estimativa encontrada seria inverossímil caso o verdadeiro valor de $\beta$ fosse 0. Se o valor $p$ for muito baixo (próximo de zero), significa que a estimativa obtida teria baixa probabilidade caso $\beta$ fosse 0 e então rejeitamos a hipótese de que esse coeficiente é nulo. Caso contrário, se o valor $p$ for alto, significa que a estimativa obtida não é inverossímil em um cenário em que $\beta$ é zero, e então não rejeitamos a hipótese de que $\beta = 0$. 

Ao cálculo do valor $p$ está associado uma estatística de teste, que também pode ser usada para quantificar a importância dos preditores. No modelo de regressão linear, por exemplo, o valor da estatística do teste $t$ pode ser utilizada, de tal forma que, quanto maior o valor absoluto da estatística, maior será a importância do preditor para explicar a variável resposta.

Em alguns casos, a variação no erro preditivo quando um preditor é eliminado do modelo é utilizada como medida de importância. Essa métrica mais geral é bastante utilizada em modelos de regressão que envolvem funções suavizadoras, como os modelos aditivos generalizados.

Já para para a regressão ridge ou o LASSO, em que padronizamos as variáveis explicativas, uma medida de importância pode ser o próprio valor do coeficiente.

As métricas de importância vão depender sempre do modelo utilizado. De uma forma geral, os programas estatísticos já possuem métricas de importância implementadas. No R, a função \texttt{varImp()} do pacote \texttt{caret} calcula uma medida de importância para a maioria dos modelos disponíveis.

\section{Modelos de árvores}

Modelos baseados em árvores \citep{Hastie1990,James2013} são algoritmos bastante utilizados tanto para regressão quanto para classificação. Esses métodos envolvem a segmentação do espaço gerado pelas variáveis explicativas em algumas regiões mais simples, onde a média ou a moda da variável resposta são utilizadas como predição.

As chamadas árvores de decisão são modelos conceitualmente e computacionalmente simples, bastante populares pela sua interpretabilidade, apesar da precisão inferior quando comparados com modelos mais flexíveis. Generalizações desse modelos, como as \textit{random forests}, costumam apresentar alta precisão, mesmo quando comparadas a modelos lineares, porém são pouco interpretáveis.

Nesta seção, introduziremos os principais conceitos por trás das árvores de decisão e das \textit{random forests}.


\subsection{Árvores de decisão}

As árvores de decisão se baseiam no particionamento das variáveis explicativas, de tal forma que as regiões formadas gerem predições para a variável resposta com baixo erro segundo alguma métrica (geralmente RMSE para regressão, o foco deste trabalho). O particionamento é feito a partir de \textit{regras} que dividem o espeço gerado pelas variáveis explicativas. Cada regra é representada por um \textit{nó} e a cada partição criada podem ser acrescentadas mais regras.

Na Figura \ref{fig:cap-aprend-estat-arvore}, apresentamos um exemplo de árvore de decisão para a concentração de ozônio explicada pela temperatura. Para interpretá-la, basta começarmos pelo primeiro nó (a caixa mais alta da figura) e seguir as regras de decisão até encontrarmos um nó final (uma das caixas no nível mais baixo). Cada nó final apresenta a estimativa para as observações que caíram naquela partição e o número de observações dentro da partição (absoluto e proporcional ao tamanho da amostra). A Figura \ref{fig:cap-aprend-estat-arvore} indica que dias com temperatura menor de 26° C serão preditos com concentração de ozônio igual a 38 $\mu/m^3$; dias com temperatura entre 27 e 29° C serão preditos com concentração de ozônio igual a 69 $\mu/m^3$; e dias com temperatura maior de 92° C serão preditos com temperatura igual a 92° C. Além de conseguirmos predizer facilmente o nível de ozônio a partir da temperatura, também podemos observar que a concentração de ozônio aumenta com a temperatura. Na prática, podemos utilizar as árvores de decisão com quantos preditores forem necessários, sendo que cada nó poderá conter uma regra com um preditor diferente.

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.7\linewidth]{figuras/cap-aprend-estat-arvore.pdf}
	\caption{Exemplo de uma árvore de decisão para a concentração de ozônio explicada pela temperatura.}
	\label{fig:cap-aprend-estat-arvore}
\end{figure}

Observe que no exemplo temos 3 nós finais, mas, teoricamente, poderíamos continuar particionando a temperatura até cada possível valor ter o seu próprio nó. Esse seria um caso de árvore de decisão sobreajustada, apresentando pouco poder de generalização. A escolha do número de nós finais é feita a partir de uma técnica conhecida como \textit{poda}, que consiste em usar validação cruzada para definir a melhor altura para a árvore.

As árvores de decisão copiam bem o processo de tomada de decisão do cérebro humano, e por isso são mais simples de interpretar até mesmo que o modelo de regressão linear. No entanto, elas apresentam baixo poder preditivo e raramente são utilizadas para descrever processos muito complexos. A seguir, apresentaremos as \textit{random forests}, que abrem mão da interpretabilidade em troca de um alto grau de precisão.

\subsection{Random Forests}
\label{sec:random-forest}

As árvores de decisão apresentadas na seção anterior tendem a ter alta variância, isto é, se dividirmos nossa amostra em duas e aplicarmos o mesmo modelo de árvores de decisão em cada um, há uma grande chance de obtermos divisões diferentes no espaço amostral e, consequentemente, diferentes predições. 

Uma maneira de lidar com esse problema é gerar diferentes amostras de \textit{bootstrapping}, ajustar o mesmo modelo em cada uma delas e utilizar predições médias como estimativas final do modelo. Em um contexto geral, essa técnica é chamada de \textit{bootstrapping aggregation} ou \textit{bagging}. Ele pode ser utilizada para qualquer modelo e seu objetivo geral é diminuir a variância das estimativas.

As chamadas \textit{random forests} \cite{Hastie2008} são uma aplicação do \textit{bagging} para árvores de decisão. Neste modelo, além de utilizarmos uma amostra de \textit{boostraping} diferente para cada árvore ajustada, também fazemos uma seleção dos preditores para cada ajuste. Isso impede que as árvores ajustadas sejam muito correlacionadas e permite que preditores que seriam preteridos por serem "menos importantes" também ajudem a explicar a variação da variável resposta.

Em cada iteração do algoritmo,

\begin{enumerate}
	\item sorteamos $m$ dos $p$ preditores, $m \leq p$;
	\item geramos uma amostra de \textit{boostrapping} a partir da base inicial;
	\item ajustamos uma árvore de decisão utilizando os $m$ preditores escolhidos em (1) e a amostra em (2).
\end{enumerate}

Repetindo esse procedimento $M$ vezes, teremos $M$ estimativas para cada nova observação. A predição final será a média dessas $M$ estimativas (no caso de regressão) ou a classe mais frequente (no caso de classificação). Geralmente, $m$ é escolhido como $\sqrt{p}$, mas esse hiperparâmetro pode ser definido utilizando validação cruzada. $M$ por volta de 200 costuma ser suficiente para gerar bons resultados\footnote{Para valores de $M$ muito altos (maior que 200), a amostragem dos preditores passa a criar árvores mais parecidas com as já existentes, o que não gera maiores ganhos de precisão.}.

\textit{Random forests} podem ser ajustadas utilizando a função \texttt{train()} do pacote \texttt{caret} com \texttt{methord = "ranger"}. Os principais hiperparâmetros que devem ser escolhidos são o número de sorteados em cada ajuste ($m$) e o número mínimo de observações nos nós finais de cada árvore.

\subsection{XGBoost}

Assim como o \textit{bagging}, apresentado na seção anterior, o \textit{boosting} é uma abordagem geral aplicada a diversos modelos para aumentar seu poder preditivo. No contexto de árvores de decisão, a ideia aqui é construir árvores sequencialmente, sendo que cada árvore utiliza informações obtidas da árvore anterior. Ao contrário do \textit{bagging}, o \textit{boosting} não envolve amostras de \textit{bootstraping}, sendo que cada passo do algoritmo utiliza uma versão modificada dos dados originais.

A ideia por trás do \textit{boosting} é \textit{aprender devagar}. Em vez de tentarmos ajustar uma grande árvore em toda a base de dados, ajustamos pequenas árvores $\hat{f}^1(x), \hat{f}^2(x), \dots, \hat{f}^B(x)$ sequencialmente, de tal forma que a árvore $\hat{f}^i(x)$ é ajustada utilizando os resíduos da árvore $\hat{f}^{i-1}(x)$. Considere $\hat{f}(x)$ a nossa função estimada final do modelo e que, inicialmente, $\hat{f}(x) = 0$. Em cada passo do algoritmo, a nova árvore $\hat{f}^i(x)$ é somada à função estimada $\hat{f}(x)$ utilizando-se um parâmetro de encolhimento $\lambda > 0$, isto é, no passo $i$, $\hat{f}(x)$ passa a ser $\hat{f}(x) + \lambda\hat{f}^i(x)$. Assim, os resíduos são atualizados em cada passo e $\hat{f}(x)$  melhora lentamente em regiões onde sua performance era ruim. O parâmetro de encolhimento $\lambda$, o número de árvores $B$ e o tamanho de cada árvore (controlado pelo número de nós terminais) são os principais hiperparâmetros do modelo e podem ser escolhidos por validação cruzada.

O \textit{gradient boosting} generaliza o \textit{boosting} substituindo o ajuste dos resíduos pela minimização de uma função de custo $L(y, f(x))$. Em cada etapa, ajustamos uma árvore não mais aos resíduos, mas sim às quantidades

\begin{displaymath}
g_i = - \left[\frac{\delta L(y_i - f(x_i))}{\delta f(x_i)}\right]_{f = f^{i-1}}, \quad i = 1, \dots, n.
\end{displaymath}
As quantidades $g_1, \dots, g_n$ representam o gradiente da função de custo em relação a cada árvore $f^i(x)$. Portanto, em cada etapa, minimizamos lentamente a função de custo a partir da nova árvore ajustada. Esse é algoritmo de minimização é chamado de \textit{gradient descent} \cite{Kiefer1952}.

Por fim, o \textit{XGBoost} (ou \textit{extreme gradient boosting}) é uma implementação eficiente do \textit{gradient boosting} que utiliza alguns "truques" para otimizar o processo de estimação, como penalização das árvores para controlar a velocidade de aprendizado, randomização dos parâmetros para diminuir a correlação entre as árvores e encolhimento dos nós terminais para diminuir o sobreajuste. Esse algoritmo é um dos mais utilizados em problemas preditivos complexos hoje em dia, estando entre os três modelos mais vencedores de competições de \textit{machine learning}.

Assim como as \textit{random forests}, esses modelos são não-interpretáveis e precisam de ferramentas como gráficos de dependência parcial ou o LIME para a avaliação da associação entre as variáveis.

Na linguagem R, podemos ajustar o \textit{XGBoost} utilizando a função \texttt{train()} do pacote \texttt{caret} com o argumento \texttt{method = "xgbTree"} ou diretamente as funções do pacote \texttt{xgboost}.

\section{Interpretando modelos caixa-preta}

Ao contrário dos modelos apresentados no capítulo anterior, modelos como o \textit{random forest} ou o \textit{XGboost} não são interpretáveis. Isso implica que não conseguimos avaliar diretamente como cada preditor está associado com a variável resposta, impossibilitando que esses modelos sejam usados para inferência ou criando muita desconfiança a respeito do que esses modelos estão fazendo por trás das cortinas.

Neste capítulo, vamos apresentar algumas técnicas que, sob algumas suposições ou restrições, visam elucidar qual é a relação dos preditores com os valores preditos, abrindo um pouco a caixa-preta dos modelos de \textit{machine learning}.

\subsection{Gráficos de dependência parcial}

\subsection{Esperança condicional individual}

\subsection{Gráficos de efeitos locais acumulados}

\subsection{LIME}

%The idea is quite intuitive. First of all, forget about the training data and imagine you only have the black box model where you can input data points and get the models predicted outcome. You can probe the box as often as you want. Your goal is to understand why the machine learning model made a certain prediction. LIME tests out what happens to the model?s predictions when you feed variations of your data into the machine learning model. LIME generates a new dataset consisting of perturbed samples and the associated black box model?s predictions. On this dataset LIME then trains an interpretable model weighted by the proximity of the sampled instances to the instance of interest. The interpretable model can basically be anything from this chapter, for example LASSO or a decision tree. The learned model should be a good approximation of the machine learning model locally, but it does not have to be so globally. This kind of accuracy is also called local fidelity.

Uma outra técnica utilizada para interpretar modelos caixa-preta é o LIME (\textit{Local snterpretable Model-Agnostic Explanations}) \citep{Ribeiro2016}. Dado qualquer modelo estatístico, essa técnica consiste em explicar quais preditores foram responsáveis pela predição de uma observação específica. Nos últimos anos, o LIME vem sendo muito utilizado para explicar, corrigir e aperfeiçoar modelos não interpretáveis.

Essa técnica assume que todo modelo complexo prevê valores parecidos para duas observações muito próximas, sendo possível ajustar um modelo simples ao redor de uma única observação para copiar o comportamento global do modelo complexo. Assim, a partir de um modelo interpretável podemos obter para cada observação uma explicação sobre a predição feita pelo modelo não-interpretável.

O algoritmo consiste em:

\begin{enumerate}
	\item Para cada predição a ser explicada, permutar a observação $n$ vezes.
	\item Predizer cada uma das observações permutadas usando o modelo complexo.
	\item Calcular uma medida de distância e similaridade entre as permutações e a observação original. Geralmente a distância de Gower é utilizada \citep{Gower1971}.
	\item Selecionar as $m$ variáveis mais importantes utilizadas pelo modelo complexo para explicar os dados permutados.
	\item Ajustar um modelo interpretável aos dados permutados, utilizando as predições do modelo complexo como variável resposta e as $m$ variáveis selecionadas no passo anterior como preditores, ponderando pela medida de similaridade com a observação original.
	\item Usar as estimativas desse modelo simples como as explicações para o comportamento local do modelo complexo.	 
\end{enumerate}

A implementação do LIME exige definir como as observações serão permutadas, qual medida de similaridade será usada, qual o valor de $m$ e qual modelo interpretável será utilizado. Uma boa discussão sobre esses pontos pode ser encontrada no \textit{vignette} do pacote \texttt{lime}, no qual a técnica foi implementada na linguagem R. O texto pode ser acessado por este link: \href{https://cran.r-project.org/web/packages/lime/vignettes/Understanding_lime.html}{https://goo.gl/Nu9ZsA}.